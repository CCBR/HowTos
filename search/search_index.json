{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This page is designed to share knowledge between analysts within CCBR . You'll find how-to-guides, best practices and tutorials under the following pages: - How To, Best Practices: GitHub, TechDev - How To: Setup and use HPCDME - How To: Create UCSC Tracks - How To: Create an R package - How To: use Datashare - How To: Create a Zenodo DOI Link - Tutorials: Snakemake This page was created through the contributions of several members within CCBR . If you would like to contribute to its development, please reach out to Vishal Koparde to get added to this repo in order to submit a PR.","title":"Intro"},{"location":"contributions/","text":"The following individuals were the main contributors to these HowTo manuals: GitHub: Samantha Sevilla & Vishal Koparde TechDev: Samantha Sevilla HPCDME: Vishal Koparde USCS Tracks: Samantha Sevilla Rpackage: Ned Cauley Other How To's: Ned Cauley Tutorials: Samantha Sevilla","title":"Contributions"},{"location":"GitHub/basic_actions/","text":"GitHub Basics: GitHub Actions \u00b6 The following describe the minimum GitHub actions that should be deployed with any production pipeline. The actions are automatically provided via the cookiecutter templates: NextFlow and Snakemake . Documentation (assumes mkdocs build ; required for all repos) These rules will automatically update any documentation built with mkdocs for all PR's. Rule Name(s): mkdocs_build \u2192 pages-build-and-deployment Lintr (required for CCBR projects and new pipelines) This rule will automatically perform a lintr with the data provided in the .test folder of the pipeline. Review the GitHub Best Practices - Test Data page for more information. Dry-run with test sample data for any PR to dev branch (required for CCBR projects and new pipelines) This rule will automatically perform a dry-run with the data provided in the .test folder of the pipeline. Review the GitHub Best Practices - Test Data page for more information. Full-run with full sample data for any PR to main branch (required for CCBR projects and new pipelines) This rule will automatically perform a full-run with the data provided in the .test folder of the pipeline. Review the GitHub Best Practices - Test Data page for more information. Auto pull/push from source (if applicable for CCBR projects and new pipelines) If the pipeline is forked from another location and updating this forked pipeline is required, an action will automatically perform a pull from the source location at least once a week. Add assigned issues & PRs to user projects. When an issue or PR is assigned to a CCBR member, this action will automatically add it to their personal GitHub Project, if they have one. This file can be copy and pasted exactly as-is into any CCBR repo from here .","title":"Creating GitHub Actions"},{"location":"GitHub/basic_actions/#github-basics-github-actions","text":"The following describe the minimum GitHub actions that should be deployed with any production pipeline. The actions are automatically provided via the cookiecutter templates: NextFlow and Snakemake . Documentation (assumes mkdocs build ; required for all repos) These rules will automatically update any documentation built with mkdocs for all PR's. Rule Name(s): mkdocs_build \u2192 pages-build-and-deployment Lintr (required for CCBR projects and new pipelines) This rule will automatically perform a lintr with the data provided in the .test folder of the pipeline. Review the GitHub Best Practices - Test Data page for more information. Dry-run with test sample data for any PR to dev branch (required for CCBR projects and new pipelines) This rule will automatically perform a dry-run with the data provided in the .test folder of the pipeline. Review the GitHub Best Practices - Test Data page for more information. Full-run with full sample data for any PR to main branch (required for CCBR projects and new pipelines) This rule will automatically perform a full-run with the data provided in the .test folder of the pipeline. Review the GitHub Best Practices - Test Data page for more information. Auto pull/push from source (if applicable for CCBR projects and new pipelines) If the pipeline is forked from another location and updating this forked pipeline is required, an action will automatically perform a pull from the source location at least once a week. Add assigned issues & PRs to user projects. When an issue or PR is assigned to a CCBR member, this action will automatically add it to their personal GitHub Project, if they have one. This file can be copy and pasted exactly as-is into any CCBR repo from here .","title":"GitHub Basics: GitHub Actions"},{"location":"GitHub/basic_docs/","text":"GitHub Basics: Documentation \u00b6 GitHub Pages is quick and easy way to build static websites for your GitHub repositories. Essentially, you write pages in Markdown which are then rendered to HTML and hosted on GitHub, free of cost! CCBR has used GitHub pages to provide extensive, legible and organized documentation for our pipelines. Examples are included below: CARLISLE Pipeliner RNA-seek Mkdocs is the with documentation tool preferred, with the Material theme, for most of the CCBR GitHub Pages websites. Install MkDocs, themes \u00b6 mkdocs and the Material for mkdocs theme can be installed using the following: pip install --upgrade pip pip install mkdocs pip install mkdocs-material Also install other common dependencies: pip install mkdocs-pymdownx-material-extras pip install mkdocs-git-revision-date-localized-plugin pip install mkdocs-git-revision-date-plugin pip install mkdocs-minify-plugin Conventions \u00b6 Generally, for GitHub repos with GitHub pages: The repository needs to be public (not private) The main/master branch has the markdown documents under a docs folder at the root level Rendered HTMLs are hosted under a gh-pages branch at root level Create website \u00b6 The following steps can be followed to build your first website Add mkdocs.yaml \u00b6 mkdocs.yaml needs to be added to the root of the master branch. A template of this file is available in the cookiecutter template. git clone https://github.com/CCBR/xyz.git cd xyz vi mkdocs.yaml git add mkdocs.yaml git commit -m \"adding mkdocs.yaml\" git push Here is a sample mkdocs.yaml : # Project Information site_name: CCBR How Tos site_author: Vishal Koparde, Ph.D. site_description: >- The **DEVIL** is in the **DETAILS**. Step-by-step detailed How To Guides for data management and other CCBR-relevant tasks. # Repository repo_name: CCBR/HowTos repo_url: https://github.com/CCBR/HowTos edit_uri: https://github.com/CCBR/HowTos/edit/main/docs/ # Copyright copyright: Copyright &copy; 2023 CCBR # Configuration theme: name: material features: - navigation.tabs - navigation.top - navigation.indexes - toc.integrate palette: - scheme: default primary: indigo accent: indigo toggle: icon: material/toggle-switch-off-outline name: Switch to dark mode - scheme: slate primary: red accent: red toggle: icon: material/toggle-switch name: Switch to light mode logo: assets/images/doc-book.svg favicon: assets/images/favicon.png # Plugins plugins: - search - git-revision-date - minify: minify_html: true # Customization extra: social: - icon: fontawesome/solid/users link: http://bioinformatics.cancer.gov - icon: fontawesome/brands/github link: https://github.com/CCRGeneticsBranch - icon: fontawesome/brands/docker link: https://hub.docker.com/orgs/nciccbr/repositories version: provider: mike # Extensions markdown_extensions: - markdown.extensions.admonition - markdown.extensions.attr_list - markdown.extensions.def_list - markdown.extensions.footnotes - markdown.extensions.meta - markdown.extensions.toc: permalink: true - pymdownx.arithmatex: generic: true - pymdownx.betterem: smart_enable: all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji: emoji_index: !!python/name:materialx.emoji.twemoji emoji_generator: !!python/name:materialx.emoji.to_svg - pymdownx.highlight - pymdownx.inlinehilite - pymdownx.keys - pymdownx.magiclink: repo_url_shorthand: true user: squidfunk repo: mkdocs-material - pymdownx.mark - pymdownx.smartsymbols - pymdownx.snippets: check_paths: true - pymdownx.superfences - pymdownx.tabbed - pymdownx.tasklist: custom_checkbox: true - pymdownx.tilde # Page Tree nav: - Intro : index.md Create index.md \u00b6 Create docs folder, add your index.md there. mkdir docs echo \"### Testing\" > docs/index.md git add docs/index.md git commit -m \"adding landing page\" git push Build site \u00b6 mkdocs can now be used to render .md to HTML mkdocs build INFO - Cleaning site directory INFO - Building documentation to directory: /Users/$USER/Documents/GitRepos/parkit/site INFO - Documentation built in 0.32 seconds The above command creates a local site folder which is the root of your \"to-be-hosted\" website. You can now open the HTMLs in the site folder locally to ensure that that HTML is as per you liking. If not, then you can make edits to the .md files and rebuild the site. NOTE : You do not want to push the site folder back to GH and hence it needs to be added to .gitignore file: echo \"**/site/*\" > .gitignore git add .gitignore git commit -m \"adding .gitignore\" git push Deploy site \u00b6 The following command with auto-create a gh-pages branch (if it does not exist) and copy the contents of the site folder to the root of that branch. It will also provide you the URL to your newly created website. mkdocs gh-deploy INFO - Cleaning site directory INFO - Building documentation to directory: /Users/kopardevn/Documents/GitRepos/xyz/site INFO - Documentation built in 0.34 seconds WARNING - Version check skipped: No version specified in previous deployment. INFO - Copying '/Users/kopardevn/Documents/GitRepos/xyz/site' to 'gh-pages' branch and pushing to GitHub. Enumerating objects: 51, done. Counting objects: 100(51/51), done. Delta compression using up to 16 threads Compressing objects: 100(47/47), done. Writing objects: 100(51/51), 441.71 KiB | 4.29 MiB/s, done. Total 51 (delta 4), reused 0 (delta 0), pack-reused 0 remote: Resolving deltas: 100(4/4), done. remote: remote: Create a pull request for 'gh-pages' on GitHub by visiting: remote: https://github.com/CCBR/xyz/pull/new/gh-pages remote: To https://github.com/CCBR/xyz.git * [new branch] gh-pages -> gh-pages INFO - Your documentation should shortly be available at: https://CCBR.github.io/xyz/ Now if you point your web browser to the URL from gh-deploy command (IE https://CCBR.github.io/xyz/) you will see your HTML hosted on GitHub. After creating your docs, the cookiecutter template includes a GitHub action which will automatically perform the above tasks whenever a push is performed to the main branch. Add to the GitHub page \u00b6 Go to the main GitHub page of your repository On the top right select the gear icon next to About Under Website , select Use your GitHub Pages website . Select Save Changes","title":"Creating your Documentation"},{"location":"GitHub/basic_docs/#github-basics-documentation","text":"GitHub Pages is quick and easy way to build static websites for your GitHub repositories. Essentially, you write pages in Markdown which are then rendered to HTML and hosted on GitHub, free of cost! CCBR has used GitHub pages to provide extensive, legible and organized documentation for our pipelines. Examples are included below: CARLISLE Pipeliner RNA-seek Mkdocs is the with documentation tool preferred, with the Material theme, for most of the CCBR GitHub Pages websites.","title":"GitHub Basics: Documentation"},{"location":"GitHub/basic_docs/#install-mkdocs-themes","text":"mkdocs and the Material for mkdocs theme can be installed using the following: pip install --upgrade pip pip install mkdocs pip install mkdocs-material Also install other common dependencies: pip install mkdocs-pymdownx-material-extras pip install mkdocs-git-revision-date-localized-plugin pip install mkdocs-git-revision-date-plugin pip install mkdocs-minify-plugin","title":"Install MkDocs, themes"},{"location":"GitHub/basic_docs/#conventions","text":"Generally, for GitHub repos with GitHub pages: The repository needs to be public (not private) The main/master branch has the markdown documents under a docs folder at the root level Rendered HTMLs are hosted under a gh-pages branch at root level","title":"Conventions"},{"location":"GitHub/basic_docs/#create-website","text":"The following steps can be followed to build your first website","title":"Create website"},{"location":"GitHub/basic_docs/#add-mkdocsyaml","text":"mkdocs.yaml needs to be added to the root of the master branch. A template of this file is available in the cookiecutter template. git clone https://github.com/CCBR/xyz.git cd xyz vi mkdocs.yaml git add mkdocs.yaml git commit -m \"adding mkdocs.yaml\" git push Here is a sample mkdocs.yaml : # Project Information site_name: CCBR How Tos site_author: Vishal Koparde, Ph.D. site_description: >- The **DEVIL** is in the **DETAILS**. Step-by-step detailed How To Guides for data management and other CCBR-relevant tasks. # Repository repo_name: CCBR/HowTos repo_url: https://github.com/CCBR/HowTos edit_uri: https://github.com/CCBR/HowTos/edit/main/docs/ # Copyright copyright: Copyright &copy; 2023 CCBR # Configuration theme: name: material features: - navigation.tabs - navigation.top - navigation.indexes - toc.integrate palette: - scheme: default primary: indigo accent: indigo toggle: icon: material/toggle-switch-off-outline name: Switch to dark mode - scheme: slate primary: red accent: red toggle: icon: material/toggle-switch name: Switch to light mode logo: assets/images/doc-book.svg favicon: assets/images/favicon.png # Plugins plugins: - search - git-revision-date - minify: minify_html: true # Customization extra: social: - icon: fontawesome/solid/users link: http://bioinformatics.cancer.gov - icon: fontawesome/brands/github link: https://github.com/CCRGeneticsBranch - icon: fontawesome/brands/docker link: https://hub.docker.com/orgs/nciccbr/repositories version: provider: mike # Extensions markdown_extensions: - markdown.extensions.admonition - markdown.extensions.attr_list - markdown.extensions.def_list - markdown.extensions.footnotes - markdown.extensions.meta - markdown.extensions.toc: permalink: true - pymdownx.arithmatex: generic: true - pymdownx.betterem: smart_enable: all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji: emoji_index: !!python/name:materialx.emoji.twemoji emoji_generator: !!python/name:materialx.emoji.to_svg - pymdownx.highlight - pymdownx.inlinehilite - pymdownx.keys - pymdownx.magiclink: repo_url_shorthand: true user: squidfunk repo: mkdocs-material - pymdownx.mark - pymdownx.smartsymbols - pymdownx.snippets: check_paths: true - pymdownx.superfences - pymdownx.tabbed - pymdownx.tasklist: custom_checkbox: true - pymdownx.tilde # Page Tree nav: - Intro : index.md","title":"Add mkdocs.yaml"},{"location":"GitHub/basic_docs/#create-indexmd","text":"Create docs folder, add your index.md there. mkdir docs echo \"### Testing\" > docs/index.md git add docs/index.md git commit -m \"adding landing page\" git push","title":"Create index.md"},{"location":"GitHub/basic_docs/#build-site","text":"mkdocs can now be used to render .md to HTML mkdocs build INFO - Cleaning site directory INFO - Building documentation to directory: /Users/$USER/Documents/GitRepos/parkit/site INFO - Documentation built in 0.32 seconds The above command creates a local site folder which is the root of your \"to-be-hosted\" website. You can now open the HTMLs in the site folder locally to ensure that that HTML is as per you liking. If not, then you can make edits to the .md files and rebuild the site. NOTE : You do not want to push the site folder back to GH and hence it needs to be added to .gitignore file: echo \"**/site/*\" > .gitignore git add .gitignore git commit -m \"adding .gitignore\" git push","title":"Build site"},{"location":"GitHub/basic_docs/#deploy-site","text":"The following command with auto-create a gh-pages branch (if it does not exist) and copy the contents of the site folder to the root of that branch. It will also provide you the URL to your newly created website. mkdocs gh-deploy INFO - Cleaning site directory INFO - Building documentation to directory: /Users/kopardevn/Documents/GitRepos/xyz/site INFO - Documentation built in 0.34 seconds WARNING - Version check skipped: No version specified in previous deployment. INFO - Copying '/Users/kopardevn/Documents/GitRepos/xyz/site' to 'gh-pages' branch and pushing to GitHub. Enumerating objects: 51, done. Counting objects: 100(51/51), done. Delta compression using up to 16 threads Compressing objects: 100(47/47), done. Writing objects: 100(51/51), 441.71 KiB | 4.29 MiB/s, done. Total 51 (delta 4), reused 0 (delta 0), pack-reused 0 remote: Resolving deltas: 100(4/4), done. remote: remote: Create a pull request for 'gh-pages' on GitHub by visiting: remote: https://github.com/CCBR/xyz/pull/new/gh-pages remote: To https://github.com/CCBR/xyz.git * [new branch] gh-pages -> gh-pages INFO - Your documentation should shortly be available at: https://CCBR.github.io/xyz/ Now if you point your web browser to the URL from gh-deploy command (IE https://CCBR.github.io/xyz/) you will see your HTML hosted on GitHub. After creating your docs, the cookiecutter template includes a GitHub action which will automatically perform the above tasks whenever a push is performed to the main branch.","title":"Deploy site"},{"location":"GitHub/basic_docs/#add-to-the-github-page","text":"Go to the main GitHub page of your repository On the top right select the gear icon next to About Under Website , select Use your GitHub Pages website . Select Save Changes","title":"Add to the GitHub page"},{"location":"GitHub/basic_repo/","text":"GitHub Basics: Repository \u00b6 Repository Location \u00b6 All CCBR developed pipelines, and techdev efforts should be created under CCBR's GitHub Org Account and all CCBR team members should have a minimal of read-only permission to the repository. Use of CookieCutter Templates \u00b6 All CCBR developed pipelines should be created from the appropriate templates: Nextflow Pipelines: https://github.com/CCBR/CCBR_NextflowTemplate Snakemake Pipelines: https://github.com/CCBR/CCBR_SnakemakeTemplate TechDev Projects: https://github.com/CCBR/CCBR_TechDevTemplate Note: The above templates are themselves under active development! As we continue building a multitude of analysis pipelines, we keep expanding on the list of \"commonalities\" between these analysis pipelines which need to be added to the template itself. Hence, templates are updated from time-to-time. Creating a new repository \u00b6 To create a new repository on Github using gh cli , you can run the following command on Biowulf after you update the new repository name ( <ADD NEW REPO NAME> ) and the repository description ( <ADD REPO DESCRIPTION> ) commands below. Naming Nomenclature: - All Repositories: Do not remove the CCBR/ leading the repository name, as this will correctly place the repository under the CCBR organization account. - CCBR Projects: These should be named with the CCBR#. IE CCBR/CCBR1155 - CCBR Pipelines: These should be descriptive of the pipelines main process; acronyms are encouraged. IE CCBR/CARLISLE - TechDev projects: These should be descriptive of the techDev project to be performed and should begin with techdev_ . IE CCBR/techdev_peakcall_benchmarking 1) Creating a new project OR new pipeline repository \u00b6 gh repo create CCBR/<ADD NEW REPO NAME> \\ --template CCBR/CCBR_NextflowTemplate \\ --description \"<ADD REPO DESCRIPTION>\" \\ --public \\ --confirm gh repo create CCBR/<ADD NEW REPO NAME> \\ --template CCBR/CCBR_SnakemakeTemplate \\ --description \"<ADD REPO DESCRIPTION>\" \\ --public \\ --confirm 2) Creating a new techDev \u00b6 gh repo create CCBR/techdev_<ADD NEW REPO NAME> \\ --template CCBR/CCBR_TechDevTemplate \\ --description \"<ADD REPO DESCRIPTION>\" \\ --public \\ --confirm Once the repo is created, then you can clone a local copy of the new repository: gh repo clone CCBR/<reponame>.git Minimal helper components \u00b6 If you start from one of the above templates, you'll have these files already. However, if you're updating an established repository, you may need to add some of these manually. CHANGELOG.md The changelog file should be at the top level of your repo. One exception is if your repo is an R package, it should be called NEWS.md instead. You can see an example changelog of a pipeline in active development here . VERSION The version file should be at the top level of your repo. If your repo is a Python package , it can be at at the package root instead, e.g. src/pkg/VERSION . If your repo is an R package , the version should be inside the DESCRIPTION file instead. Every time a PR is opened, the PR owner should add a line to the changelog to describe any user-facing changes such as new features added, bugs fixed, documentation updates, or performance improvements. You will also need a CLI command to print the version. The implementation will be different depending on your repo's language and structure. CITATION.cff The citation file must be at the top level of your repo. See template here . You will also need a CLI command to print the citation. The implementation will be different depending on your repo's language and structure. Pre-commit config files Pre-commit hooks provide an automated way to style code, draw attention to typos, and validate commit messages. Learn more about pre-commit here . These files can be customized depending on the needs of the repo. For example, you don't need the hook for formatting R code if your repo does not and will never contain any R code. .pre-commit-config.yaml .pretterignore .prettierrc .github/PULL_REQUEST_TEMPLATE.md The Pull Request template file helps developers and collaborators remember to write descriptive PR comments, link any relevant issues that the PR resolves, write unit tests, update the docs, and update the changelog. You can customize the Checklist in the template depending on the needs of the repo. Issue templates (optional) Issue templates help users know how they can best communicate with maintainers to report bugs and request new features. These are helpful but not required. .github/ISSUE_TEMPLATE/bug_report.yml .github/ISSUE_TEMPLATE/config.yml .github/ISSUE_TEMPLATE/feature_request.yml GitHub Actions GitHub Actions are automated workflows that run on GitHub's servers to execute unit tests, render documentation, build docker containers, etc. Most Actions need to be customized for each repo. Learn more about them here .","title":"Creating your GitHub Repo"},{"location":"GitHub/basic_repo/#github-basics-repository","text":"","title":"GitHub Basics: Repository"},{"location":"GitHub/basic_repo/#repository-location","text":"All CCBR developed pipelines, and techdev efforts should be created under CCBR's GitHub Org Account and all CCBR team members should have a minimal of read-only permission to the repository.","title":"Repository Location"},{"location":"GitHub/basic_repo/#use-of-cookiecutter-templates","text":"All CCBR developed pipelines should be created from the appropriate templates: Nextflow Pipelines: https://github.com/CCBR/CCBR_NextflowTemplate Snakemake Pipelines: https://github.com/CCBR/CCBR_SnakemakeTemplate TechDev Projects: https://github.com/CCBR/CCBR_TechDevTemplate Note: The above templates are themselves under active development! As we continue building a multitude of analysis pipelines, we keep expanding on the list of \"commonalities\" between these analysis pipelines which need to be added to the template itself. Hence, templates are updated from time-to-time.","title":"Use of CookieCutter Templates"},{"location":"GitHub/basic_repo/#creating-a-new-repository","text":"To create a new repository on Github using gh cli , you can run the following command on Biowulf after you update the new repository name ( <ADD NEW REPO NAME> ) and the repository description ( <ADD REPO DESCRIPTION> ) commands below. Naming Nomenclature: - All Repositories: Do not remove the CCBR/ leading the repository name, as this will correctly place the repository under the CCBR organization account. - CCBR Projects: These should be named with the CCBR#. IE CCBR/CCBR1155 - CCBR Pipelines: These should be descriptive of the pipelines main process; acronyms are encouraged. IE CCBR/CARLISLE - TechDev projects: These should be descriptive of the techDev project to be performed and should begin with techdev_ . IE CCBR/techdev_peakcall_benchmarking","title":"Creating a new repository"},{"location":"GitHub/basic_repo/#1-creating-a-new-project-or-new-pipeline-repository","text":"gh repo create CCBR/<ADD NEW REPO NAME> \\ --template CCBR/CCBR_NextflowTemplate \\ --description \"<ADD REPO DESCRIPTION>\" \\ --public \\ --confirm gh repo create CCBR/<ADD NEW REPO NAME> \\ --template CCBR/CCBR_SnakemakeTemplate \\ --description \"<ADD REPO DESCRIPTION>\" \\ --public \\ --confirm","title":"1) Creating a new project OR new pipeline repository"},{"location":"GitHub/basic_repo/#2-creating-a-new-techdev","text":"gh repo create CCBR/techdev_<ADD NEW REPO NAME> \\ --template CCBR/CCBR_TechDevTemplate \\ --description \"<ADD REPO DESCRIPTION>\" \\ --public \\ --confirm Once the repo is created, then you can clone a local copy of the new repository: gh repo clone CCBR/<reponame>.git","title":"2) Creating a new techDev"},{"location":"GitHub/basic_repo/#minimal-helper-components","text":"If you start from one of the above templates, you'll have these files already. However, if you're updating an established repository, you may need to add some of these manually. CHANGELOG.md The changelog file should be at the top level of your repo. One exception is if your repo is an R package, it should be called NEWS.md instead. You can see an example changelog of a pipeline in active development here . VERSION The version file should be at the top level of your repo. If your repo is a Python package , it can be at at the package root instead, e.g. src/pkg/VERSION . If your repo is an R package , the version should be inside the DESCRIPTION file instead. Every time a PR is opened, the PR owner should add a line to the changelog to describe any user-facing changes such as new features added, bugs fixed, documentation updates, or performance improvements. You will also need a CLI command to print the version. The implementation will be different depending on your repo's language and structure. CITATION.cff The citation file must be at the top level of your repo. See template here . You will also need a CLI command to print the citation. The implementation will be different depending on your repo's language and structure. Pre-commit config files Pre-commit hooks provide an automated way to style code, draw attention to typos, and validate commit messages. Learn more about pre-commit here . These files can be customized depending on the needs of the repo. For example, you don't need the hook for formatting R code if your repo does not and will never contain any R code. .pre-commit-config.yaml .pretterignore .prettierrc .github/PULL_REQUEST_TEMPLATE.md The Pull Request template file helps developers and collaborators remember to write descriptive PR comments, link any relevant issues that the PR resolves, write unit tests, update the docs, and update the changelog. You can customize the Checklist in the template depending on the needs of the repo. Issue templates (optional) Issue templates help users know how they can best communicate with maintainers to report bugs and request new features. These are helpful but not required. .github/ISSUE_TEMPLATE/bug_report.yml .github/ISSUE_TEMPLATE/config.yml .github/ISSUE_TEMPLATE/feature_request.yml GitHub Actions GitHub Actions are automated workflows that run on GitHub's servers to execute unit tests, render documentation, build docker containers, etc. Most Actions need to be customized for each repo. Learn more about them here .","title":"Minimal helper components"},{"location":"GitHub/howto_functions/","text":"GitHub HowTo: Basic Functions \u00b6 The following outlines basic GitHub function to push and pull from your repository. It also includes information on creating a new branch and deleting a branch. These commands should be used in line with guidance on GitHub Repo Management . Pushing local changes to remote \u00b6 Check which files have been changed. git status Stage files that need to be pushed git add <thisfile> git add <thatfile> Push changes to branch named new_feature git push origin new_feature Pulling remote changes to local \u00b6 Pull changes from branch new_feature into your branch old_feature git checkout old_feature git pull new_feature If you have non-compatible changes in the old_feature branch, there are two options: 1) ignore local changes and pull remote anyways. This will delete the changes you've made to your remote respository. git reset --hard git pull 2) temporarily stash changes away, pull and reapply changes after. git stash git pull git stash pop Creating a new branch \u00b6 This is a two step process. Create the branch locally git checkout -b <newbranch> Push the branch to remote git push -u origin <newbranch> OR git push -u origin HEAD This is a shortcut to push the current branch to a branch of the same name on origin and track it so that you don't need to specify origin HEAD in the future. Deleting branches \u00b6 Locally \u00b6 git branch -d <BranchName> on GitHub \u00b6 git push origin --delete <BranchName>","title":"Basic Commands"},{"location":"GitHub/howto_functions/#github-howto-basic-functions","text":"The following outlines basic GitHub function to push and pull from your repository. It also includes information on creating a new branch and deleting a branch. These commands should be used in line with guidance on GitHub Repo Management .","title":"GitHub HowTo: Basic Functions"},{"location":"GitHub/howto_functions/#pushing-local-changes-to-remote","text":"Check which files have been changed. git status Stage files that need to be pushed git add <thisfile> git add <thatfile> Push changes to branch named new_feature git push origin new_feature","title":"Pushing local changes to remote"},{"location":"GitHub/howto_functions/#pulling-remote-changes-to-local","text":"Pull changes from branch new_feature into your branch old_feature git checkout old_feature git pull new_feature If you have non-compatible changes in the old_feature branch, there are two options: 1) ignore local changes and pull remote anyways. This will delete the changes you've made to your remote respository. git reset --hard git pull 2) temporarily stash changes away, pull and reapply changes after. git stash git pull git stash pop","title":"Pulling remote changes to local"},{"location":"GitHub/howto_functions/#creating-a-new-branch","text":"This is a two step process. Create the branch locally git checkout -b <newbranch> Push the branch to remote git push -u origin <newbranch> OR git push -u origin HEAD This is a shortcut to push the current branch to a branch of the same name on origin and track it so that you don't need to specify origin HEAD in the future.","title":"Creating a new branch"},{"location":"GitHub/howto_functions/#deleting-branches","text":"","title":"Deleting branches"},{"location":"GitHub/howto_functions/#locally","text":"git branch -d <BranchName>","title":"Locally"},{"location":"GitHub/howto_functions/#on-github","text":"git push origin --delete <BranchName>","title":"on GitHub"},{"location":"GitHub/howto_precommit/","text":"GitHub HowTo: Pre-Commit \u00b6 Pre-commit should be added to all GitHub repositories on Biowulf and any clones created elsewhere to ensure cohesive and informative commit messages. After the creating the repository the following commands should be run in order to initialize the pre-commit hook, and establish the following requirements for all commit messages: Using Precommit \u00b6 Pre-commit has been installed as a module on Biowulf. Set up an interactive session, and follow the steps below. A pre-commit configuration file is needed, and can be copied from the CCBR template repo . # load module on biowulf module load precommit # CD into the GitHub repo cd <repo_name> # install precommit in the repo - this is the only time you need to do this, per local repo location pre-commit install # copy and update the precommit config touch .pre-commit.config Structure of Commit Messages \u00b6 Commits must follow the format listed below, as designated by Angular : <type>(<scope>): <subject> <body> <BLANK LINE> <footer> Type (REQUIRED) \u00b6 The type must be one of the following options: build: Changes that affect the build system or external dependencies ci: Changes to our CI configuration files and scripts docs: Documentation only changes feat: A new feature fix: A bug fix perf: A code change that improves performance refactor: A code change that neither fixes a bug nor adds a feature style: Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc) test: Adding missing tests or correcting existing tests Scope (OPTIONAL) \u00b6 The scope must be one of the following options: animations common compiler compiler-cli core elements forms http language-service platform-browser platform-browser-dynamic platform-server platform-webworker platform-webworker-dynamic router service-worker upgrade Subject (REQUIRED) \u00b6 The subject must be a succinct description of the change. It should follow the following rules: use the imperative, present tense: \"change\" not \"changed\" nor \"changes\" don't capitalize the first letter no dot (.) at the end Body (OPTIONAL) \u00b6 The body should include the motivation for the change and contrast this with previous behavior. It should follow the following rule: use the imperative, present tense: \"change\" not \"changed\" nor \"changes\" Footer (OPTIONAL) \u00b6 The footer should contain any information about Breaking Changes and is also the place to reference GitHub issues that this commit Closes. Breaking Changes should start with the word BREAKING CHANGE: with a space or two newlines. The rest of the commit message is then used for this. Closed bugs should be listed on a separate line in the footer from Breaking Changes , prefixed with \"Closes\" keyword (Closes #234 or Closes #123, #245, #992). Examples \u00b6 Below are some examples of properly formatted commit messages # example docs(changelog): update changelog to beta.5 # example fix(release): need to depend on latest rxjs and zone.js # example feat($browser): onUrlChange event (popstate/hashchange/polling) Added new event to $browser: - forward popstate event if available - forward hashchange event if popstate not available - do polling when neither popstate nor hashchange available Breaks $browser.onHashChange, which was removed (use onUrlChange instead) # example fix($compile): couple of unit tests for IE9 Older IEs serialize html uppercased, but IE9 does not... Would be better to expect case insensitive, unfortunately jasmine does not allow to user regexps for throw expectations. Closes #392 Breaks foo.bar api, foo.baz should be used instead","title":"Using Pre-Commit"},{"location":"GitHub/howto_precommit/#github-howto-pre-commit","text":"Pre-commit should be added to all GitHub repositories on Biowulf and any clones created elsewhere to ensure cohesive and informative commit messages. After the creating the repository the following commands should be run in order to initialize the pre-commit hook, and establish the following requirements for all commit messages:","title":"GitHub HowTo: Pre-Commit"},{"location":"GitHub/howto_precommit/#using-precommit","text":"Pre-commit has been installed as a module on Biowulf. Set up an interactive session, and follow the steps below. A pre-commit configuration file is needed, and can be copied from the CCBR template repo . # load module on biowulf module load precommit # CD into the GitHub repo cd <repo_name> # install precommit in the repo - this is the only time you need to do this, per local repo location pre-commit install # copy and update the precommit config touch .pre-commit.config","title":"Using Precommit"},{"location":"GitHub/howto_precommit/#structure-of-commit-messages","text":"Commits must follow the format listed below, as designated by Angular : <type>(<scope>): <subject> <body> <BLANK LINE> <footer>","title":"Structure of Commit Messages"},{"location":"GitHub/howto_precommit/#type-required","text":"The type must be one of the following options: build: Changes that affect the build system or external dependencies ci: Changes to our CI configuration files and scripts docs: Documentation only changes feat: A new feature fix: A bug fix perf: A code change that improves performance refactor: A code change that neither fixes a bug nor adds a feature style: Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc) test: Adding missing tests or correcting existing tests","title":"Type (REQUIRED)"},{"location":"GitHub/howto_precommit/#scope-optional","text":"The scope must be one of the following options: animations common compiler compiler-cli core elements forms http language-service platform-browser platform-browser-dynamic platform-server platform-webworker platform-webworker-dynamic router service-worker upgrade","title":"Scope (OPTIONAL)"},{"location":"GitHub/howto_precommit/#subject-required","text":"The subject must be a succinct description of the change. It should follow the following rules: use the imperative, present tense: \"change\" not \"changed\" nor \"changes\" don't capitalize the first letter no dot (.) at the end","title":"Subject (REQUIRED)"},{"location":"GitHub/howto_precommit/#body-optional","text":"The body should include the motivation for the change and contrast this with previous behavior. It should follow the following rule: use the imperative, present tense: \"change\" not \"changed\" nor \"changes\"","title":"Body (OPTIONAL)"},{"location":"GitHub/howto_precommit/#footer-optional","text":"The footer should contain any information about Breaking Changes and is also the place to reference GitHub issues that this commit Closes. Breaking Changes should start with the word BREAKING CHANGE: with a space or two newlines. The rest of the commit message is then used for this. Closed bugs should be listed on a separate line in the footer from Breaking Changes , prefixed with \"Closes\" keyword (Closes #234 or Closes #123, #245, #992).","title":"Footer (OPTIONAL)"},{"location":"GitHub/howto_precommit/#examples","text":"Below are some examples of properly formatted commit messages # example docs(changelog): update changelog to beta.5 # example fix(release): need to depend on latest rxjs and zone.js # example feat($browser): onUrlChange event (popstate/hashchange/polling) Added new event to $browser: - forward popstate event if available - forward hashchange event if popstate not available - do polling when neither popstate nor hashchange available Breaks $browser.onHashChange, which was removed (use onUrlChange instead) # example fix($compile): couple of unit tests for IE9 Older IEs serialize html uppercased, but IE9 does not... Would be better to expect case insensitive, unfortunately jasmine does not allow to user regexps for throw expectations. Closes #392 Breaks foo.bar api, foo.baz should be used instead","title":"Examples"},{"location":"GitHub/howto_setup/","text":"GitHub Setup: Preparing the Environment \u00b6 Using GitHub CLI \u00b6 The gh is installed on Biowulf at /data/CCBR_Pipeliner/db/PipeDB/bin/gh_1.7.0_linux_amd64/bin/gh . You can run the following lines to edit your ~/.bashrc file to add gh to your $PATH : echo \"export PATH=$PATH:/data/CCBR_Pipeliner/db/PipeDB/bin/gh_1.7.0_linux_amd64/bin\" >> ~/.bashrc source ~/.bashrc Alternatively, you can use the git commands provided through a Biowulf module module load git Creating PAT for GH \u00b6 Personal Access Token (PAT) is required to access GitHub (GH) without having to authenticate by other means (like password) every single time. You will need gh cli installed on your laptop or use /data/CCBR_Pipeliner/db/PipeDB/bin/gh_1.7.0_linux_amd64/bin/gh on Biowulf, as described above. You can create a PAT by going here . Then you can copy the PAT and save it into a file on Biowulf (say ~/gh_token ). Next, you can run the following command to set everything up correctly on Biowulf (or your laptop) gh auth login --with-token < ~/git_token Password-less Login \u00b6 If you hate to re-enter (username and) password every time you push/pull to/from github (or mkdocs gh-deploy), then it is totally worthwhile to spend a couple minutes to set up SSH keys for auto-authentication. The instructions to do this are available here .","title":"Preparing your environment"},{"location":"GitHub/howto_setup/#github-setup-preparing-the-environment","text":"","title":"GitHub Setup: Preparing the Environment"},{"location":"GitHub/howto_setup/#using-github-cli","text":"The gh is installed on Biowulf at /data/CCBR_Pipeliner/db/PipeDB/bin/gh_1.7.0_linux_amd64/bin/gh . You can run the following lines to edit your ~/.bashrc file to add gh to your $PATH : echo \"export PATH=$PATH:/data/CCBR_Pipeliner/db/PipeDB/bin/gh_1.7.0_linux_amd64/bin\" >> ~/.bashrc source ~/.bashrc Alternatively, you can use the git commands provided through a Biowulf module module load git","title":"Using GitHub CLI"},{"location":"GitHub/howto_setup/#creating-pat-for-gh","text":"Personal Access Token (PAT) is required to access GitHub (GH) without having to authenticate by other means (like password) every single time. You will need gh cli installed on your laptop or use /data/CCBR_Pipeliner/db/PipeDB/bin/gh_1.7.0_linux_amd64/bin/gh on Biowulf, as described above. You can create a PAT by going here . Then you can copy the PAT and save it into a file on Biowulf (say ~/gh_token ). Next, you can run the following command to set everything up correctly on Biowulf (or your laptop) gh auth login --with-token < ~/git_token","title":"Creating PAT for GH"},{"location":"GitHub/howto_setup/#password-less-login","text":"If you hate to re-enter (username and) password every time you push/pull to/from github (or mkdocs gh-deploy), then it is totally worthwhile to spend a couple minutes to set up SSH keys for auto-authentication. The instructions to do this are available here .","title":"Password-less Login"},{"location":"GitHub/overview/","text":"Overview of GitHub Topics \u00b6 Getting set-up and familiar with GitHub \u00b6 Preparing your environment : Describes how to create a PAT, add GH to your bash profile and use password-less login features Basic Commands : Describes basic functions to be used with this SOP Basics \u00b6 Creating your GitHub repo : Provides information on how to setup a GitHub repository under CCBR and the use of templates Creating your Documentation : Provides information on how to setup documentation under your repository; provided with all template repos GitHub Actions : Provides information for using GitHub Actions under your repository; provided with all template repos Best Practices \u00b6 CCBR Projects, new Pipelines TechDev","title":"Overview"},{"location":"GitHub/overview/#overview-of-github-topics","text":"","title":"Overview of GitHub Topics"},{"location":"GitHub/overview/#getting-set-up-and-familiar-with-github","text":"Preparing your environment : Describes how to create a PAT, add GH to your bash profile and use password-less login features Basic Commands : Describes basic functions to be used with this SOP","title":"Getting set-up and familiar with GitHub"},{"location":"GitHub/overview/#basics","text":"Creating your GitHub repo : Provides information on how to setup a GitHub repository under CCBR and the use of templates Creating your Documentation : Provides information on how to setup documentation under your repository; provided with all template repos GitHub Actions : Provides information for using GitHub Actions under your repository; provided with all template repos","title":"Basics"},{"location":"GitHub/overview/#best-practices","text":"CCBR Projects, new Pipelines TechDev","title":"Best Practices"},{"location":"GitHub/sop_projpipes/","text":"GitHub Best Practices: Projects and Pipelines \u00b6 Users should follow these links to learn more about setting up the repository, before reviewing the best practices below: Preparing your environment Basic Commands Creating your GitHub repo Creating your Documentation GitHub Actions Pipeline Documentation \u00b6 All pipelines should provide users with documentation for usage, test data, expected outputs, and troubleshooting information. Mkdocs is the recommended tool to perform this action, however, other tools may be utilized. The template's ( NextFlow , Snakemake ) were written for mkdocs, and provide basic yaml markdown files provided for this use. They should be edited according to the pipelines function and user needs. Examples of the requirements for each page are provided in the templates. Background Information on who the pipeline was developed for, and a statement if it's only been tested on Biowulf. Also include a workflow image to summarize the pipeline. Getting Started This should set the stage for all of the pipeline requirements. This should include the following pages: Introduction Setup Dependencies Login to the cluster Load an interactive session Preparing Files This should include the following pages: Configs Cluster Config Tools Config Config YAML User Parameters References Preparing Manifests Samples Manifest Running the Pipeline This should include all information about the various run commands provided within the pipeline. This should include the following pages: Pipeline Overview Commands explained Typical Workflow Expected Output This should include all pertinent information about output files, including extensions that differentiate files. Running Test Data This should walk the user through the steps of running the pipeline using test data. This should include the following pages: Getting Started About the test data Submit the test data Review outputs Repository Management \u00b6 Security settings \u00b6 AdminTeam should have a minimum of maintain role to the repo Both the develop and master branch must be protected (IE have to have a PR to be changed) Repo visibility should be set to Private or Internal and made Public only when required. CCBR Branch Strategy \u00b6 Branch Naming \u00b6 All repositories should follow the strategy outlined in the Creating your GitHub repo Branch Overview \u00b6 All repositories should include a minimum of two branches at any time: main ( or master ) dev Additional branches should be created as needed. These would include feature branches, developed using individual, feature specific addition and hotfix branches, developed using individual, bug specific fixes. Utilization of these branches should follow the documentation below. Strategy Outline \u00b6 We encourage the use of the Git Flow tools for some actions, available on Biowulf (module load gitflow). Our current branching strategy is based off of the Git Flow strategy shown below : ref:https://nvie.com/posts/a-successful-git-branching-model/ Master (named main or master) branch that contains the current release / tagged version of the pipeline merges from Dev branch or hotfix branch allowed merges require actions_master_branch pass from GitHub actions. See GitHub actions #4 for more information testing requirements for merge Develop (named dev or activeDev) branch that contains current dev merges from feature branch allowed merges require actions_dev_branch pass from GitHub actions. See GitHub actions #3 for more information testing requirements for merge Feature (named feature/unique_feature_name) branch to develop new features that branches off the develop branch recommended usages of git flow feature start unique_feature_name followed by git flow feature publish unique_feature_name no merges into this branch are expected Hotfix (named unique_hotfix_name) branches arise from a bug that has been discovered and must be resolved; it enables developers to keep working on their own changes on the develop branch while the bug is being fixed recommended usage of git flow hotfix start unique_hotfix_name no merges into this branch are expected \ud83d\udca1 Note While the git flow feature start command is recommended for feature branch creation, the git flow feature finish command is not. Using the finish command will automatically merge the feature branch into the dev branch without any testing and regardless of any divergence that may have occurred during feature development. General Workflow (Steps) \u00b6 Assuming that you have already cloned the repo and initiated git flow with git flow init Create a new feature and publish it git flow feature start unique_feature_name and then git flow feature publish unique_feature_name . The unique_feature_name will be created from the develop branch. Code normally, make frequent commits, push frequently to remote. These commits are added to the unique_feature_name branch. When you are ready to add your feature enhancements back to the develop branch, you may have to make sure that the develop branch has not marched forward while you were working on the unique_feature_name feature (This is possible as other may have added their PRs in the interim). If it has, then you may have to pull in changes made to develop branch into your unique_feature_name feature branch. This can be achieved using the GitHub web interface... merge develop \u2190 unique_feature_name . Resolve conflicts if any. Retest your unique_feature_name branch. Now send in a pull request using the GitHub web interface to pull your unique_feature_name into develop branch. Occasionally, we may create a new release branch from the develop branch and perform thorough E2E testing with fixes. Once, everything is working as expected, we pull the release branch back into develop and also push it to main with a version tag. Release, Tagged Nomenclature \u00b6 The following format of versioning should be followed: vX.Y.Z The following rules should be applies when determining the version release: X is major; non-backward compatible (dependent on the amount of changes; from dev) Y is minor; backwards compatible (dependent on the amount of changes; from dev) Z is patches; backwards compatible (bugs; hot fixes) Other notes: X,Y,Z must be numeric only All updates to the main (master) branch must be tagged and versioned using the parameters above Updates to the dev branch can be tagged, but should not be versioned If the pipeline is available locally (IE on Biowulf), version changes should be added for use Pipelines Test Data \u00b6 The following information is meant to outline test_data requirements for all pipelines, however, should be altered to fit the needs of the specific pipeline or project developed. Requirements \u00b6 Location of data Test data sets should be stored within a .test directory, as found in all templates. Documentation Review information on the documentation page, which will provide basic information on test data used within the project/pipeline. A README file should be created under the .test directory, to include the following information: Date of implementation Information on species (IE Homo Sapiens) and data type (IE RNA-Seq) Information on the references to be used (IE hg38) Metadata manifests required by the pipeline The source of the files Link to scripts used in created the partial test data Choosing a test data set At a minimum three test sets are recommended to be available: 1) Should include sub-sampled inputs, to test the pipelines functionality, and to be used as the tutorial test set . 2) Should include full-sample inputs, of high quality, to test the robustness of the pipelines resources 3) Should include full-sample inputs, of expected project-level quality, to test the robustness of the pipelines error handling Test data should come from a CCBR project or a publicly available source. Care should be taken when choosing test data sets, to ensure that the robustness of the pipeline will be tested, as well as the ability of the pipeline to handle both high and low quality data. Multiple test sets may need to be created to meet these goals. On BIOWULF, these test data files can be stored under /data/CCBR_Pipeliner/testdata for easy access by all users of the pipeline(s).","title":"Projects,Pipelines"},{"location":"GitHub/sop_projpipes/#github-best-practices-projects-and-pipelines","text":"Users should follow these links to learn more about setting up the repository, before reviewing the best practices below: Preparing your environment Basic Commands Creating your GitHub repo Creating your Documentation GitHub Actions","title":"GitHub Best Practices: Projects and Pipelines"},{"location":"GitHub/sop_projpipes/#pipeline-documentation","text":"All pipelines should provide users with documentation for usage, test data, expected outputs, and troubleshooting information. Mkdocs is the recommended tool to perform this action, however, other tools may be utilized. The template's ( NextFlow , Snakemake ) were written for mkdocs, and provide basic yaml markdown files provided for this use. They should be edited according to the pipelines function and user needs. Examples of the requirements for each page are provided in the templates. Background Information on who the pipeline was developed for, and a statement if it's only been tested on Biowulf. Also include a workflow image to summarize the pipeline. Getting Started This should set the stage for all of the pipeline requirements. This should include the following pages: Introduction Setup Dependencies Login to the cluster Load an interactive session Preparing Files This should include the following pages: Configs Cluster Config Tools Config Config YAML User Parameters References Preparing Manifests Samples Manifest Running the Pipeline This should include all information about the various run commands provided within the pipeline. This should include the following pages: Pipeline Overview Commands explained Typical Workflow Expected Output This should include all pertinent information about output files, including extensions that differentiate files. Running Test Data This should walk the user through the steps of running the pipeline using test data. This should include the following pages: Getting Started About the test data Submit the test data Review outputs","title":"Pipeline Documentation"},{"location":"GitHub/sop_projpipes/#repository-management","text":"","title":"Repository Management"},{"location":"GitHub/sop_projpipes/#security-settings","text":"AdminTeam should have a minimum of maintain role to the repo Both the develop and master branch must be protected (IE have to have a PR to be changed) Repo visibility should be set to Private or Internal and made Public only when required.","title":"Security settings"},{"location":"GitHub/sop_projpipes/#ccbr-branch-strategy","text":"","title":"CCBR Branch Strategy"},{"location":"GitHub/sop_projpipes/#branch-naming","text":"All repositories should follow the strategy outlined in the Creating your GitHub repo","title":"Branch Naming"},{"location":"GitHub/sop_projpipes/#branch-overview","text":"All repositories should include a minimum of two branches at any time: main ( or master ) dev Additional branches should be created as needed. These would include feature branches, developed using individual, feature specific addition and hotfix branches, developed using individual, bug specific fixes. Utilization of these branches should follow the documentation below.","title":"Branch Overview"},{"location":"GitHub/sop_projpipes/#strategy-outline","text":"We encourage the use of the Git Flow tools for some actions, available on Biowulf (module load gitflow). Our current branching strategy is based off of the Git Flow strategy shown below : ref:https://nvie.com/posts/a-successful-git-branching-model/ Master (named main or master) branch that contains the current release / tagged version of the pipeline merges from Dev branch or hotfix branch allowed merges require actions_master_branch pass from GitHub actions. See GitHub actions #4 for more information testing requirements for merge Develop (named dev or activeDev) branch that contains current dev merges from feature branch allowed merges require actions_dev_branch pass from GitHub actions. See GitHub actions #3 for more information testing requirements for merge Feature (named feature/unique_feature_name) branch to develop new features that branches off the develop branch recommended usages of git flow feature start unique_feature_name followed by git flow feature publish unique_feature_name no merges into this branch are expected Hotfix (named unique_hotfix_name) branches arise from a bug that has been discovered and must be resolved; it enables developers to keep working on their own changes on the develop branch while the bug is being fixed recommended usage of git flow hotfix start unique_hotfix_name no merges into this branch are expected \ud83d\udca1 Note While the git flow feature start command is recommended for feature branch creation, the git flow feature finish command is not. Using the finish command will automatically merge the feature branch into the dev branch without any testing and regardless of any divergence that may have occurred during feature development.","title":"Strategy Outline"},{"location":"GitHub/sop_projpipes/#general-workflow-steps","text":"Assuming that you have already cloned the repo and initiated git flow with git flow init Create a new feature and publish it git flow feature start unique_feature_name and then git flow feature publish unique_feature_name . The unique_feature_name will be created from the develop branch. Code normally, make frequent commits, push frequently to remote. These commits are added to the unique_feature_name branch. When you are ready to add your feature enhancements back to the develop branch, you may have to make sure that the develop branch has not marched forward while you were working on the unique_feature_name feature (This is possible as other may have added their PRs in the interim). If it has, then you may have to pull in changes made to develop branch into your unique_feature_name feature branch. This can be achieved using the GitHub web interface... merge develop \u2190 unique_feature_name . Resolve conflicts if any. Retest your unique_feature_name branch. Now send in a pull request using the GitHub web interface to pull your unique_feature_name into develop branch. Occasionally, we may create a new release branch from the develop branch and perform thorough E2E testing with fixes. Once, everything is working as expected, we pull the release branch back into develop and also push it to main with a version tag.","title":"General Workflow (Steps)"},{"location":"GitHub/sop_projpipes/#release-tagged-nomenclature","text":"The following format of versioning should be followed: vX.Y.Z The following rules should be applies when determining the version release: X is major; non-backward compatible (dependent on the amount of changes; from dev) Y is minor; backwards compatible (dependent on the amount of changes; from dev) Z is patches; backwards compatible (bugs; hot fixes) Other notes: X,Y,Z must be numeric only All updates to the main (master) branch must be tagged and versioned using the parameters above Updates to the dev branch can be tagged, but should not be versioned If the pipeline is available locally (IE on Biowulf), version changes should be added for use","title":"Release, Tagged Nomenclature"},{"location":"GitHub/sop_projpipes/#pipelines-test-data","text":"The following information is meant to outline test_data requirements for all pipelines, however, should be altered to fit the needs of the specific pipeline or project developed.","title":"Pipelines Test Data"},{"location":"GitHub/sop_projpipes/#requirements","text":"Location of data Test data sets should be stored within a .test directory, as found in all templates. Documentation Review information on the documentation page, which will provide basic information on test data used within the project/pipeline. A README file should be created under the .test directory, to include the following information: Date of implementation Information on species (IE Homo Sapiens) and data type (IE RNA-Seq) Information on the references to be used (IE hg38) Metadata manifests required by the pipeline The source of the files Link to scripts used in created the partial test data Choosing a test data set At a minimum three test sets are recommended to be available: 1) Should include sub-sampled inputs, to test the pipelines functionality, and to be used as the tutorial test set . 2) Should include full-sample inputs, of high quality, to test the robustness of the pipelines resources 3) Should include full-sample inputs, of expected project-level quality, to test the robustness of the pipelines error handling Test data should come from a CCBR project or a publicly available source. Care should be taken when choosing test data sets, to ensure that the robustness of the pipeline will be tested, as well as the ability of the pipeline to handle both high and low quality data. Multiple test sets may need to be created to meet these goals. On BIOWULF, these test data files can be stored under /data/CCBR_Pipeliner/testdata for easy access by all users of the pipeline(s).","title":"Requirements"},{"location":"GitHub/sop_techdev/","text":"GitHub Best Practices \u00b6 Users should follow these links to learn more about setting up the repository, before reviewing the best practices below: Preparing your environment Basic Commands Creating your GitHub repo Creating your Documentation GitHub Actions TechDev Documentation \u00b6 All pipelines should provide users with: documentation for usage test data expected outputs and reports troubleshooting information Markdown pages can be hosted directly within the repo using GH Pages. Mkdocs is the recommended tool to perform this action, however, other tools may be utilized. The templates ( TechDev ) template's (written for mkdocs) provided have basic yaml markdown files provided for this use, and should be edited according to the pipelines function and user needs. Also, track blockers/hurdles using GitHub Issues. Overview Information on the goal of the TechDev project. Analysis 2.1. Background - Provide any relevant background to the project to be completed. This might include information on: - problem that was encountered leading to this techdev - new feature or tool developed to be benchmarked - relevant biological or statistical information needed to perform this analysis 2.2. Resources - This page should include any relevant tools that were used for testing. If these tools were loaded via Biowulf, include all version numbers. If they were installed locally, provide information on installation, source location, and any reference documents used. 2.3. Test Data - This will include information on the test data included within the project. Species information, source information, and references should be included. Any manipulation performed on the source samples should also be included. Manifest information should also be outlined. Provide location to toy dataset or real dataset or both. It is best to park these datasets at a commonly accessible location on Biowulf and share that location here. Also, make the location read-only to prevent accidental edits by other users. 2.4. Analysis Plan - Provide a summary of the plan of action. - If benchmarking tools, include the dataset used, where the source material was obtained, and all relevant metadata. - Include the location of any relevant config / data files used in this analysis - Provide information on the limits of the analysis - IE this was only tested on Biowulf, this was only performed in human samples, etc. 2.5. Results - What were the findings of this TechDev exercise? Include links to where intermediate files may be located. Include tables, plots, etc. This will serve as a permanent record of the TechDev efforts to be shared within the team and beyond. 2.6 Conclusions - Provide brief, concise conclusion drawn from the analysis, including any alterations being made to pipelines or analysis. Contributions Provide any names that contributed to this work. TechDev Repository Management \u00b6 Security settings \u00b6 Two members of CCBR (creator and one manager) should be granted full administrative privileges to the repository to ensure the source code can be accessed by other members, as needed Both the develop and master branch must be protected (IE have to have a PR to be changed) CCBR Branch Strategy \u00b6 Branch Naming \u00b6 All repositories should follow the strategy outlined in the Creating your GitHub repo Branch Overview \u00b6 All repositories should include a minimum of two branches at any time: main (master) dev Utilization of these branches should follow the documentation below. Branch Strategy \u00b6 We encourage the use of the Git Flow tools for some actions, available on Biowulf. Our current branching strategy is based off of the Git Flow strategy shown below : Master (named main or master) branch that contains the current release / tagged version of the pipeline merges from Dev branch or hotfix branch allowed merges require actions_master_branch pass from GitHub actions. See GitHub actions #4 for more information testing requirements for merge Develop (named dev or activeDev) branch that contains current dev merges from feature branch allowed merges require actions_dev_branch pass from GitHub actions. See GitHub actions #3 for more information testing requirements for merge TechDev Test Data \u00b6 Requirements \u00b6 Location of data Test data sets should be stored within a .test directory, as found in all templates. Documentation Review information on the documentation page, which will provide basic information on test data used within the project/pipeline. A README file should be created under the .test directory, to include the following information: Date of implementation Information on species (IE Homo Sapiens) and data type (IE RNA-Seq) Information on the references to be used (IE hg38) Metadata manifests required by the pipeline The source of the files Link to scripts used in created the partial test data Choosing a test data set Test data should come from a CCBR project or a a publicly available source. Care should be taken when choosing test data sets, to ensure that the data matches the goals of the techdev effort.","title":"TechDev"},{"location":"GitHub/sop_techdev/#github-best-practices","text":"Users should follow these links to learn more about setting up the repository, before reviewing the best practices below: Preparing your environment Basic Commands Creating your GitHub repo Creating your Documentation GitHub Actions","title":"GitHub Best Practices"},{"location":"GitHub/sop_techdev/#techdev-documentation","text":"All pipelines should provide users with: documentation for usage test data expected outputs and reports troubleshooting information Markdown pages can be hosted directly within the repo using GH Pages. Mkdocs is the recommended tool to perform this action, however, other tools may be utilized. The templates ( TechDev ) template's (written for mkdocs) provided have basic yaml markdown files provided for this use, and should be edited according to the pipelines function and user needs. Also, track blockers/hurdles using GitHub Issues. Overview Information on the goal of the TechDev project. Analysis 2.1. Background - Provide any relevant background to the project to be completed. This might include information on: - problem that was encountered leading to this techdev - new feature or tool developed to be benchmarked - relevant biological or statistical information needed to perform this analysis 2.2. Resources - This page should include any relevant tools that were used for testing. If these tools were loaded via Biowulf, include all version numbers. If they were installed locally, provide information on installation, source location, and any reference documents used. 2.3. Test Data - This will include information on the test data included within the project. Species information, source information, and references should be included. Any manipulation performed on the source samples should also be included. Manifest information should also be outlined. Provide location to toy dataset or real dataset or both. It is best to park these datasets at a commonly accessible location on Biowulf and share that location here. Also, make the location read-only to prevent accidental edits by other users. 2.4. Analysis Plan - Provide a summary of the plan of action. - If benchmarking tools, include the dataset used, where the source material was obtained, and all relevant metadata. - Include the location of any relevant config / data files used in this analysis - Provide information on the limits of the analysis - IE this was only tested on Biowulf, this was only performed in human samples, etc. 2.5. Results - What were the findings of this TechDev exercise? Include links to where intermediate files may be located. Include tables, plots, etc. This will serve as a permanent record of the TechDev efforts to be shared within the team and beyond. 2.6 Conclusions - Provide brief, concise conclusion drawn from the analysis, including any alterations being made to pipelines or analysis. Contributions Provide any names that contributed to this work.","title":"TechDev Documentation"},{"location":"GitHub/sop_techdev/#techdev-repository-management","text":"","title":"TechDev Repository Management"},{"location":"GitHub/sop_techdev/#security-settings","text":"Two members of CCBR (creator and one manager) should be granted full administrative privileges to the repository to ensure the source code can be accessed by other members, as needed Both the develop and master branch must be protected (IE have to have a PR to be changed)","title":"Security settings"},{"location":"GitHub/sop_techdev/#ccbr-branch-strategy","text":"","title":"CCBR Branch Strategy"},{"location":"GitHub/sop_techdev/#branch-naming","text":"All repositories should follow the strategy outlined in the Creating your GitHub repo","title":"Branch Naming"},{"location":"GitHub/sop_techdev/#branch-overview","text":"All repositories should include a minimum of two branches at any time: main (master) dev Utilization of these branches should follow the documentation below.","title":"Branch Overview"},{"location":"GitHub/sop_techdev/#branch-strategy","text":"We encourage the use of the Git Flow tools for some actions, available on Biowulf. Our current branching strategy is based off of the Git Flow strategy shown below : Master (named main or master) branch that contains the current release / tagged version of the pipeline merges from Dev branch or hotfix branch allowed merges require actions_master_branch pass from GitHub actions. See GitHub actions #4 for more information testing requirements for merge Develop (named dev or activeDev) branch that contains current dev merges from feature branch allowed merges require actions_dev_branch pass from GitHub actions. See GitHub actions #3 for more information testing requirements for merge","title":"Branch Strategy"},{"location":"GitHub/sop_techdev/#techdev-test-data","text":"","title":"TechDev Test Data"},{"location":"GitHub/sop_techdev/#requirements","text":"Location of data Test data sets should be stored within a .test directory, as found in all templates. Documentation Review information on the documentation page, which will provide basic information on test data used within the project/pipeline. A README file should be created under the .test directory, to include the following information: Date of implementation Information on species (IE Homo Sapiens) and data type (IE RNA-Seq) Information on the references to be used (IE hg38) Metadata manifests required by the pipeline The source of the files Link to scripts used in created the partial test data Choosing a test data set Test data should come from a CCBR project or a a publicly available source. Care should be taken when choosing test data sets, to ensure that the data matches the goals of the techdev effort.","title":"Requirements"},{"location":"HPCDME/setup/","text":"Background \u00b6 HPC_DME_APIs provides command line utilities or CLUs to interface with HPCDME. This document describes some of the initial setup steps to get the CLUs working on Biowulf . Setup steps: \u00b6 Clone repo: \u00b6 The repo can be cloned at a location accessible to you: cd /data/$USER/ git clone https://github.com/CBIIT/HPC_DME_APIs.git Create dirs, log files needed for HPCMDE \u00b6 mkdir -p /data/$USER/HPCDMELOG/tmp touch /data/$USER/HPCDMELOG/tmp/hpc-cli.log Copy properties template \u00b6 hpcdme.properties is the file that all CLUs look into for various parameters like authentication password, file size limits, number of CPUs, etc. Make a copy of the template provided and prepare it for customization. cd /data/$USER/HPC_DME_APIs/utils cp hpcdme.properties-sample hpcdme.properties Customize properties file \u00b6 Some of the parameters in this file have become obsolete over the course of time and are commmented out. Change paths and default values, as needed. NOTES Replace $USER with your actual username in the properties file. Bash variables will not be interpolated. Leave hpc.ssl.keystore.password=changeit as-is. changeit is not a variable but the actual password for our team. Be sure to set the proxy server URL as below ( hpc.server.proxy.url=10.1.200.75 ) when running on biowulf/helix. #HPC DME Server URL #Production server settings hpc.server.url=https://hpcdmeapi.nci.nih.gov:8080 hpc.ssl.keystore.path=hpc-client/keystore/keystore-prod.jks #hpc.ssl.keystore.password=hpcdmncif hpc.ssl.keystore.password=changeit #UAT server settings #hpc.server.url=https://fr-s-hpcdm-uat-p.ncifcrf.gov:7738/hpc-server #hpc.ssl.keystore.path=hpc-client/keystore/keystore-uat.jks #hpc.ssl.keystore.password=hpc-server-store-pwd #Proxy Settings hpc.server.proxy.url=10.1.200.75 hpc.server.proxy.port=3128 hpc.user=$USER #Globus settings #default globus endpoint to be used in registration and download hpc.globus.user=$USER hpc.default.globus.endpoint=ea6c8fd6-4810-11e8-8ee3-0a6d4e044368 #Log files directory hpc.error-log.dir=/data/$USER/HPCDMELOG/tmp ###HPC CLI Logging START#### #ERROR, WARN, INFO, DEBUG hpc.log.level=ERROR hpc.log.file=/data/$USER/HPCDMELOG/tmp/hpc-cli.log ###HPC CLI Logging END#### ############################################################################# # Please use caution changing following properties. They don't change usually ############################################################################# #hpc.collection.service=collection #hpc.dataobject.service=dataObject #Log files directory #hpc.error-log.dir=. #Number of thread to run data file import from a CSV file hpc.job.thread.count=1 upload.buffer.size=10000000 #Retry count and backoff period for registerFromFilePath (Fixed backoff) hpc.retry.max.attempts=3 #hpc.retry.backoff.period=5000 #Multi-part upload thread pool, threshold and part size configuration #hpc.multipart.threadpoolsize=10 #hpc.multipart.threshold=1074790400 #hpc.multipart.chunksize=1073741824 #globus.nexus.url=nexus.api.globusonline.org #globus.url=www.globusonline.org #HPC DME Login token file location hpc.login.token=tokens/hpcdme-auth.txt #Globus Login token file location #hpc.globus.login.token=tokens/globus-auth.txt #validate.md5.checksum=false # JAR version #hpc.jar.version=hpc-cli-1.4.0.jar NOTE : The current java version used is: bash java -version openjdk version \"1.8.0_181\" OpenJDK Runtime Environment (build 1.8.0_181-b13) OpenJDK 64-Bit Server VM (build 25.181-b13, mixed mode) Edit ~/.bashrc \u00b6 Add the CLUs to PATH by adding the following to ~/.bashrc file # export environment variable HPC_DM_UTILS pointing to directory where # HPC DME client utilities are, then source functions script in there export HPC_DM_UTILS=/data/$USER/HPC_DME_APIs/utils source $HPC_DM_UTILS/functions Next, source it source ~/.bashrc Generate token \u00b6 Now, you are all set to generate a token. This prevents from re-entering your password everytime. dm_generate_token If the token generation takes longer than 45 seconds, check the connection: ping hpcdmeapi.nci.nih.gov If the connection responds, try to export the following proxy, and then re-run the dm_generate_tokens command : export https_proxy=http://dtn01-e0:3128 Done! You are now all set to use CLUs. References and Links \u00b6 HPC_DME_APIs repo User guides Wiki pages Yuri Dinh","title":"Setup"},{"location":"HPCDME/setup/#background","text":"HPC_DME_APIs provides command line utilities or CLUs to interface with HPCDME. This document describes some of the initial setup steps to get the CLUs working on Biowulf .","title":"Background"},{"location":"HPCDME/setup/#setup-steps","text":"","title":"Setup steps:"},{"location":"HPCDME/setup/#clone-repo","text":"The repo can be cloned at a location accessible to you: cd /data/$USER/ git clone https://github.com/CBIIT/HPC_DME_APIs.git","title":"Clone repo:"},{"location":"HPCDME/setup/#create-dirs-log-files-needed-for-hpcmde","text":"mkdir -p /data/$USER/HPCDMELOG/tmp touch /data/$USER/HPCDMELOG/tmp/hpc-cli.log","title":"Create dirs, log files needed for HPCMDE"},{"location":"HPCDME/setup/#copy-properties-template","text":"hpcdme.properties is the file that all CLUs look into for various parameters like authentication password, file size limits, number of CPUs, etc. Make a copy of the template provided and prepare it for customization. cd /data/$USER/HPC_DME_APIs/utils cp hpcdme.properties-sample hpcdme.properties","title":"Copy properties template"},{"location":"HPCDME/setup/#customize-properties-file","text":"Some of the parameters in this file have become obsolete over the course of time and are commmented out. Change paths and default values, as needed. NOTES Replace $USER with your actual username in the properties file. Bash variables will not be interpolated. Leave hpc.ssl.keystore.password=changeit as-is. changeit is not a variable but the actual password for our team. Be sure to set the proxy server URL as below ( hpc.server.proxy.url=10.1.200.75 ) when running on biowulf/helix. #HPC DME Server URL #Production server settings hpc.server.url=https://hpcdmeapi.nci.nih.gov:8080 hpc.ssl.keystore.path=hpc-client/keystore/keystore-prod.jks #hpc.ssl.keystore.password=hpcdmncif hpc.ssl.keystore.password=changeit #UAT server settings #hpc.server.url=https://fr-s-hpcdm-uat-p.ncifcrf.gov:7738/hpc-server #hpc.ssl.keystore.path=hpc-client/keystore/keystore-uat.jks #hpc.ssl.keystore.password=hpc-server-store-pwd #Proxy Settings hpc.server.proxy.url=10.1.200.75 hpc.server.proxy.port=3128 hpc.user=$USER #Globus settings #default globus endpoint to be used in registration and download hpc.globus.user=$USER hpc.default.globus.endpoint=ea6c8fd6-4810-11e8-8ee3-0a6d4e044368 #Log files directory hpc.error-log.dir=/data/$USER/HPCDMELOG/tmp ###HPC CLI Logging START#### #ERROR, WARN, INFO, DEBUG hpc.log.level=ERROR hpc.log.file=/data/$USER/HPCDMELOG/tmp/hpc-cli.log ###HPC CLI Logging END#### ############################################################################# # Please use caution changing following properties. They don't change usually ############################################################################# #hpc.collection.service=collection #hpc.dataobject.service=dataObject #Log files directory #hpc.error-log.dir=. #Number of thread to run data file import from a CSV file hpc.job.thread.count=1 upload.buffer.size=10000000 #Retry count and backoff period for registerFromFilePath (Fixed backoff) hpc.retry.max.attempts=3 #hpc.retry.backoff.period=5000 #Multi-part upload thread pool, threshold and part size configuration #hpc.multipart.threadpoolsize=10 #hpc.multipart.threshold=1074790400 #hpc.multipart.chunksize=1073741824 #globus.nexus.url=nexus.api.globusonline.org #globus.url=www.globusonline.org #HPC DME Login token file location hpc.login.token=tokens/hpcdme-auth.txt #Globus Login token file location #hpc.globus.login.token=tokens/globus-auth.txt #validate.md5.checksum=false # JAR version #hpc.jar.version=hpc-cli-1.4.0.jar NOTE : The current java version used is: bash java -version openjdk version \"1.8.0_181\" OpenJDK Runtime Environment (build 1.8.0_181-b13) OpenJDK 64-Bit Server VM (build 25.181-b13, mixed mode)","title":"Customize properties file"},{"location":"HPCDME/setup/#edit-bashrc","text":"Add the CLUs to PATH by adding the following to ~/.bashrc file # export environment variable HPC_DM_UTILS pointing to directory where # HPC DME client utilities are, then source functions script in there export HPC_DM_UTILS=/data/$USER/HPC_DME_APIs/utils source $HPC_DM_UTILS/functions Next, source it source ~/.bashrc","title":"Edit ~/.bashrc"},{"location":"HPCDME/setup/#generate-token","text":"Now, you are all set to generate a token. This prevents from re-entering your password everytime. dm_generate_token If the token generation takes longer than 45 seconds, check the connection: ping hpcdmeapi.nci.nih.gov If the connection responds, try to export the following proxy, and then re-run the dm_generate_tokens command : export https_proxy=http://dtn01-e0:3128 Done! You are now all set to use CLUs.","title":"Generate token"},{"location":"HPCDME/setup/#references-and-links","text":"HPC_DME_APIs repo User guides Wiki pages Yuri Dinh","title":"References and Links"},{"location":"HPCDME/transfer/","text":"File Transfers from Biowulf \u00b6 \ud83d\uded1 STOP : dm_register_dataobject_multipart does not work via SLURM. Hence, file transfers are NOT working when initiated from BIOWULF. We recommend transferring files from Helix. Background \u00b6 Rawdata or Project folders from Biowulf can be parked at a secure location after the analysis has reached an endpoint. Traditionally, CCBR analysts have been using GridFTP Globus Archive for doing this. But, this Globus Archive has been running relatively full lately and it is hard to estimate how much space is left there as the volume is shared among multiple groups. parkit \u00b6 parkit is designed to assist analysts in archiving project data from the NIH's Biowulf/Helix systems to the HPC-DME storage platform. It provides functionalities to package and store data such as raw FastQ files or processed data from bioinformatics pipelines. Users can automatically: create tarballs of their data (including .filelist and .md5sum files), generate metadata, create collections on HPC-DME, and deposit tar files into the system for long-term storage. parkit also features comprehensive workflows that support both folder-based and tarball-based archiving. These workflows are integrated with the SLURM job scheduler, enabling efficient execution of archival tasks on the Biowulf HPC cluster. This integration ensures that bioinformatics project data is securely archived and well-organized, allowing for seamless long-term storage. \u2757 NOTE : HPC DME API CLUs should already be setup as per these instructions in order to use parkit \u2757 NOTE : HPC_DM_UTILS environment variable should be set to point to the utils folder under the HPC_DME_APIs repo setup. Please see these instructions. projark is the preferred parkit command to completely archive an entire folder as a tarball on HPCDME using SLURM. projark usage \u00b6 load conda env \u00b6 # source conda . \"/data/CCBR_Pipeliner/db/PipeDB/Conda/etc/profile.d/conda.sh\" # activate parkit or parkit_dev environment conda activate parkit # check version of parkit parkit --version projark --version Expected sample output v2.0.2-dev projark is using the following parkit version: v2.0.2-dev projark help \u00b6 projark --help Expected sample output usage: projark [-h] --folder FOLDER --projectnumber PROJECTNUMBER [--executor EXECUTOR] [--rawdata] [--cleanup] Wrapper for folder2hpcdme for quick CCBR project archiving! options: -h, --help show this help message and exit --folder FOLDER Input folder path to archive --projectnumber PROJECTNUMBER CCBR project number.. destination will be /CCBR_Archive/GRIDFTP/Project_CCBR-<projectnumber> --executor EXECUTOR slurm or local --rawdata If tarball is rawdata and needs to go under folder Rawdata --cleanup post transfer step to delete local files projark testing \u00b6 get dummy data \u00b6 # make a tmp folder mkdir -p /data/$USER/parkit_tmp # copy dummy project folder into the tmp folder cp -r /data/CCBR/projects/CCBR-12345 /data/$USER/parkit_tmp/CCBR-12345-$USER # check if HPC_DM_UTILS has been set echo $HPC_DM_UTILS run projark \u00b6 projark --folder /data/$USER/parkit_tmp/CCBR-12345-$USER --projectnumber 12345-$USER --executor local Expected sample output SOURCE_CONDA_CMD is set to: . \"/data/CCBR_Pipeliner/db/PipeDB/Conda/etc/profile.d/conda.sh\" HPC_DM_UTILS is set to: /data/kopardevn/GitRepos/HPC_DME_APIs/utils parkit_folder2hpcdme --folder \"/data/$USER/parkit_tmp/CCBR-12345-$USER\" --dest \"/CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\" --projecttitle \"CCBR-12345-kopardevn\" --projectdesc \"CCBR-12345-kopardevn\" --executor \"local\" --hpcdmutilspath /data/kopardevn/GitRepos/HPC_DME_APIs/utils --makereadme ################ Running createtar ############################# parkit createtar --folder \"/data/$USER/parkit_tmp/CCBR-12345-$USER\" tar cvf /data/$USER/parkit_tmp/CCBR-12345-$USER.tar /data/$USER/parkit_tmp/CCBR-12345-$USER > /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist createmetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar file was created! createmetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist file was created! createmetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.md5 file was created! createmetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist.md5 file was created! ################################################################ ############ Running createemptycollection ###################### parkit createemptycollection --dest \"/CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\" --projectdesc \"CCBR-12345-kopardevn\" --projecttitle \"CCBR-12345-kopardevn\" module load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_collection /dev/shm/a213dedc-9363-44ec-8a7a-d29f2345a0b5.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn cat /dev/shm/a213dedc-9363-44ec-8a7a-d29f2345a0b5.json && rm -f /dev/shm/a213dedc-9363-44ec-8a7a-d29f2345a0b5.json module load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_collection /dev/shm/cabf7826-81b5-4b6a-addd-09fbcf279591.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn/Analysis module load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_collection /dev/shm/cabf7826-81b5-4b6a-addd-09fbcf279591.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn/Rawdata cat /dev/shm/cabf7826-81b5-4b6a-addd-09fbcf279591.json && rm -f /dev/shm/cabf7826-81b5-4b6a-addd-09fbcf279591.json ################################################################ ########### Running createmetadata ############################## parkit createmetadata --tarball \"/data/$USER/parkit_tmp/CCBR-12345-$USER.tar\" --dest \"/CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\" createmetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.metadata.json file was created! createmetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist.metadata.json file was created! ################################################################ ############# Running deposittar ############################### parkit deposittar --tarball \"/data/$USER/parkit_tmp/CCBR-12345-$USER.tar\" --dest \"/CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\" module load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_dataobject /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist.metadata.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn/Analysis/CCBR-12345.tar.filelist /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist module load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_dataobject_multipart /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.metadata.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn/Analysis/CCBR-12345.tar /data/$USER/parkit_tmp/CCBR-12345-$USER.tar ################################################################ \u2757 NOTE : remove --executor local from the command when running on real data (not test data) to submit jobs through SLURM \u2757 NOTE : add --rawdata when folder contains raw fastqs verify transfer \u00b6 Transfer can be verified by logging into HPC DME web interface . cleanup \u00b6 Delete unwanted collection from HPC DME. # load java module load java # load dm_ commands source $HPC_DM_UTILS/functions # delete collection recursively dm_delete_collection -r /CCBR_Archive/GRIDFTP/Project_CCBR-12345-$USER Expected sample output Reading properties from /data/kopardevn/GitRepos/HPC_DME_APIs/utils/hpcdme.properties WARNING: You have requested recursive delete of the collection. This will delete all files and sub-collections within it recursively. Are you sure you want to proceed? (Y/N): Y Would you like to see the list of files to delete ? N The collection /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn and all files and sub-collections within it will be recursively deleted. Proceed with deletion ? (Y/N): Y Executing: https://hpcdmeapi.nci.nih.gov:8080/collection Wrote results into /data/kopardevn/HPCDMELOG/tmp/getCollections_Records20241010.txt Cmd process Completed Oct 10, 2024 4:43:09 PM org.springframework.shell.core.AbstractShell handleExecutionResult INFO: CLI_SUCCESS \u26a0\ufe0f Reach out to Vishal Koparde in case you run into issues.","title":"Biowulf2HPCDME"},{"location":"HPCDME/transfer/#file-transfers-from-biowulf","text":"\ud83d\uded1 STOP : dm_register_dataobject_multipart does not work via SLURM. Hence, file transfers are NOT working when initiated from BIOWULF. We recommend transferring files from Helix.","title":"File Transfers from Biowulf"},{"location":"HPCDME/transfer/#background","text":"Rawdata or Project folders from Biowulf can be parked at a secure location after the analysis has reached an endpoint. Traditionally, CCBR analysts have been using GridFTP Globus Archive for doing this. But, this Globus Archive has been running relatively full lately and it is hard to estimate how much space is left there as the volume is shared among multiple groups.","title":"Background"},{"location":"HPCDME/transfer/#parkit","text":"parkit is designed to assist analysts in archiving project data from the NIH's Biowulf/Helix systems to the HPC-DME storage platform. It provides functionalities to package and store data such as raw FastQ files or processed data from bioinformatics pipelines. Users can automatically: create tarballs of their data (including .filelist and .md5sum files), generate metadata, create collections on HPC-DME, and deposit tar files into the system for long-term storage. parkit also features comprehensive workflows that support both folder-based and tarball-based archiving. These workflows are integrated with the SLURM job scheduler, enabling efficient execution of archival tasks on the Biowulf HPC cluster. This integration ensures that bioinformatics project data is securely archived and well-organized, allowing for seamless long-term storage. \u2757 NOTE : HPC DME API CLUs should already be setup as per these instructions in order to use parkit \u2757 NOTE : HPC_DM_UTILS environment variable should be set to point to the utils folder under the HPC_DME_APIs repo setup. Please see these instructions. projark is the preferred parkit command to completely archive an entire folder as a tarball on HPCDME using SLURM.","title":"parkit"},{"location":"HPCDME/transfer/#projark-usage","text":"","title":"projark usage"},{"location":"HPCDME/transfer/#load-conda-env","text":"# source conda . \"/data/CCBR_Pipeliner/db/PipeDB/Conda/etc/profile.d/conda.sh\" # activate parkit or parkit_dev environment conda activate parkit # check version of parkit parkit --version projark --version Expected sample output v2.0.2-dev projark is using the following parkit version: v2.0.2-dev","title":"load conda env"},{"location":"HPCDME/transfer/#projark-help","text":"projark --help Expected sample output usage: projark [-h] --folder FOLDER --projectnumber PROJECTNUMBER [--executor EXECUTOR] [--rawdata] [--cleanup] Wrapper for folder2hpcdme for quick CCBR project archiving! options: -h, --help show this help message and exit --folder FOLDER Input folder path to archive --projectnumber PROJECTNUMBER CCBR project number.. destination will be /CCBR_Archive/GRIDFTP/Project_CCBR-<projectnumber> --executor EXECUTOR slurm or local --rawdata If tarball is rawdata and needs to go under folder Rawdata --cleanup post transfer step to delete local files","title":"projark help"},{"location":"HPCDME/transfer/#projark-testing","text":"","title":"projark testing"},{"location":"HPCDME/transfer/#get-dummy-data","text":"# make a tmp folder mkdir -p /data/$USER/parkit_tmp # copy dummy project folder into the tmp folder cp -r /data/CCBR/projects/CCBR-12345 /data/$USER/parkit_tmp/CCBR-12345-$USER # check if HPC_DM_UTILS has been set echo $HPC_DM_UTILS","title":"get dummy data"},{"location":"HPCDME/transfer/#run-projark","text":"projark --folder /data/$USER/parkit_tmp/CCBR-12345-$USER --projectnumber 12345-$USER --executor local Expected sample output SOURCE_CONDA_CMD is set to: . \"/data/CCBR_Pipeliner/db/PipeDB/Conda/etc/profile.d/conda.sh\" HPC_DM_UTILS is set to: /data/kopardevn/GitRepos/HPC_DME_APIs/utils parkit_folder2hpcdme --folder \"/data/$USER/parkit_tmp/CCBR-12345-$USER\" --dest \"/CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\" --projecttitle \"CCBR-12345-kopardevn\" --projectdesc \"CCBR-12345-kopardevn\" --executor \"local\" --hpcdmutilspath /data/kopardevn/GitRepos/HPC_DME_APIs/utils --makereadme ################ Running createtar ############################# parkit createtar --folder \"/data/$USER/parkit_tmp/CCBR-12345-$USER\" tar cvf /data/$USER/parkit_tmp/CCBR-12345-$USER.tar /data/$USER/parkit_tmp/CCBR-12345-$USER > /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist createmetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar file was created! createmetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist file was created! createmetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.md5 file was created! createmetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist.md5 file was created! ################################################################ ############ Running createemptycollection ###################### parkit createemptycollection --dest \"/CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\" --projectdesc \"CCBR-12345-kopardevn\" --projecttitle \"CCBR-12345-kopardevn\" module load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_collection /dev/shm/a213dedc-9363-44ec-8a7a-d29f2345a0b5.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn cat /dev/shm/a213dedc-9363-44ec-8a7a-d29f2345a0b5.json && rm -f /dev/shm/a213dedc-9363-44ec-8a7a-d29f2345a0b5.json module load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_collection /dev/shm/cabf7826-81b5-4b6a-addd-09fbcf279591.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn/Analysis module load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_collection /dev/shm/cabf7826-81b5-4b6a-addd-09fbcf279591.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn/Rawdata cat /dev/shm/cabf7826-81b5-4b6a-addd-09fbcf279591.json && rm -f /dev/shm/cabf7826-81b5-4b6a-addd-09fbcf279591.json ################################################################ ########### Running createmetadata ############################## parkit createmetadata --tarball \"/data/$USER/parkit_tmp/CCBR-12345-$USER.tar\" --dest \"/CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\" createmetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.metadata.json file was created! createmetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist.metadata.json file was created! ################################################################ ############# Running deposittar ############################### parkit deposittar --tarball \"/data/$USER/parkit_tmp/CCBR-12345-$USER.tar\" --dest \"/CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\" module load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_dataobject /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist.metadata.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn/Analysis/CCBR-12345.tar.filelist /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist module load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_dataobject_multipart /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.metadata.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn/Analysis/CCBR-12345.tar /data/$USER/parkit_tmp/CCBR-12345-$USER.tar ################################################################ \u2757 NOTE : remove --executor local from the command when running on real data (not test data) to submit jobs through SLURM \u2757 NOTE : add --rawdata when folder contains raw fastqs","title":"run projark"},{"location":"HPCDME/transfer/#verify-transfer","text":"Transfer can be verified by logging into HPC DME web interface .","title":"verify transfer"},{"location":"HPCDME/transfer/#cleanup","text":"Delete unwanted collection from HPC DME. # load java module load java # load dm_ commands source $HPC_DM_UTILS/functions # delete collection recursively dm_delete_collection -r /CCBR_Archive/GRIDFTP/Project_CCBR-12345-$USER Expected sample output Reading properties from /data/kopardevn/GitRepos/HPC_DME_APIs/utils/hpcdme.properties WARNING: You have requested recursive delete of the collection. This will delete all files and sub-collections within it recursively. Are you sure you want to proceed? (Y/N): Y Would you like to see the list of files to delete ? N The collection /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn and all files and sub-collections within it will be recursively deleted. Proceed with deletion ? (Y/N): Y Executing: https://hpcdmeapi.nci.nih.gov:8080/collection Wrote results into /data/kopardevn/HPCDMELOG/tmp/getCollections_Records20241010.txt Cmd process Completed Oct 10, 2024 4:43:09 PM org.springframework.shell.core.AbstractShell handleExecutionResult INFO: CLI_SUCCESS \u26a0\ufe0f Reach out to Vishal Koparde in case you run into issues.","title":"cleanup"},{"location":"HPCDME/transferFromHelix/","text":"File Transfer from HELIX \u00b6 Background \u00b6 Rawdata or Project folders from Helix can be parked at a secure location after the analysis has reached an endpoint. Traditionally, CCBR analysts have been using GridFTP Globus Archive for doing this. But, this Globus Archive has been running relatively full lately and it is hard to estimate how much space is left there as the volume is shared among multiple groups. parkit \u00b6 parkit is designed to assist analysts in archiving project data from the NIH's Helix/Helix systems to the HPC-DME storage platform. It provides functionalities to package and store data such as raw FastQ files or processed data from bioinformatics pipelines. Users can automatically: - create tarballs of their data (including .filelist and .md5sum files), - generate metadata, - create collections on HPC-DME, and - deposit tar files into the system for long-term storage. parkit also features comprehensive workflows that support both folder-based and tarball-based archiving. This integration ensures that bioinformatics project data is securely archived and well-organized, allowing for seamless long-term storage. \u2757 NOTE : HPC DME API CLUs should already be setup as per these instructions in order to use parkit \u2757 NOTE : HPC_DM_UTILS environment variable should be set to point to the utils folder under the HPC_DME_APIs repo setup. Please see these instructions. \u2757 NOTE : If it has been a few months since you last used HPC_DME_APIs or parkit or projark, then please run the following commands before you start using parkit or projark : cd $HPC_DM_UTILS git pull source $HPC_DM_UTILS/functions dm_generate_token projark is the preferred parkit command to completely archive an entire folder as a tarball on HPCDME. SLURM is not available on Helix and it could take a few hours to upload large files. Hence, it is recommended to use \"tmux\" or \"screen\" command for projark to continue running even when you log out of Helix. \u2757 NOTE : Run the following commands inside a screen or tmux session. projark usage \u00b6 load conda env \u00b6 # source conda . \"/data/CCBR_Pipeliner/db/PipeDB/Conda/etc/profile.d/conda.sh\" # activate parkit or parkit_dev environment conda activate parkit # check version of parkit parkit --version projark --version Expected sample output v2.1.2 projark is using the following parkit version: v2.1.2 projark help \u00b6 projark --help Expected sample output usage: projark [-h] --folder FOLDER --projectnumber PROJECTNUMBER [--executor EXECUTOR] [--rawdata] [--cleanup] Wrapper for folder2hpcdme for quick CCBR project archiving! options: -h, --help show this help message and exit --folder FOLDER Input folder path to archive --projectnumber PROJECTNUMBER CCBR project number.. destination will be /CCBR_Archive/GRIDFTP/Project_CCBR-<projectnumber> --executor EXECUTOR slurm or local --rawdata If tarball is rawdata and needs to go under folder Rawdata --cleanup post transfer step to delete local files projark testing \u00b6 get dummy data \u00b6 # make a tmp folder mkdir -p /data/$USER/parkit_tmp # copy dummy project folder into the tmp folder cp -r /data/CCBR/projects/CCBR-12345 /data/$USER/parkit_tmp/CCBR-12345-$USER # check if HPC_DM_UTILS has been set echo $HPC_DM_UTILS run projark \u00b6 projark --folder /data/$USER/parkit_tmp/CCBR-12345-$USER --projectnumber 12345-$USER --executor local \u2757 NOTE : --executor slurm will not work on Helix Expected sample output SOURCE_CONDA_CMD is set to: . \"/data/CCBR_Pipeliner/db/PipeDB/Conda/etc/profile.d/conda.sh\" HPC_DM_UTILS is set to: /data/kopardevn/GitRepos/HPC_DME_APIs/utils parkit_folder2hpcdme --folder \"/data/$USER/parkit_tmp/CCBR-12345-$USER\" --dest \"/CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\" --projecttitle \"CCBR-12345-kopardevn\" --projectdesc \"CCBR-12345-kopardevn\" --executor \"local\" --hpcdmutilspath /data/kopardevn/GitRepos/HPC_DME_APIs/utils --makereadme ################ Running createtar ############################# parkit createtar --folder \"/data/$USER/parkit_tmp/CCBR-12345-$USER\" tar cvf /data/$USER/parkit_tmp/CCBR-12345-$USER.tar /data/$USER/parkit_tmp/CCBR-12345-$USER > /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist createmetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar file was created! createmetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist file was created! createmetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.md5 file was created! createmetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist.md5 file was created! ################################################################ ############ Running createemptycollection ###################### parkit createemptycollection --dest \"/CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\" --projectdesc \"CCBR-12345-kopardevn\" --projecttitle \"CCBR-12345-kopardevn\" module load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_collection /dev/shm/a213dedc-9363-44ec-8a7a-d29f2345a0b5.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn cat /dev/shm/a213dedc-9363-44ec-8a7a-d29f2345a0b5.json && rm -f /dev/shm/a213dedc-9363-44ec-8a7a-d29f2345a0b5.json module load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_collection /dev/shm/cabf7826-81b5-4b6a-addd-09fbcf279591.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn/Analysis module load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_collection /dev/shm/cabf7826-81b5-4b6a-addd-09fbcf279591.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn/Rawdata cat /dev/shm/cabf7826-81b5-4b6a-addd-09fbcf279591.json && rm -f /dev/shm/cabf7826-81b5-4b6a-addd-09fbcf279591.json ################################################################ ########### Running createmetadata ############################## parkit createmetadata --tarball \"/data/$USER/parkit_tmp/CCBR-12345-$USER.tar\" --dest \"/CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\" createmetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.metadata.json file was created! createmetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist.metadata.json file was created! ################################################################ ############# Running deposittar ############################### parkit deposittar --tarball \"/data/$USER/parkit_tmp/CCBR-12345-$USER.tar\" --dest \"/CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\" module load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_dataobject /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist.metadata.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn/Analysis/CCBR-12345.tar.filelist /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist module load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_dataobject_multipart /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.metadata.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn/Analysis/CCBR-12345.tar /data/$USER/parkit_tmp/CCBR-12345-$USER.tar ################################################################ \u2757 NOTE : add --rawdata when folder contains raw fastqs verify transfer \u00b6 Transfer can be verified by logging into HPC DME web interface . cleanup \u00b6 Delete unwanted collection from HPC DME. # load java module load java # load dm_ commands source $HPC_DM_UTILS/functions # delete collection recursively dm_delete_collection -r /CCBR_Archive/GRIDFTP/Project_CCBR-12345-$USER Expected sample output Reading properties from /data/kopardevn/GitRepos/HPC_DME_APIs/utils/hpcdme.properties WARNING: You have requested recursive delete of the collection. This will delete all files and sub-collections within it recursively. Are you sure you want to proceed? (Y/N): Y Would you like to see the list of files to delete ? N The collection /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn and all files and sub-collections within it will be recursively deleted. Proceed with deletion ? (Y/N): Y Executing: https://hpcdmeapi.nci.nih.gov:8080/collection Wrote results into /data/kopardevn/HPCDMELOG/tmp/getCollections_Records20241010.txt Cmd process Completed Oct 10, 2024 4:43:09 PM org.springframework.shell.core.AbstractShell handleExecutionResult INFO: CLI_SUCCESS \u26a0\ufe0f Reach out to Vishal Koparde in case you run into issues.","title":"Helix2HPCDME"},{"location":"HPCDME/transferFromHelix/#file-transfer-from-helix","text":"","title":"File Transfer from HELIX"},{"location":"HPCDME/transferFromHelix/#background","text":"Rawdata or Project folders from Helix can be parked at a secure location after the analysis has reached an endpoint. Traditionally, CCBR analysts have been using GridFTP Globus Archive for doing this. But, this Globus Archive has been running relatively full lately and it is hard to estimate how much space is left there as the volume is shared among multiple groups.","title":"Background"},{"location":"HPCDME/transferFromHelix/#parkit","text":"parkit is designed to assist analysts in archiving project data from the NIH's Helix/Helix systems to the HPC-DME storage platform. It provides functionalities to package and store data such as raw FastQ files or processed data from bioinformatics pipelines. Users can automatically: - create tarballs of their data (including .filelist and .md5sum files), - generate metadata, - create collections on HPC-DME, and - deposit tar files into the system for long-term storage. parkit also features comprehensive workflows that support both folder-based and tarball-based archiving. This integration ensures that bioinformatics project data is securely archived and well-organized, allowing for seamless long-term storage. \u2757 NOTE : HPC DME API CLUs should already be setup as per these instructions in order to use parkit \u2757 NOTE : HPC_DM_UTILS environment variable should be set to point to the utils folder under the HPC_DME_APIs repo setup. Please see these instructions. \u2757 NOTE : If it has been a few months since you last used HPC_DME_APIs or parkit or projark, then please run the following commands before you start using parkit or projark : cd $HPC_DM_UTILS git pull source $HPC_DM_UTILS/functions dm_generate_token projark is the preferred parkit command to completely archive an entire folder as a tarball on HPCDME. SLURM is not available on Helix and it could take a few hours to upload large files. Hence, it is recommended to use \"tmux\" or \"screen\" command for projark to continue running even when you log out of Helix. \u2757 NOTE : Run the following commands inside a screen or tmux session.","title":"parkit"},{"location":"HPCDME/transferFromHelix/#projark-usage","text":"","title":"projark usage"},{"location":"HPCDME/transferFromHelix/#load-conda-env","text":"# source conda . \"/data/CCBR_Pipeliner/db/PipeDB/Conda/etc/profile.d/conda.sh\" # activate parkit or parkit_dev environment conda activate parkit # check version of parkit parkit --version projark --version Expected sample output v2.1.2 projark is using the following parkit version: v2.1.2","title":"load conda env"},{"location":"HPCDME/transferFromHelix/#projark-help","text":"projark --help Expected sample output usage: projark [-h] --folder FOLDER --projectnumber PROJECTNUMBER [--executor EXECUTOR] [--rawdata] [--cleanup] Wrapper for folder2hpcdme for quick CCBR project archiving! options: -h, --help show this help message and exit --folder FOLDER Input folder path to archive --projectnumber PROJECTNUMBER CCBR project number.. destination will be /CCBR_Archive/GRIDFTP/Project_CCBR-<projectnumber> --executor EXECUTOR slurm or local --rawdata If tarball is rawdata and needs to go under folder Rawdata --cleanup post transfer step to delete local files","title":"projark help"},{"location":"HPCDME/transferFromHelix/#projark-testing","text":"","title":"projark testing"},{"location":"HPCDME/transferFromHelix/#get-dummy-data","text":"# make a tmp folder mkdir -p /data/$USER/parkit_tmp # copy dummy project folder into the tmp folder cp -r /data/CCBR/projects/CCBR-12345 /data/$USER/parkit_tmp/CCBR-12345-$USER # check if HPC_DM_UTILS has been set echo $HPC_DM_UTILS","title":"get dummy data"},{"location":"HPCDME/transferFromHelix/#run-projark","text":"projark --folder /data/$USER/parkit_tmp/CCBR-12345-$USER --projectnumber 12345-$USER --executor local \u2757 NOTE : --executor slurm will not work on Helix Expected sample output SOURCE_CONDA_CMD is set to: . \"/data/CCBR_Pipeliner/db/PipeDB/Conda/etc/profile.d/conda.sh\" HPC_DM_UTILS is set to: /data/kopardevn/GitRepos/HPC_DME_APIs/utils parkit_folder2hpcdme --folder \"/data/$USER/parkit_tmp/CCBR-12345-$USER\" --dest \"/CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\" --projecttitle \"CCBR-12345-kopardevn\" --projectdesc \"CCBR-12345-kopardevn\" --executor \"local\" --hpcdmutilspath /data/kopardevn/GitRepos/HPC_DME_APIs/utils --makereadme ################ Running createtar ############################# parkit createtar --folder \"/data/$USER/parkit_tmp/CCBR-12345-$USER\" tar cvf /data/$USER/parkit_tmp/CCBR-12345-$USER.tar /data/$USER/parkit_tmp/CCBR-12345-$USER > /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist createmetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar file was created! createmetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist file was created! createmetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.md5 file was created! createmetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist.md5 file was created! ################################################################ ############ Running createemptycollection ###################### parkit createemptycollection --dest \"/CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\" --projectdesc \"CCBR-12345-kopardevn\" --projecttitle \"CCBR-12345-kopardevn\" module load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_collection /dev/shm/a213dedc-9363-44ec-8a7a-d29f2345a0b5.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn cat /dev/shm/a213dedc-9363-44ec-8a7a-d29f2345a0b5.json && rm -f /dev/shm/a213dedc-9363-44ec-8a7a-d29f2345a0b5.json module load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_collection /dev/shm/cabf7826-81b5-4b6a-addd-09fbcf279591.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn/Analysis module load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_collection /dev/shm/cabf7826-81b5-4b6a-addd-09fbcf279591.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn/Rawdata cat /dev/shm/cabf7826-81b5-4b6a-addd-09fbcf279591.json && rm -f /dev/shm/cabf7826-81b5-4b6a-addd-09fbcf279591.json ################################################################ ########### Running createmetadata ############################## parkit createmetadata --tarball \"/data/$USER/parkit_tmp/CCBR-12345-$USER.tar\" --dest \"/CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\" createmetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.metadata.json file was created! createmetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist.metadata.json file was created! ################################################################ ############# Running deposittar ############################### parkit deposittar --tarball \"/data/$USER/parkit_tmp/CCBR-12345-$USER.tar\" --dest \"/CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\" module load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_dataobject /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist.metadata.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn/Analysis/CCBR-12345.tar.filelist /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist module load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_dataobject_multipart /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.metadata.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn/Analysis/CCBR-12345.tar /data/$USER/parkit_tmp/CCBR-12345-$USER.tar ################################################################ \u2757 NOTE : add --rawdata when folder contains raw fastqs","title":"run projark"},{"location":"HPCDME/transferFromHelix/#verify-transfer","text":"Transfer can be verified by logging into HPC DME web interface .","title":"verify transfer"},{"location":"HPCDME/transferFromHelix/#cleanup","text":"Delete unwanted collection from HPC DME. # load java module load java # load dm_ commands source $HPC_DM_UTILS/functions # delete collection recursively dm_delete_collection -r /CCBR_Archive/GRIDFTP/Project_CCBR-12345-$USER Expected sample output Reading properties from /data/kopardevn/GitRepos/HPC_DME_APIs/utils/hpcdme.properties WARNING: You have requested recursive delete of the collection. This will delete all files and sub-collections within it recursively. Are you sure you want to proceed? (Y/N): Y Would you like to see the list of files to delete ? N The collection /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn and all files and sub-collections within it will be recursively deleted. Proceed with deletion ? (Y/N): Y Executing: https://hpcdmeapi.nci.nih.gov:8080/collection Wrote results into /data/kopardevn/HPCDMELOG/tmp/getCollections_Records20241010.txt Cmd process Completed Oct 10, 2024 4:43:09 PM org.springframework.shell.core.AbstractShell handleExecutionResult INFO: CLI_SUCCESS \u26a0\ufe0f Reach out to Vishal Koparde in case you run into issues.","title":"cleanup"},{"location":"OtherHowTos/datashare/","text":"Datashare \u00b6 Host data on Helix or Biowulf , that is publicly accessible through a URL Setup \u00b6 Access Login \u00b6 # Helix ssh -Y username@helix.nih.gov # Biowulf ssh -Y username@biowulf.nih.gov Create a new dir ( tutorial ) in the datashare path: cd /data/CCBR/datashare/ mkdir tutorial Processing \u00b6 Make a directory in the datashare folder \u00b6 NOTE: For all steps below, an example is shown for Helix, but the same process is applicable for Biowulf, after changing the helix.nih.gov to biowulf.nih.gov Now you can transfer your data to the new directory. One method is to use scp to copy data from your local machine to Helix. Here is an example of using scp to copy the file file.txt from a local directory to Helix. scp /data/$USER/file.txt username@helix.nih.gov:/data/CCBR/datashare/tutorial/ To copy multiple directories recursively, you can also include the -r command with scp and from the top level directory: scp -r /data/$USER/ username@helix.nih.gov:/data/CCBR/datashare/tutorial/ Create public permissions for data \u00b6 When the data has been successully copied, we need to open the permissions. NOTE: This will give open access to anyone with the link. Ensure this is appropriate for the data type # cd to the shared dir cd /data/CCBR/datashare/ # run CHMOD, twice chmod -R 777 tutorial chmod -R 777 tutorial/* # run SETFACL setfacl -m u:webcpu:r-x tutorial/* Public Access \u00b6 NOTE: You must be logged into HPC in order to access these files from a web browser. Files will be available for access through a browser, via tools like wget and UCSC genome track browser via the following format: http://hpc.nih.gov/~CCBR/tutorial/file.txt For more information and a tutorial for creating UCSC tracks , visit the CCBR HowTo Page .","title":"Datashare"},{"location":"OtherHowTos/datashare/#datashare","text":"Host data on Helix or Biowulf , that is publicly accessible through a URL","title":"Datashare"},{"location":"OtherHowTos/datashare/#setup","text":"","title":"Setup"},{"location":"OtherHowTos/datashare/#access-login","text":"# Helix ssh -Y username@helix.nih.gov # Biowulf ssh -Y username@biowulf.nih.gov Create a new dir ( tutorial ) in the datashare path: cd /data/CCBR/datashare/ mkdir tutorial","title":"Access Login"},{"location":"OtherHowTos/datashare/#processing","text":"","title":"Processing"},{"location":"OtherHowTos/datashare/#make-a-directory-in-the-datashare-folder","text":"NOTE: For all steps below, an example is shown for Helix, but the same process is applicable for Biowulf, after changing the helix.nih.gov to biowulf.nih.gov Now you can transfer your data to the new directory. One method is to use scp to copy data from your local machine to Helix. Here is an example of using scp to copy the file file.txt from a local directory to Helix. scp /data/$USER/file.txt username@helix.nih.gov:/data/CCBR/datashare/tutorial/ To copy multiple directories recursively, you can also include the -r command with scp and from the top level directory: scp -r /data/$USER/ username@helix.nih.gov:/data/CCBR/datashare/tutorial/","title":"Make a directory in the datashare folder"},{"location":"OtherHowTos/datashare/#create-public-permissions-for-data","text":"When the data has been successully copied, we need to open the permissions. NOTE: This will give open access to anyone with the link. Ensure this is appropriate for the data type # cd to the shared dir cd /data/CCBR/datashare/ # run CHMOD, twice chmod -R 777 tutorial chmod -R 777 tutorial/* # run SETFACL setfacl -m u:webcpu:r-x tutorial/*","title":"Create public permissions for data"},{"location":"OtherHowTos/datashare/#public-access","text":"NOTE: You must be logged into HPC in order to access these files from a web browser. Files will be available for access through a browser, via tools like wget and UCSC genome track browser via the following format: http://hpc.nih.gov/~CCBR/tutorial/file.txt For more information and a tutorial for creating UCSC tracks , visit the CCBR HowTo Page .","title":"Public Access"},{"location":"OtherHowTos/lucidcharts/","text":"Account management of Lucid Charts Documents \u00b6 Remember: All Lucid charts CCBR documents should be owned by nciccbr@mail.nih.gov Your lucidcharts username should be your nih.gov email address Do NOT use non nih.gov email address, aka personal email account, for viewing or editing CCBR lucid chart documents Do NOT share lucid chart documents with non nih.gov email address ( unless absolutely necessary ) Existing document: \u00b6 For all existing Lucid Charts documents, transfer ownership to nciccbr@mail.nih.gov . This is a 2-step process: Share the document with edit permissions with nciccbr@mail.nih.gov . Reach out to Vishal Koparde to get the sharing invite accepted. Once, nciccbr@mail.nih.gov has accepted the invite, transfer the ownership over to nciccbr@mail.nih.gov . Creating new document: \u00b6 You can create new documents after logging into lucidcharts with your NIH.gov email account. Then, follow the above instructions to transfer ownership to nciccbr@mail.nih.gov .","title":"LucidCharts"},{"location":"OtherHowTos/lucidcharts/#account-management-of-lucid-charts-documents","text":"Remember: All Lucid charts CCBR documents should be owned by nciccbr@mail.nih.gov Your lucidcharts username should be your nih.gov email address Do NOT use non nih.gov email address, aka personal email account, for viewing or editing CCBR lucid chart documents Do NOT share lucid chart documents with non nih.gov email address ( unless absolutely necessary )","title":"Account management of Lucid Charts Documents"},{"location":"OtherHowTos/lucidcharts/#existing-document","text":"For all existing Lucid Charts documents, transfer ownership to nciccbr@mail.nih.gov . This is a 2-step process: Share the document with edit permissions with nciccbr@mail.nih.gov . Reach out to Vishal Koparde to get the sharing invite accepted. Once, nciccbr@mail.nih.gov has accepted the invite, transfer the ownership over to nciccbr@mail.nih.gov .","title":"Existing document:"},{"location":"OtherHowTos/lucidcharts/#creating-new-document","text":"You can create new documents after logging into lucidcharts with your NIH.gov email account. Then, follow the above instructions to transfer ownership to nciccbr@mail.nih.gov .","title":"Creating new document:"},{"location":"OtherHowTos/zenodo/","text":"Zenodo \u00b6 Submit a pipeline to Zenodo in order to create a DOI for publication. Reference link \u00b6 Use the link for full information, summarized below: https://www.youtube.com/watch?v=A9FGAU9S9Ow Prepare GitHub Repository \u00b6 The GitHub repository should include the following: README page Documentation page, such as mkdocs, with usage and contact information A citation CITATION.cff ; Example here Tagged and versioned, stable repository Link GitHub account to Zenodo \u00b6 Go to Zenodo Select username in the top right >> Profile . Select `GitHub`` Click Sync Now (top right) to update repos. NOTE: You may have to refresh the page Toggle the On button on the repo you wish to publish. This will move the pipeline to the Enable Repositories list. Prepare GitHub Repo \u00b6 Go to GitHub and find the repository page. Select Releases >> Draft a new release Create a tag, following naming semantics described here Describe the tag with the following: \"Connecting pipeline to Zenodo\" Update Zenodo Information \u00b6 Go to Zenodo Select My dashboard >> Edit Update the following information: Resource Type: Software Title: Full Pipeline Name (ShorthandName) (IE Mouse nEoanTigen pRedictOr (METRO) Creators: Add creators, including ORCID's whenever possible Description: A short description of the main features of the pipeline Additional Description: If you use this software, please cite it as below. Keywords and subjects: Add several keywords related to the pipeline Version: add the version used in GitHub Publisher: Zenodo Related works: \"Is original form of\" \"github website\" URL Add DOI, citation to GitHub \u00b6 Go to Zenodo Select username in the top right >> Profile . Select `GitHub`` Click Sync Now (top right) to update repos. NOTE: You may have to refresh the page Copy the DOI for the repository Return to the GitHub repository and edit the README of the GitHub repo, adding the DOI link. Update the CITATION.cff as needed. Create a new tagged version.","title":"Zenodo"},{"location":"OtherHowTos/zenodo/#zenodo","text":"Submit a pipeline to Zenodo in order to create a DOI for publication.","title":"Zenodo"},{"location":"OtherHowTos/zenodo/#reference-link","text":"Use the link for full information, summarized below: https://www.youtube.com/watch?v=A9FGAU9S9Ow","title":"Reference link"},{"location":"OtherHowTos/zenodo/#prepare-github-repository","text":"The GitHub repository should include the following: README page Documentation page, such as mkdocs, with usage and contact information A citation CITATION.cff ; Example here Tagged and versioned, stable repository","title":"Prepare GitHub Repository"},{"location":"OtherHowTos/zenodo/#link-github-account-to-zenodo","text":"Go to Zenodo Select username in the top right >> Profile . Select `GitHub`` Click Sync Now (top right) to update repos. NOTE: You may have to refresh the page Toggle the On button on the repo you wish to publish. This will move the pipeline to the Enable Repositories list.","title":"Link GitHub account to Zenodo"},{"location":"OtherHowTos/zenodo/#prepare-github-repo","text":"Go to GitHub and find the repository page. Select Releases >> Draft a new release Create a tag, following naming semantics described here Describe the tag with the following: \"Connecting pipeline to Zenodo\"","title":"Prepare GitHub Repo"},{"location":"OtherHowTos/zenodo/#update-zenodo-information","text":"Go to Zenodo Select My dashboard >> Edit Update the following information: Resource Type: Software Title: Full Pipeline Name (ShorthandName) (IE Mouse nEoanTigen pRedictOr (METRO) Creators: Add creators, including ORCID's whenever possible Description: A short description of the main features of the pipeline Additional Description: If you use this software, please cite it as below. Keywords and subjects: Add several keywords related to the pipeline Version: add the version used in GitHub Publisher: Zenodo Related works: \"Is original form of\" \"github website\" URL","title":"Update Zenodo Information"},{"location":"OtherHowTos/zenodo/#add-doi-citation-to-github","text":"Go to Zenodo Select username in the top right >> Profile . Select `GitHub`` Click Sync Now (top right) to update repos. NOTE: You may have to refresh the page Copy the DOI for the repository Return to the GitHub repository and edit the README of the GitHub repo, adding the DOI link. Update the CITATION.cff as needed. Create a new tagged version.","title":"Add DOI, citation to GitHub"},{"location":"Rpackage/build_pkg/","text":"Creating a Conda Package from an R Package \u00b6 Overview \u00b6 To create a package with Conda, you first need to make a new release and tag in the github repo of the R package you would like to create into a Conda Package. For more information on best practices in R package creation, review this documentation . Before creating the release on github, please check for proper dependencies listed in the files NAMESPACE and DESCRIPTION . These files should have the same list of dependencies, and the version numbers for dependencies can be specified in the DESCRIPTION file. The DESCRIPTION file must be edited manually, while the NAMESPACE file should not be edited manually, but rather created automatically using the document() function. The DESCRIPTION file must also be correctly formatted. For more information, see the following website . Download the R package release from Github \u00b6 To download the most recent release from the most recent tag on Github, activate Conda then use Conda skeleton to pull the correct URL. In the example below, replace $githubURL with the URL to your R package's github repo. conda activate conda skeleton cran $githubURL A folder is then created for the downloaded release. Ror example running the following: conda skeleton cran https://github.com/NIDAP-Community/DSPWorkflow Creates the folder r-dspworkflow Within this newly created folder is a file named meta.yaml . You will need to edit this file to include the channels and edit any information on the the package version number or dependency version numbers. Here is an example of the top of the meta.yaml file with the channels section added: {% set version = '0.9.5.2' %} {% set posix = 'm2-' if win else '' %} {% set native = 'm2w64-' if win else '' %} package: name: r-dspworkflow version: {{ version|replace(\"-\", \"_\") }} channels: - conda-forge - bioconda - default - file://rstudio-files/RH/ccbr-projects/Conda_package_tutorial/local_channel/channel source: git_url: https://github.com/NIDAP-Community/DSPWorkflow git_tag: 0.9.5 build: merge_build_host: True # [win] # If this is a new build for the same version, increment the build number. number: 0 # no skip # This is required to make R link correctly on Linux. rpaths: - lib/R/lib/ - lib/ # Suggests: testthat (== 3.1.4) requirements: build: - {{ posix }}filesystem # [win] - {{ posix }}git - {{ posix }}zip # [win] Here is an example of the sections for specifying dependency versions from the meta.yaml file: host: - r-base =4.1.3=h2f963a2_5 - bioconductor-biobase =2.54.0=r41hc0cfd56_2 - bioconductor-biocgenerics =0.40.0=r41hdfd78af_0 - bioconductor-geomxtools =3.1.1=r41hdfd78af_0 - bioconductor-nanostringnctools =1.2.0 - bioconductor-spatialdecon =1.4.3 - bioconductor-complexheatmap =2.10.0=r41hdfd78af_0 - r-cowplot =1.1.1=r41hc72bb7e_1 - r-dplyr =1.0.9=r41h7525677_0 - r-ggforce =0.3.4=r41h7525677_0 - r-ggplot2 =3.3.6=r41hc72bb7e_1 - r-gridextra =2.3=r41hc72bb7e_1004 - r-gtable =0.3.0=r41hc72bb7e_3 - r-knitr =1.40=r41hc72bb7e_1 - r-patchwork =1.1.2=r41hc72bb7e_1 - r-reshape2 =1.4.4=r41h7525677_2 - r-scales =1.2.1=r41hc72bb7e_1 - r-tibble =3.1.8=r41h06615bd_1 - r-tidyr =1.2.1=r41h7525677_1 - r-umap =0.2.9.0=r41h7525677_1 - r-rtsne =0.16=r41h37cf8d7_1 - r-magrittr =2.0.3=r41h06615bd_1 - r-rlang =1.1.0=r41h38f115c_0 run: - r-base =4.1.3=h2f963a2_5 - bioconductor-biobase =2.54.0=r41hc0cfd56_2 - bioconductor-biocgenerics =0.40.0=r41hdfd78af_0 - bioconductor-geomxtools =3.1.1=r41hdfd78af_0 - bioconductor-nanostringnctools =1.2.0 - bioconductor-spatialdecon =1.4.3 - bioconductor-complexheatmap =2.10.0=r41hdfd78af_0 - r-cowplot =1.1.1=r41hc72bb7e_1 - r-dplyr =1.0.9=r41h7525677_0 - r-ggforce =0.3.4=r41h7525677_0 - r-ggplot2 =3.3.6=r41hc72bb7e_1 - r-gridextra =2.3=r41hc72bb7e_1004 - r-gtable =0.3.0=r41hc72bb7e_3 - r-knitr =1.40=r41hc72bb7e_1 - r-patchwork =1.1.2=r41hc72bb7e_1 - r-reshape2 =1.4.4=r41h7525677_2 - r-scales =1.2.1=r41hc72bb7e_1 - r-tibble =3.1.8=r41h06615bd_1 - r-tidyr =1.2.1=r41h7525677_1 - r-umap =0.2.9.0=r41h7525677_1 - r-rtsne =0.16=r41h37cf8d7_1 - r-magrittr =2.0.3=r41h06615bd_1 - r-rlang =1.1.0=r41h38f115c_0 In the above example, each of the dependencies has been assigned a conda build string, so that when conda builds a conda package, it will only use that specific build of the dependency from the listed conda channels. The above example is very restrictive, the dependencies can also be listed in the \"meta.yaml\" file to be more open--it will choose a conda build string that fits in with the other resolved dependency build strings based on what is available in the channels. Also note that the \"host\" section matches the \"run\" section. Here is some examples of a more open setup for these dependencies: host: - r-base >=4.1.3 - bioconductor-biobase >=2.54.0 - bioconductor-biocgenerics >=0.40.0 - bioconductor-geomxtools >=3.1.1 - bioconductor-nanostringnctools >=1.2.0 - bioconductor-spatialdecon =1.4.3 - bioconductor-complexheatmap >=2.10.0 - r-cowplot >=1.1.1 - r-dplyr >=1.0.9 - r-ggforce >=0.3.4 - r-ggplot2 >=3.3.6 - r-gridextra >=2.3 - r-gtable >=0.3.0 - r-knitr >=1.40 - r-patchwork >=1.1.2 - r-reshape2 >=1.4.4 - r-scales >=1.2.1 - r-tibble >=3.1.8 - r-tidyr >=1.2.1 - r-umap >=0.2.9.0 - r-rtsne >=0.16 - r-magrittr >=2.0.3 - r-rlang >=1.1.0 Build the Conda package \u00b6 When the meta.yaml has been prepared, you can now build the Conda package. To do so, run the command, replacing $r-package with the name of the R package folder that was created after running conda skeleton (the folder where the meta.yaml is located). $build_log_name.log with the name for the log file, such as the date, time, and initials. conda-build $r-package 2>&1|tee $build_log_name.log Example conda-build r-dspworkflow 2>&1|tee 05_12_23_330_nc.log The log file will list how conda has built the package, including what dependencies version numbers and corresponding build strings were used to resolve the conda environment. These dependencies are what we specified in the \"meta.yaml\" file. The log file will be useful troubleshooting a failed build. Be aware, the build can take anywhere from several minutes to an hour to complete, depending on the size of the package and the number of dependencies. The conda package will be built as a tar.bz2 file.","title":"Creating the package"},{"location":"Rpackage/build_pkg/#creating-a-conda-package-from-an-r-package","text":"","title":"Creating a Conda Package from an R Package"},{"location":"Rpackage/build_pkg/#overview","text":"To create a package with Conda, you first need to make a new release and tag in the github repo of the R package you would like to create into a Conda Package. For more information on best practices in R package creation, review this documentation . Before creating the release on github, please check for proper dependencies listed in the files NAMESPACE and DESCRIPTION . These files should have the same list of dependencies, and the version numbers for dependencies can be specified in the DESCRIPTION file. The DESCRIPTION file must be edited manually, while the NAMESPACE file should not be edited manually, but rather created automatically using the document() function. The DESCRIPTION file must also be correctly formatted. For more information, see the following website .","title":"Overview"},{"location":"Rpackage/build_pkg/#download-the-r-package-release-from-github","text":"To download the most recent release from the most recent tag on Github, activate Conda then use Conda skeleton to pull the correct URL. In the example below, replace $githubURL with the URL to your R package's github repo. conda activate conda skeleton cran $githubURL A folder is then created for the downloaded release. Ror example running the following: conda skeleton cran https://github.com/NIDAP-Community/DSPWorkflow Creates the folder r-dspworkflow Within this newly created folder is a file named meta.yaml . You will need to edit this file to include the channels and edit any information on the the package version number or dependency version numbers. Here is an example of the top of the meta.yaml file with the channels section added: {% set version = '0.9.5.2' %} {% set posix = 'm2-' if win else '' %} {% set native = 'm2w64-' if win else '' %} package: name: r-dspworkflow version: {{ version|replace(\"-\", \"_\") }} channels: - conda-forge - bioconda - default - file://rstudio-files/RH/ccbr-projects/Conda_package_tutorial/local_channel/channel source: git_url: https://github.com/NIDAP-Community/DSPWorkflow git_tag: 0.9.5 build: merge_build_host: True # [win] # If this is a new build for the same version, increment the build number. number: 0 # no skip # This is required to make R link correctly on Linux. rpaths: - lib/R/lib/ - lib/ # Suggests: testthat (== 3.1.4) requirements: build: - {{ posix }}filesystem # [win] - {{ posix }}git - {{ posix }}zip # [win] Here is an example of the sections for specifying dependency versions from the meta.yaml file: host: - r-base =4.1.3=h2f963a2_5 - bioconductor-biobase =2.54.0=r41hc0cfd56_2 - bioconductor-biocgenerics =0.40.0=r41hdfd78af_0 - bioconductor-geomxtools =3.1.1=r41hdfd78af_0 - bioconductor-nanostringnctools =1.2.0 - bioconductor-spatialdecon =1.4.3 - bioconductor-complexheatmap =2.10.0=r41hdfd78af_0 - r-cowplot =1.1.1=r41hc72bb7e_1 - r-dplyr =1.0.9=r41h7525677_0 - r-ggforce =0.3.4=r41h7525677_0 - r-ggplot2 =3.3.6=r41hc72bb7e_1 - r-gridextra =2.3=r41hc72bb7e_1004 - r-gtable =0.3.0=r41hc72bb7e_3 - r-knitr =1.40=r41hc72bb7e_1 - r-patchwork =1.1.2=r41hc72bb7e_1 - r-reshape2 =1.4.4=r41h7525677_2 - r-scales =1.2.1=r41hc72bb7e_1 - r-tibble =3.1.8=r41h06615bd_1 - r-tidyr =1.2.1=r41h7525677_1 - r-umap =0.2.9.0=r41h7525677_1 - r-rtsne =0.16=r41h37cf8d7_1 - r-magrittr =2.0.3=r41h06615bd_1 - r-rlang =1.1.0=r41h38f115c_0 run: - r-base =4.1.3=h2f963a2_5 - bioconductor-biobase =2.54.0=r41hc0cfd56_2 - bioconductor-biocgenerics =0.40.0=r41hdfd78af_0 - bioconductor-geomxtools =3.1.1=r41hdfd78af_0 - bioconductor-nanostringnctools =1.2.0 - bioconductor-spatialdecon =1.4.3 - bioconductor-complexheatmap =2.10.0=r41hdfd78af_0 - r-cowplot =1.1.1=r41hc72bb7e_1 - r-dplyr =1.0.9=r41h7525677_0 - r-ggforce =0.3.4=r41h7525677_0 - r-ggplot2 =3.3.6=r41hc72bb7e_1 - r-gridextra =2.3=r41hc72bb7e_1004 - r-gtable =0.3.0=r41hc72bb7e_3 - r-knitr =1.40=r41hc72bb7e_1 - r-patchwork =1.1.2=r41hc72bb7e_1 - r-reshape2 =1.4.4=r41h7525677_2 - r-scales =1.2.1=r41hc72bb7e_1 - r-tibble =3.1.8=r41h06615bd_1 - r-tidyr =1.2.1=r41h7525677_1 - r-umap =0.2.9.0=r41h7525677_1 - r-rtsne =0.16=r41h37cf8d7_1 - r-magrittr =2.0.3=r41h06615bd_1 - r-rlang =1.1.0=r41h38f115c_0 In the above example, each of the dependencies has been assigned a conda build string, so that when conda builds a conda package, it will only use that specific build of the dependency from the listed conda channels. The above example is very restrictive, the dependencies can also be listed in the \"meta.yaml\" file to be more open--it will choose a conda build string that fits in with the other resolved dependency build strings based on what is available in the channels. Also note that the \"host\" section matches the \"run\" section. Here is some examples of a more open setup for these dependencies: host: - r-base >=4.1.3 - bioconductor-biobase >=2.54.0 - bioconductor-biocgenerics >=0.40.0 - bioconductor-geomxtools >=3.1.1 - bioconductor-nanostringnctools >=1.2.0 - bioconductor-spatialdecon =1.4.3 - bioconductor-complexheatmap >=2.10.0 - r-cowplot >=1.1.1 - r-dplyr >=1.0.9 - r-ggforce >=0.3.4 - r-ggplot2 >=3.3.6 - r-gridextra >=2.3 - r-gtable >=0.3.0 - r-knitr >=1.40 - r-patchwork >=1.1.2 - r-reshape2 >=1.4.4 - r-scales >=1.2.1 - r-tibble >=3.1.8 - r-tidyr >=1.2.1 - r-umap >=0.2.9.0 - r-rtsne >=0.16 - r-magrittr >=2.0.3 - r-rlang >=1.1.0","title":"Download the R package release from Github"},{"location":"Rpackage/build_pkg/#build-the-conda-package","text":"When the meta.yaml has been prepared, you can now build the Conda package. To do so, run the command, replacing $r-package with the name of the R package folder that was created after running conda skeleton (the folder where the meta.yaml is located). $build_log_name.log with the name for the log file, such as the date, time, and initials. conda-build $r-package 2>&1|tee $build_log_name.log Example conda-build r-dspworkflow 2>&1|tee 05_12_23_330_nc.log The log file will list how conda has built the package, including what dependencies version numbers and corresponding build strings were used to resolve the conda environment. These dependencies are what we specified in the \"meta.yaml\" file. The log file will be useful troubleshooting a failed build. Be aware, the build can take anywhere from several minutes to an hour to complete, depending on the size of the package and the number of dependencies. The conda package will be built as a tar.bz2 file.","title":"Build the Conda package"},{"location":"Rpackage/common_issues/","text":"Creating a Conda Package from an R Package \u00b6 Package Dependency Issues \u00b6 An important consideration for Conda builds is the list of dependencies, specified versions, and compatibility with each of the other dependencies. If the meta.yaml and DESCRIPTION file specify specific package versions, Conda's ability to resolve the Conda environment also becomes more limited. For example, if the Conda package we are building has the following requirements: Dependency A version == 1.0 Dependency B version >= 2.5 And the Dependencies located in our Conda channel have the following dependencies: Dependency A version 1.0 - Dependency C version == 0.5 Dependency A version 1.2 - Dependency C version >= 0.7 Dependency B version 2.7 - Dependency C version >= 0.7 As you can see, the Conda build will not be able to resolve the environment because Dependency A version 1.0 needs an old version of Dependency C, while Dependency B version 2.7 needs a newer version. In this case, if we changed our package's DESCRIPTION and meta.yaml file to be: Dependency A version >= 1.0 Dependency B version >= 2.5 The conda build will be able to resolve. This is a simplified version of a what are more often complex dependency structures, but it is an important concept in conda package building that will inevitably arise as a package's dependencies become more specific. To check on the versions of packages that are available in a Conda channel, use the command: conda search $dependency Replace $dependency with the name of package you would like to investigate. There are more optional commands for this function which can be found here To check the dependencies of packages that exist in your Conda cache, go to the folder specified for your conda cache (that we specified earlier). In case you need to find that path you can use: conda info Here there will be a folder for each of the packages that has been used in a conda build (including the dependencies). In each folder is another folder called \"info\" and a file called \"index.json\" that lists information, such as depends for the package. Here is an example: ``` cat /rstudio-files/ccbr-data/users/Ned/conda-cache/r-ggplot2-3.3.6-r41hc72bb7e_1/info/index.json { \"arch\": null, \"build\": \"r41hc72bb7e_1\", \"build_number\": 1, \"depends\": [ \"r-base >=4.1,<4.2.0a0\", \"r-digest\", \"r-glue\", \"r-gtable >=0.1.1\", \"r-isoband\", \"r-mass\", \"r-mgcv\", \"r-rlang >=0.3.0\", \"r-scales >=0.5.0\", \"r-tibble\", \"r-withr >=2.0.0\" ], \"license\": \"GPL-2.0-only\", \"license_family\": \"GPL2\", \"name\": \"r-ggplot2\", \"noarch\": \"generic\", \"platform\": null, \"subdir\": \"noarch\", \"timestamp\": 1665515494942, \"version\": \"3.3.6\" } ``` If you would like to specify an exact package to use in a conda channel for your conda build, specify the build string in your \"meta.yaml\" file. In the above example for ggplot version 3.3.6, the build string is listed in the folder name for package as well as in the index.json file for \"build: \"r41hc72bb7e_1\".","title":"Troubleshooting"},{"location":"Rpackage/common_issues/#creating-a-conda-package-from-an-r-package","text":"","title":"Creating a Conda Package from an R Package"},{"location":"Rpackage/common_issues/#package-dependency-issues","text":"An important consideration for Conda builds is the list of dependencies, specified versions, and compatibility with each of the other dependencies. If the meta.yaml and DESCRIPTION file specify specific package versions, Conda's ability to resolve the Conda environment also becomes more limited. For example, if the Conda package we are building has the following requirements: Dependency A version == 1.0 Dependency B version >= 2.5 And the Dependencies located in our Conda channel have the following dependencies: Dependency A version 1.0 - Dependency C version == 0.5 Dependency A version 1.2 - Dependency C version >= 0.7 Dependency B version 2.7 - Dependency C version >= 0.7 As you can see, the Conda build will not be able to resolve the environment because Dependency A version 1.0 needs an old version of Dependency C, while Dependency B version 2.7 needs a newer version. In this case, if we changed our package's DESCRIPTION and meta.yaml file to be: Dependency A version >= 1.0 Dependency B version >= 2.5 The conda build will be able to resolve. This is a simplified version of a what are more often complex dependency structures, but it is an important concept in conda package building that will inevitably arise as a package's dependencies become more specific. To check on the versions of packages that are available in a Conda channel, use the command: conda search $dependency Replace $dependency with the name of package you would like to investigate. There are more optional commands for this function which can be found here To check the dependencies of packages that exist in your Conda cache, go to the folder specified for your conda cache (that we specified earlier). In case you need to find that path you can use: conda info Here there will be a folder for each of the packages that has been used in a conda build (including the dependencies). In each folder is another folder called \"info\" and a file called \"index.json\" that lists information, such as depends for the package. Here is an example: ``` cat /rstudio-files/ccbr-data/users/Ned/conda-cache/r-ggplot2-3.3.6-r41hc72bb7e_1/info/index.json { \"arch\": null, \"build\": \"r41hc72bb7e_1\", \"build_number\": 1, \"depends\": [ \"r-base >=4.1,<4.2.0a0\", \"r-digest\", \"r-glue\", \"r-gtable >=0.1.1\", \"r-isoband\", \"r-mass\", \"r-mgcv\", \"r-rlang >=0.3.0\", \"r-scales >=0.5.0\", \"r-tibble\", \"r-withr >=2.0.0\" ], \"license\": \"GPL-2.0-only\", \"license_family\": \"GPL2\", \"name\": \"r-ggplot2\", \"noarch\": \"generic\", \"platform\": null, \"subdir\": \"noarch\", \"timestamp\": 1665515494942, \"version\": \"3.3.6\" } ``` If you would like to specify an exact package to use in a conda channel for your conda build, specify the build string in your \"meta.yaml\" file. In the above example for ggplot version 3.3.6, the build string is listed in the folder name for package as well as in the index.json file for \"build: \"r41hc72bb7e_1\".","title":"Package Dependency Issues"},{"location":"Rpackage/setup/","text":"Creating a Conda Package from an R Package \u00b6 Set up Conda Tools, installation (if needed) \u00b6 You will first need to set up Conda in order to use the Conda tools for creating your Conda package. The documentation for getting started can be found here , including installation guidelines. Set up Conda cache \u00b6 In a space shared with other users that may use Conda, your personal Conda cache needs to be specified. To edit how your cache is saved perform the following steps: 1) Create a new directory where you would like to store the conda cache called 'conda-cache' mkdir conda/conda-cache 2) In your home directory, create the file .condarc touch ~/.condarc 3) Open the new file .condarc and add the following sections: pkgs_dirs envs_dirs conda-build channels In each section you will add the path to the directories you would like to use for each section. Example: kgs_dirs: - /rstudio-files/ccbr-data/users/$USER/conda-cache envs_dirs: - /rstudio-files/ccbr-data/users/$USER/conda-envs conda-build: root-dir: /rstudio-files/ccbr-data/users/$USER/conda-bld build_folder: /rstudio-files/ccbr-data/users/$USER/conda-bld conda-build output_folder: /rstudio-files/ccbr-data/users/$USER/conda-bld/conda-output channels: - file://rstudio-files/RH/ccbr-projects/Conda_package_tutorial/local_channel/channel - conda-forge - bioconda - defaults Check Conda setup \u00b6 To check that conda has been setup with the specified paths from .condarc start conda: conda activate Then check the conda info: conda info Setup Conda Channels \u00b6 To build a Conda package, 'channels' are needed to supply the dependencies that are specified in the DESCRIPTION and meta.yaml files (discussed below). These R packages in these channels are also Conda packages that have been previously built as a specific version of that package and given a 'build string', a unique indentifier for the build of that specific conda package. For channels to be available to you when you build your own conda package, you first need to add them. To add a Conda channel run: config --add channels For , some examples are: file://rstudio-files/RH/ccbr-projects/Conda_package_tutorial/local_channel/channel conda-forge bioconda defaults Conda R Package Creation \u00b6 To create a package with Conda, you first need to make a new release and tag in the github repo of the R package you would like to create into a Conda Package. For more information on best practices in R package creation, see: https://r-pkgs.org/whole-game.html Before creating the release on github, please check for proper dependencies listed in the files NAMESPACE and DESCRIPTION. These files should have the same list of dependencies, and the version numbers for dependencies can be specified in the DESCRIPTION file. The DESCRIPTION file must be editted manually, while the NAMESPACE file should not be editted manually, but rather created automatically using the document() function. The DESCRITPION file must also be correctly formatted. For more information, see: https://r-pkgs.org/description.html Download the R package release from Github \u00b6 To download the most recent release from the most recent tag on Github, activate Conda then use Conda skeleton like so: conda activate conda skeleton cran $githubURL Replace $githubURL with the URL to your R package's github repo. A folder is then created for the downloaded release, for example running the following: conda skeleton cran https://github.com/NIDAP-Community/DSPWorkflow creates the folder r-dspworkflow Within this newly created folder is a file named \"meta.yaml\". You will need to edit this file to include the channels and edit any information on the the package version number or dependency version numbers. Here is an example of the top of the \"meta.yaml\" file with the channels section added: {% set version = '0.9.5.2' %} {% set posix = 'm2-' if win else '' %} {% set native = 'm2w64-' if win else '' %} package: name: r-dspworkflow version: {{ version|replace(\"-\", \"_\") }} channels: - conda-forge - bioconda - default - file://rstudio-files/RH/ccbr-projects/Conda_package_tutorial/local_channel/channel source: git_url: https://github.com/NIDAP-Community/DSPWorkflow git_tag: 0.9.5 build: merge_build_host: True # [win] # If this is a new build for the same version, increment the build number. number: 0 # no skip # This is required to make R link correctly on Linux. rpaths: - lib/R/lib/ - lib/ # Suggests: testthat (== 3.1.4) requirements: build: - {{ posix }}filesystem # [win] - {{ posix }}git - {{ posix }}zip # [win] Here is an example of the sections for specifying dependency versions from the \"meta.yaml\" file: host: - r-base =4.1.3=h2f963a2_5 - bioconductor-biobase =2.54.0=r41hc0cfd56_2 - bioconductor-biocgenerics =0.40.0=r41hdfd78af_0 - bioconductor-geomxtools =3.1.1=r41hdfd78af_0 - bioconductor-nanostringnctools =1.2.0 - bioconductor-spatialdecon =1.4.3 - bioconductor-complexheatmap =2.10.0=r41hdfd78af_0 - r-cowplot =1.1.1=r41hc72bb7e_1 - r-dplyr =1.0.9=r41h7525677_0 - r-ggforce =0.3.4=r41h7525677_0 - r-ggplot2 =3.3.6=r41hc72bb7e_1 - r-gridextra =2.3=r41hc72bb7e_1004 - r-gtable =0.3.0=r41hc72bb7e_3 - r-knitr =1.40=r41hc72bb7e_1 - r-patchwork =1.1.2=r41hc72bb7e_1 - r-reshape2 =1.4.4=r41h7525677_2 - r-scales =1.2.1=r41hc72bb7e_1 - r-tibble =3.1.8=r41h06615bd_1 - r-tidyr =1.2.1=r41h7525677_1 - r-umap =0.2.9.0=r41h7525677_1 - r-rtsne =0.16=r41h37cf8d7_1 - r-magrittr =2.0.3=r41h06615bd_1 - r-rlang =1.1.0=r41h38f115c_0 run: - r-base =4.1.3=h2f963a2_5 - bioconductor-biobase =2.54.0=r41hc0cfd56_2 - bioconductor-biocgenerics =0.40.0=r41hdfd78af_0 - bioconductor-geomxtools =3.1.1=r41hdfd78af_0 - bioconductor-nanostringnctools =1.2.0 - bioconductor-spatialdecon =1.4.3 - bioconductor-complexheatmap =2.10.0=r41hdfd78af_0 - r-cowplot =1.1.1=r41hc72bb7e_1 - r-dplyr =1.0.9=r41h7525677_0 - r-ggforce =0.3.4=r41h7525677_0 - r-ggplot2 =3.3.6=r41hc72bb7e_1 - r-gridextra =2.3=r41hc72bb7e_1004 - r-gtable =0.3.0=r41hc72bb7e_3 - r-knitr =1.40=r41hc72bb7e_1 - r-patchwork =1.1.2=r41hc72bb7e_1 - r-reshape2 =1.4.4=r41h7525677_2 - r-scales =1.2.1=r41hc72bb7e_1 - r-tibble =3.1.8=r41h06615bd_1 - r-tidyr =1.2.1=r41h7525677_1 - r-umap =0.2.9.0=r41h7525677_1 - r-rtsne =0.16=r41h37cf8d7_1 - r-magrittr =2.0.3=r41h06615bd_1 - r-rlang =1.1.0=r41h38f115c_0 In the above example, each of the dependencies has been assigned a conda build string, so that when conda builds a conda package, it will only use that specific build of the dependency from the listed conda channels. The above example is very restrictive, the dependencies can also be listed in the \"meta.yaml\" file to be more open--it will choose a conda build string that fits in with the other resolved dependency build strings based on what is available in the channels. Also note that the \"host\" section matches the \"run\" section. Here is some examples of a more open setup for these dependencies: host: - r-base >=4.1.3 - bioconductor-biobase >=2.54.0 - bioconductor-biocgenerics >=0.40.0 - bioconductor-geomxtools >=3.1.1 - bioconductor-nanostringnctools >=1.2.0 - bioconductor-spatialdecon =1.4.3 - bioconductor-complexheatmap >=2.10.0 - r-cowplot >=1.1.1 - r-dplyr >=1.0.9 - r-ggforce >=0.3.4 - r-ggplot2 >=3.3.6 - r-gridextra >=2.3 - r-gtable >=0.3.0 - r-knitr >=1.40 - r-patchwork >=1.1.2 - r-reshape2 >=1.4.4 - r-scales >=1.2.1 - r-tibble >=3.1.8 - r-tidyr >=1.2.1 - r-umap >=0.2.9.0 - r-rtsne >=0.16 - r-magrittr >=2.0.3 - r-rlang >=1.1.0 Build the Conda package \u00b6 When the \"meta.yaml\" has been prepared, you can now build the Conda package. To do so, run the command: conda-build $r-package 2>&1 | tee $build_log_name.log Replace $r-package with the name of the R package folder that was created after running conda skeleton (the folder where the meta.yaml is located). Example: r-dspworkflow Replace $build_log_name.log with the name for the log file, such as the date, time, and initials. Example: 05_12_23_330_nc.log The log file will list how conda has built the package, including what dependencies version numbers and corresponding build strings were used to resolve the conda environment. These dependencies are what we specified in the \"meta.yaml\" file. The log file will be useful troubleshooting a failed build. Be aware, the build can take anywhere from several minutes to an hour to complete, depending on the size of the package and the number of dependencies. The conda package will be built as a tar.bz2 file. Common Issues \u00b6 Package Dependency Issues \u00b6 An important consideration for Conda builds is the list of dependencies, specified versions, and compatibility with each of the other dependencies. If the meta.yaml and DESCRIPTION file specify specific package versions, Conda's ability to resolve the Conda environment also becomes more limited. For example, if the Conda package we are building has the following requirements: Dependency A version == 1.0 Dependency B version >= 2.5 And the Dependencies located in our Conda channel have the following dependencies: Dependency A version 1.0 - Dependency C version == 0.5 Dependency A version 1.2 - Dependency C version >= 0.7 Dependency B version 2.7 - Dependency C version >= 0.7 As you can see, the Conda build will not be able to resolve the environment because Dependency A verion 1.0 needs an old version of Dependency C, while Dependency B version 2.7 needs a newer version. In this case, if we changed our package's DESCRIPTION and meta.yaml file to be: Dependency A version >= 1.0 Dependency B version >= 2.5 The conda build will be able to resolve. This is a simplied version of a what are more often complex dependency structures, but it is an important concept in conda package building that will inevitably arise as a package's dependencies become more specific. To check on the versions of packages that are available in a Conda channel, use the command: conda search $dependency Replace $dependency with the name of package you would like to investigate. There are more optional commands for this function which can be found here: https://docs.conda.io/projects/conda/en/latest/commands/search.html To check the dependencies of packages that exist in your Conda cache, go to the folder specified for your conda cache (that we specified earlier). In case you need to find that path you can use: ``` conda info Here there will be a folder for each of the packages that has been used in a conda build (including the dependencies). In each folder is another folder called \"info\" and a file called \"index.json\" that lists information, such as depends for the package. Here is an example: cat /rstudio-files/ccbr-data/users/$USER/conda-cache/r-ggplot2-3.3.6-r41hc72bb7e_1/info/index.json { \"arch\": null, \"build\": \"r41hc72bb7e_1\", \"build_number\": 1, \"depends\": [ \"r-base >=4.1,<4.2.0a0\", \"r-digest\", \"r-glue\", \"r-gtable >=0.1.1\", \"r-isoband\", \"r-mass\", \"r-mgcv\", \"r-rlang >=0.3.0\", \"r-scales >=0.5.0\", \"r-tibble\", \"r-withr >=2.0.0\" ], \"license\": \"GPL-2.0-only\", \"license_family\": \"GPL2\", \"name\": \"r-ggplot2\", \"noarch\": \"generic\", \"platform\": null, \"subdir\": \"noarch\", \"timestamp\": 1665515494942, \"version\": \"3.3.6\" } ``` If you would like to specify an exact package to use in a conda channel for your conda build, specify the build string in your \"meta.yaml\" file. In the above example for ggplot version 3.3.6, the build string is listed in the folder name for package as well as in the index.json file for \"build: \"r41hc72bb7e_1\".","title":"Set-Up"},{"location":"Rpackage/setup/#creating-a-conda-package-from-an-r-package","text":"","title":"Creating a Conda Package from an R Package"},{"location":"Rpackage/setup/#set-up-conda-tools-installation-if-needed","text":"You will first need to set up Conda in order to use the Conda tools for creating your Conda package. The documentation for getting started can be found here , including installation guidelines.","title":"Set up Conda Tools, installation (if needed)"},{"location":"Rpackage/setup/#set-up-conda-cache","text":"In a space shared with other users that may use Conda, your personal Conda cache needs to be specified. To edit how your cache is saved perform the following steps: 1) Create a new directory where you would like to store the conda cache called 'conda-cache' mkdir conda/conda-cache 2) In your home directory, create the file .condarc touch ~/.condarc 3) Open the new file .condarc and add the following sections: pkgs_dirs envs_dirs conda-build channels In each section you will add the path to the directories you would like to use for each section. Example: kgs_dirs: - /rstudio-files/ccbr-data/users/$USER/conda-cache envs_dirs: - /rstudio-files/ccbr-data/users/$USER/conda-envs conda-build: root-dir: /rstudio-files/ccbr-data/users/$USER/conda-bld build_folder: /rstudio-files/ccbr-data/users/$USER/conda-bld conda-build output_folder: /rstudio-files/ccbr-data/users/$USER/conda-bld/conda-output channels: - file://rstudio-files/RH/ccbr-projects/Conda_package_tutorial/local_channel/channel - conda-forge - bioconda - defaults","title":"Set up Conda cache"},{"location":"Rpackage/setup/#check-conda-setup","text":"To check that conda has been setup with the specified paths from .condarc start conda: conda activate Then check the conda info: conda info","title":"Check Conda setup"},{"location":"Rpackage/setup/#setup-conda-channels","text":"To build a Conda package, 'channels' are needed to supply the dependencies that are specified in the DESCRIPTION and meta.yaml files (discussed below). These R packages in these channels are also Conda packages that have been previously built as a specific version of that package and given a 'build string', a unique indentifier for the build of that specific conda package. For channels to be available to you when you build your own conda package, you first need to add them. To add a Conda channel run: config --add channels For , some examples are: file://rstudio-files/RH/ccbr-projects/Conda_package_tutorial/local_channel/channel conda-forge bioconda defaults","title":"Setup Conda Channels"},{"location":"Rpackage/setup/#conda-r-package-creation","text":"To create a package with Conda, you first need to make a new release and tag in the github repo of the R package you would like to create into a Conda Package. For more information on best practices in R package creation, see: https://r-pkgs.org/whole-game.html Before creating the release on github, please check for proper dependencies listed in the files NAMESPACE and DESCRIPTION. These files should have the same list of dependencies, and the version numbers for dependencies can be specified in the DESCRIPTION file. The DESCRIPTION file must be editted manually, while the NAMESPACE file should not be editted manually, but rather created automatically using the document() function. The DESCRITPION file must also be correctly formatted. For more information, see: https://r-pkgs.org/description.html","title":"Conda R Package Creation"},{"location":"Rpackage/setup/#download-the-r-package-release-from-github","text":"To download the most recent release from the most recent tag on Github, activate Conda then use Conda skeleton like so: conda activate conda skeleton cran $githubURL Replace $githubURL with the URL to your R package's github repo. A folder is then created for the downloaded release, for example running the following: conda skeleton cran https://github.com/NIDAP-Community/DSPWorkflow creates the folder r-dspworkflow Within this newly created folder is a file named \"meta.yaml\". You will need to edit this file to include the channels and edit any information on the the package version number or dependency version numbers. Here is an example of the top of the \"meta.yaml\" file with the channels section added: {% set version = '0.9.5.2' %} {% set posix = 'm2-' if win else '' %} {% set native = 'm2w64-' if win else '' %} package: name: r-dspworkflow version: {{ version|replace(\"-\", \"_\") }} channels: - conda-forge - bioconda - default - file://rstudio-files/RH/ccbr-projects/Conda_package_tutorial/local_channel/channel source: git_url: https://github.com/NIDAP-Community/DSPWorkflow git_tag: 0.9.5 build: merge_build_host: True # [win] # If this is a new build for the same version, increment the build number. number: 0 # no skip # This is required to make R link correctly on Linux. rpaths: - lib/R/lib/ - lib/ # Suggests: testthat (== 3.1.4) requirements: build: - {{ posix }}filesystem # [win] - {{ posix }}git - {{ posix }}zip # [win] Here is an example of the sections for specifying dependency versions from the \"meta.yaml\" file: host: - r-base =4.1.3=h2f963a2_5 - bioconductor-biobase =2.54.0=r41hc0cfd56_2 - bioconductor-biocgenerics =0.40.0=r41hdfd78af_0 - bioconductor-geomxtools =3.1.1=r41hdfd78af_0 - bioconductor-nanostringnctools =1.2.0 - bioconductor-spatialdecon =1.4.3 - bioconductor-complexheatmap =2.10.0=r41hdfd78af_0 - r-cowplot =1.1.1=r41hc72bb7e_1 - r-dplyr =1.0.9=r41h7525677_0 - r-ggforce =0.3.4=r41h7525677_0 - r-ggplot2 =3.3.6=r41hc72bb7e_1 - r-gridextra =2.3=r41hc72bb7e_1004 - r-gtable =0.3.0=r41hc72bb7e_3 - r-knitr =1.40=r41hc72bb7e_1 - r-patchwork =1.1.2=r41hc72bb7e_1 - r-reshape2 =1.4.4=r41h7525677_2 - r-scales =1.2.1=r41hc72bb7e_1 - r-tibble =3.1.8=r41h06615bd_1 - r-tidyr =1.2.1=r41h7525677_1 - r-umap =0.2.9.0=r41h7525677_1 - r-rtsne =0.16=r41h37cf8d7_1 - r-magrittr =2.0.3=r41h06615bd_1 - r-rlang =1.1.0=r41h38f115c_0 run: - r-base =4.1.3=h2f963a2_5 - bioconductor-biobase =2.54.0=r41hc0cfd56_2 - bioconductor-biocgenerics =0.40.0=r41hdfd78af_0 - bioconductor-geomxtools =3.1.1=r41hdfd78af_0 - bioconductor-nanostringnctools =1.2.0 - bioconductor-spatialdecon =1.4.3 - bioconductor-complexheatmap =2.10.0=r41hdfd78af_0 - r-cowplot =1.1.1=r41hc72bb7e_1 - r-dplyr =1.0.9=r41h7525677_0 - r-ggforce =0.3.4=r41h7525677_0 - r-ggplot2 =3.3.6=r41hc72bb7e_1 - r-gridextra =2.3=r41hc72bb7e_1004 - r-gtable =0.3.0=r41hc72bb7e_3 - r-knitr =1.40=r41hc72bb7e_1 - r-patchwork =1.1.2=r41hc72bb7e_1 - r-reshape2 =1.4.4=r41h7525677_2 - r-scales =1.2.1=r41hc72bb7e_1 - r-tibble =3.1.8=r41h06615bd_1 - r-tidyr =1.2.1=r41h7525677_1 - r-umap =0.2.9.0=r41h7525677_1 - r-rtsne =0.16=r41h37cf8d7_1 - r-magrittr =2.0.3=r41h06615bd_1 - r-rlang =1.1.0=r41h38f115c_0 In the above example, each of the dependencies has been assigned a conda build string, so that when conda builds a conda package, it will only use that specific build of the dependency from the listed conda channels. The above example is very restrictive, the dependencies can also be listed in the \"meta.yaml\" file to be more open--it will choose a conda build string that fits in with the other resolved dependency build strings based on what is available in the channels. Also note that the \"host\" section matches the \"run\" section. Here is some examples of a more open setup for these dependencies: host: - r-base >=4.1.3 - bioconductor-biobase >=2.54.0 - bioconductor-biocgenerics >=0.40.0 - bioconductor-geomxtools >=3.1.1 - bioconductor-nanostringnctools >=1.2.0 - bioconductor-spatialdecon =1.4.3 - bioconductor-complexheatmap >=2.10.0 - r-cowplot >=1.1.1 - r-dplyr >=1.0.9 - r-ggforce >=0.3.4 - r-ggplot2 >=3.3.6 - r-gridextra >=2.3 - r-gtable >=0.3.0 - r-knitr >=1.40 - r-patchwork >=1.1.2 - r-reshape2 >=1.4.4 - r-scales >=1.2.1 - r-tibble >=3.1.8 - r-tidyr >=1.2.1 - r-umap >=0.2.9.0 - r-rtsne >=0.16 - r-magrittr >=2.0.3 - r-rlang >=1.1.0","title":"Download the R package release from Github"},{"location":"Rpackage/setup/#build-the-conda-package","text":"When the \"meta.yaml\" has been prepared, you can now build the Conda package. To do so, run the command: conda-build $r-package 2>&1 | tee $build_log_name.log Replace $r-package with the name of the R package folder that was created after running conda skeleton (the folder where the meta.yaml is located). Example: r-dspworkflow Replace $build_log_name.log with the name for the log file, such as the date, time, and initials. Example: 05_12_23_330_nc.log The log file will list how conda has built the package, including what dependencies version numbers and corresponding build strings were used to resolve the conda environment. These dependencies are what we specified in the \"meta.yaml\" file. The log file will be useful troubleshooting a failed build. Be aware, the build can take anywhere from several minutes to an hour to complete, depending on the size of the package and the number of dependencies. The conda package will be built as a tar.bz2 file.","title":"Build the Conda package"},{"location":"Rpackage/setup/#common-issues","text":"","title":"Common Issues"},{"location":"Rpackage/setup/#package-dependency-issues","text":"An important consideration for Conda builds is the list of dependencies, specified versions, and compatibility with each of the other dependencies. If the meta.yaml and DESCRIPTION file specify specific package versions, Conda's ability to resolve the Conda environment also becomes more limited. For example, if the Conda package we are building has the following requirements: Dependency A version == 1.0 Dependency B version >= 2.5 And the Dependencies located in our Conda channel have the following dependencies: Dependency A version 1.0 - Dependency C version == 0.5 Dependency A version 1.2 - Dependency C version >= 0.7 Dependency B version 2.7 - Dependency C version >= 0.7 As you can see, the Conda build will not be able to resolve the environment because Dependency A verion 1.0 needs an old version of Dependency C, while Dependency B version 2.7 needs a newer version. In this case, if we changed our package's DESCRIPTION and meta.yaml file to be: Dependency A version >= 1.0 Dependency B version >= 2.5 The conda build will be able to resolve. This is a simplied version of a what are more often complex dependency structures, but it is an important concept in conda package building that will inevitably arise as a package's dependencies become more specific. To check on the versions of packages that are available in a Conda channel, use the command: conda search $dependency Replace $dependency with the name of package you would like to investigate. There are more optional commands for this function which can be found here: https://docs.conda.io/projects/conda/en/latest/commands/search.html To check the dependencies of packages that exist in your Conda cache, go to the folder specified for your conda cache (that we specified earlier). In case you need to find that path you can use: ``` conda info Here there will be a folder for each of the packages that has been used in a conda build (including the dependencies). In each folder is another folder called \"info\" and a file called \"index.json\" that lists information, such as depends for the package. Here is an example: cat /rstudio-files/ccbr-data/users/$USER/conda-cache/r-ggplot2-3.3.6-r41hc72bb7e_1/info/index.json { \"arch\": null, \"build\": \"r41hc72bb7e_1\", \"build_number\": 1, \"depends\": [ \"r-base >=4.1,<4.2.0a0\", \"r-digest\", \"r-glue\", \"r-gtable >=0.1.1\", \"r-isoband\", \"r-mass\", \"r-mgcv\", \"r-rlang >=0.3.0\", \"r-scales >=0.5.0\", \"r-tibble\", \"r-withr >=2.0.0\" ], \"license\": \"GPL-2.0-only\", \"license_family\": \"GPL2\", \"name\": \"r-ggplot2\", \"noarch\": \"generic\", \"platform\": null, \"subdir\": \"noarch\", \"timestamp\": 1665515494942, \"version\": \"3.3.6\" } ``` If you would like to specify an exact package to use in a conda channel for your conda build, specify the build string in your \"meta.yaml\" file. In the above example for ggplot version 3.3.6, the build string is listed in the folder name for package as well as in the index.json file for \"build: \"r41hc72bb7e_1\".","title":"Package Dependency Issues"},{"location":"Tutorials/snakemake/","text":"Snakemake \u00b6 Step-by-step guide for setting up and learning to use Snakemake, with examples and use cases https://CCBR.github.io/snakemake_tutorial/","title":"Snakemake"},{"location":"Tutorials/snakemake/#snakemake","text":"Step-by-step guide for setting up and learning to use Snakemake, with examples and use cases https://CCBR.github.io/snakemake_tutorial/","title":"Snakemake"},{"location":"UCSC/creating_inputs/","text":"Generating Inputs \u00b6 In order to use the genomic broswer features, sample files must be created. Individual sample files \u00b6 For individual samples, where peak density is to be observed, bigwig formatted files must be generated. If using the CCBR pipelines these are automatically generated as outputs of the pipeline ( WORKDIR/results/bigwig ). In many cases, scaling or normalization of bigwig is required to visualize multiple samples in comparison with each other. See various deeptools options for details/ideas. If not using CCBR pipelines, example code is provided below for the file generation. modue load ucsc fragments_bed=\"/path/to/sample1.fragments.bed\" bw=\"/path/to/sample1.bigwig\" genome_len=\"numeric_genome_length\" bg=\"/path/to/sample1.bedgraph\" bw=\"/path/to/sample2.bigwig\" # if using a spike-in scale, the scaling factor should be applied # while not required, it is recommended for CUT&RUN experiements spikein_scale=\"spike_in_value\" # create bed file bedtools genomecov -bg -scale $spikein_scale -i $fragments_bed -g $genome_len > $bg # create bigwig file bedGraphToBigWig $bg $genome_len $bw Contrasts between samples \u00b6 For contrasts, where peak differences are to be observed, bigbed formatted files must be generated. If using the CCBR/CARLISLE pipeline these are automatically generated as outputs of the pipeline (WORKDIR/results/peaks/contrasts/contrast_id/). If not using this pipeline, example code is provided below for the file generation. module load ucsc bed=\"/path/to/sample1_vs_sample2_fragmentsbased_diffresults.bed\" bigbed=\"/path/to/output/sample1_vs_sample2_fragmentsbased_diffresults.bigbed\" genome_len=\"numeric_genome_length\" # create bigbed file bedToBigBed -type=bed9 $bed $genome_len $bigbed Sharing data \u00b6 For all sample types, data must be stored on a shared directory. It is recommended that symlnks be created from the source location to this shared directory to ensure that minial disc space is being used. Example code for creating symlinks is provided below. single sample \u00b6 # single sample ## set source file location source_loc=\"/WORKDIR/results/bigwig/sample1.bigwig \" ## set destination link location link_loc=\"/SHAREDDIR/bigwig/sample1.bigwig\" ## create hard links ln $source_loc $link_loc contrast sample \u00b6 # contrast ## set source file location source_loc=\"WORKDIR/results/peaks/contrasts/sample1_vs_sample2/sample1_vs_sample2_fragmentsbased_diffresults.bigbed \" ## set destination link location link_loc=\"/SHAREDDIR/bigbed/sample1_vs_sample2.bigbed\" ## create hard links ln $source_loc $link_loc Once the links have been generated, the data folder must be open to read and write access. ## set destination link location link_loc=\"/SHAREDDIR/bigbed/\" # open dir chmod -R a+rX $link_loc","title":"Creating Inputs"},{"location":"UCSC/creating_inputs/#generating-inputs","text":"In order to use the genomic broswer features, sample files must be created.","title":"Generating Inputs"},{"location":"UCSC/creating_inputs/#individual-sample-files","text":"For individual samples, where peak density is to be observed, bigwig formatted files must be generated. If using the CCBR pipelines these are automatically generated as outputs of the pipeline ( WORKDIR/results/bigwig ). In many cases, scaling or normalization of bigwig is required to visualize multiple samples in comparison with each other. See various deeptools options for details/ideas. If not using CCBR pipelines, example code is provided below for the file generation. modue load ucsc fragments_bed=\"/path/to/sample1.fragments.bed\" bw=\"/path/to/sample1.bigwig\" genome_len=\"numeric_genome_length\" bg=\"/path/to/sample1.bedgraph\" bw=\"/path/to/sample2.bigwig\" # if using a spike-in scale, the scaling factor should be applied # while not required, it is recommended for CUT&RUN experiements spikein_scale=\"spike_in_value\" # create bed file bedtools genomecov -bg -scale $spikein_scale -i $fragments_bed -g $genome_len > $bg # create bigwig file bedGraphToBigWig $bg $genome_len $bw","title":"Individual sample files"},{"location":"UCSC/creating_inputs/#contrasts-between-samples","text":"For contrasts, where peak differences are to be observed, bigbed formatted files must be generated. If using the CCBR/CARLISLE pipeline these are automatically generated as outputs of the pipeline (WORKDIR/results/peaks/contrasts/contrast_id/). If not using this pipeline, example code is provided below for the file generation. module load ucsc bed=\"/path/to/sample1_vs_sample2_fragmentsbased_diffresults.bed\" bigbed=\"/path/to/output/sample1_vs_sample2_fragmentsbased_diffresults.bigbed\" genome_len=\"numeric_genome_length\" # create bigbed file bedToBigBed -type=bed9 $bed $genome_len $bigbed","title":"Contrasts between samples"},{"location":"UCSC/creating_inputs/#sharing-data","text":"For all sample types, data must be stored on a shared directory. It is recommended that symlnks be created from the source location to this shared directory to ensure that minial disc space is being used. Example code for creating symlinks is provided below.","title":"Sharing data"},{"location":"UCSC/creating_inputs/#single-sample","text":"# single sample ## set source file location source_loc=\"/WORKDIR/results/bigwig/sample1.bigwig \" ## set destination link location link_loc=\"/SHAREDDIR/bigwig/sample1.bigwig\" ## create hard links ln $source_loc $link_loc","title":"single sample"},{"location":"UCSC/creating_inputs/#contrast-sample","text":"# contrast ## set source file location source_loc=\"WORKDIR/results/peaks/contrasts/sample1_vs_sample2/sample1_vs_sample2_fragmentsbased_diffresults.bigbed \" ## set destination link location link_loc=\"/SHAREDDIR/bigbed/sample1_vs_sample2.bigbed\" ## create hard links ln $source_loc $link_loc Once the links have been generated, the data folder must be open to read and write access. ## set destination link location link_loc=\"/SHAREDDIR/bigbed/\" # open dir chmod -R a+rX $link_loc","title":"contrast sample"},{"location":"UCSC/creating_track_info/","text":"Generating Track information \u00b6 Single samples \u00b6 It's recommended to create a text file of all sample track information to ease in editing and submission to the UCSC browser website. A single line of code is needed for each sample which will provide the track location, sample name, description of the sample, whether to autoscale the samples, max height of the samples, view limits, and color. An example is provided below. track type=bigWig bigDataUrl=https://hpc.nih.gov/~CCBR/ccbr1155/${dir_loc}/bigwig/${complete_sample_id}.bigwig name=${sample_id} description=${sample_id} visibility=full autoScale=off maxHeightPixels=128:30:1 viewLimits=1:120 color=65,105,225 Users may find it helpful to create a single script which would create this text file for all samples. An example of this is listed below, which assumes that input files were generated using the CARLISLE pipeline. It can be edited to adapt to other output files, as needed. Generally, each \"track\" line should have at least the following key value pairs: - name : label for the track - description : defines the center lable displayed - type : BAM, BED, bigBed, bigWig, etc. - bigDataUrl : URL of the data file - for other options see here Inputs \u00b6 samples_list.txt: a single column text file with sampleID's track_dir: path to the linked files track_output: path to output file peak_list: all peak types to be included method_list: what method to be included dedupe_list: type of duplication to be included # input arguments sample_list_input=/\"path/to/samples.txt\" track_dir=\"/path/to/shared/dir/\" track_output=\"/path/to/output/file/tracks.txt peak_list=(\"norm.relaxed.bed\" \"narrowPeak\" \"broadGo_peaks.bed\" \"narrowGo_peaks.bed\") method_list=(\"fragmentsbased\") dedup_list=(\"dedup\") # read sample file IFS=$'\\n' read -d '' -r -a sample_list < $sample_list_input run_sample_tracks (){ sample_id=$1 dedup_id=$2 # sample name # eg siNC_H3K27Ac_1.dedup.bigwig complete_sample_id=\"${sample_id}.${dedup_id}\" # set link location link_loc=\"${track_dir}/bigwig/${complete_sample_id}.bigwig\" # echo track info echo \"track type=bigWig bigDataUrl=https://hpc.nih.gov/~CCBR/ccbr1155/${dir_loc}/bigwig/${complete_sample_id}.bigwig name=${sample_id} description=${sample_id} visibility=full autoScale=off maxHeightPixels=128:30:1 viewLimits=1:120 color=65,105,225\" >> $track_output } # iterate through samples # at the sample level only DEDUP matters for sample_id in ${sample_list[@]}; do for dedup_id in ${dedup_list[@]}; do run_sample_tracks $sample_id $dedup_id done done Contrast samples \u00b6 It's recommended to create a text file of all sample track information to ease in editing and submission to the UCSC browser website. A single line of code is needed for each contrast which will provide the track location, contrast name, file type, and whether to color the sample. An example is provided below. track name=${sample_id}_${peak_type} bigDataUrl=https://hpc.nih.gov/~CCBR/ccbr1155/${dir_loc}/bigbed/${complete_sample_id}_fragmentsbased_diffresults.bigbed type=bigBed itemRgb=On Users may find it helpful to create a single script which would create this text file for all contrasts. An example of this is listed below, which assumes that input files were generated using the CARLISLE pipeline. It can be edited to adapt to other output files, as needed. Inputs \u00b6 samples_list.txt: a single column text file with sampleID's track_dir: path to the linked files track_output: path to output file peak_list: all peak types to be included method_list: what method to be included dedupe_list: type of duplication to be included # input arguments sample_list_input=/\"path/to/samples.txt\" track_dir=\"/path/to/shared/dir/\" track_output=\"/path/to/output/file/tracks.txt peak_list=(\"norm.relaxed.bed\" \"narrowPeak\" \"broadGo_peaks.bed\" \"narrowGo_peaks.bed\") method_list=(\"fragmentsbased\") dedup_list=(\"dedup\") # read sample file IFS=$'\\n' read -d '' -r -a deg_list < $deg_list_input run_comparison_tracks (){ peak_type=$1 method_type=$2 dedup_type=$3 sample_id=$4 # sample name # eg siSmyd3_2m_Smyd3_0.25HCHO_500K_vs_siNC_2m_Smyd3_0.25HCHO_500K__no_dedup__norm.relaxed complete_sample_id=\"${sample_id}__${dedup_type}__${peak_type}\" # set link location link_loc=\"${track_dir}/bigbed/${complete_sample_id}_${method_type}_diffresults.bigbed\" # echo track info echo \"track name=${sample_id}_${peak_type} bigDataUrl=https://hpc.nih.gov/~CCBR/ccbr1155/${dir_loc}/bigbed/${complete_sample_id}_fragmentsbased_diffresults.bigbed type=bigBed itemRgb=On\" >> $track_info } # iterate through samples / peaks / methods / dedup for sample_id in ${deg_list[@]}; do for peak_id in ${peak_list[@]}; do for method_id in ${method_list[@]}; do for dedup_id in ${dedup_list[@]}; do run_comparison_tracks $peak_id $method_id $dedup_id $sample_id done done done done Other tips \u00b6 Users can also change the colors of the tracks using standard HTML color features. Common colors used are provided below: Red=205,92,92 Blue=65,105,225 Black=0,0,0","title":"Creating Tracks Info"},{"location":"UCSC/creating_track_info/#generating-track-information","text":"","title":"Generating Track information"},{"location":"UCSC/creating_track_info/#single-samples","text":"It's recommended to create a text file of all sample track information to ease in editing and submission to the UCSC browser website. A single line of code is needed for each sample which will provide the track location, sample name, description of the sample, whether to autoscale the samples, max height of the samples, view limits, and color. An example is provided below. track type=bigWig bigDataUrl=https://hpc.nih.gov/~CCBR/ccbr1155/${dir_loc}/bigwig/${complete_sample_id}.bigwig name=${sample_id} description=${sample_id} visibility=full autoScale=off maxHeightPixels=128:30:1 viewLimits=1:120 color=65,105,225 Users may find it helpful to create a single script which would create this text file for all samples. An example of this is listed below, which assumes that input files were generated using the CARLISLE pipeline. It can be edited to adapt to other output files, as needed. Generally, each \"track\" line should have at least the following key value pairs: - name : label for the track - description : defines the center lable displayed - type : BAM, BED, bigBed, bigWig, etc. - bigDataUrl : URL of the data file - for other options see here","title":"Single samples"},{"location":"UCSC/creating_track_info/#inputs","text":"samples_list.txt: a single column text file with sampleID's track_dir: path to the linked files track_output: path to output file peak_list: all peak types to be included method_list: what method to be included dedupe_list: type of duplication to be included # input arguments sample_list_input=/\"path/to/samples.txt\" track_dir=\"/path/to/shared/dir/\" track_output=\"/path/to/output/file/tracks.txt peak_list=(\"norm.relaxed.bed\" \"narrowPeak\" \"broadGo_peaks.bed\" \"narrowGo_peaks.bed\") method_list=(\"fragmentsbased\") dedup_list=(\"dedup\") # read sample file IFS=$'\\n' read -d '' -r -a sample_list < $sample_list_input run_sample_tracks (){ sample_id=$1 dedup_id=$2 # sample name # eg siNC_H3K27Ac_1.dedup.bigwig complete_sample_id=\"${sample_id}.${dedup_id}\" # set link location link_loc=\"${track_dir}/bigwig/${complete_sample_id}.bigwig\" # echo track info echo \"track type=bigWig bigDataUrl=https://hpc.nih.gov/~CCBR/ccbr1155/${dir_loc}/bigwig/${complete_sample_id}.bigwig name=${sample_id} description=${sample_id} visibility=full autoScale=off maxHeightPixels=128:30:1 viewLimits=1:120 color=65,105,225\" >> $track_output } # iterate through samples # at the sample level only DEDUP matters for sample_id in ${sample_list[@]}; do for dedup_id in ${dedup_list[@]}; do run_sample_tracks $sample_id $dedup_id done done","title":"Inputs"},{"location":"UCSC/creating_track_info/#contrast-samples","text":"It's recommended to create a text file of all sample track information to ease in editing and submission to the UCSC browser website. A single line of code is needed for each contrast which will provide the track location, contrast name, file type, and whether to color the sample. An example is provided below. track name=${sample_id}_${peak_type} bigDataUrl=https://hpc.nih.gov/~CCBR/ccbr1155/${dir_loc}/bigbed/${complete_sample_id}_fragmentsbased_diffresults.bigbed type=bigBed itemRgb=On Users may find it helpful to create a single script which would create this text file for all contrasts. An example of this is listed below, which assumes that input files were generated using the CARLISLE pipeline. It can be edited to adapt to other output files, as needed.","title":"Contrast samples"},{"location":"UCSC/creating_track_info/#inputs_1","text":"samples_list.txt: a single column text file with sampleID's track_dir: path to the linked files track_output: path to output file peak_list: all peak types to be included method_list: what method to be included dedupe_list: type of duplication to be included # input arguments sample_list_input=/\"path/to/samples.txt\" track_dir=\"/path/to/shared/dir/\" track_output=\"/path/to/output/file/tracks.txt peak_list=(\"norm.relaxed.bed\" \"narrowPeak\" \"broadGo_peaks.bed\" \"narrowGo_peaks.bed\") method_list=(\"fragmentsbased\") dedup_list=(\"dedup\") # read sample file IFS=$'\\n' read -d '' -r -a deg_list < $deg_list_input run_comparison_tracks (){ peak_type=$1 method_type=$2 dedup_type=$3 sample_id=$4 # sample name # eg siSmyd3_2m_Smyd3_0.25HCHO_500K_vs_siNC_2m_Smyd3_0.25HCHO_500K__no_dedup__norm.relaxed complete_sample_id=\"${sample_id}__${dedup_type}__${peak_type}\" # set link location link_loc=\"${track_dir}/bigbed/${complete_sample_id}_${method_type}_diffresults.bigbed\" # echo track info echo \"track name=${sample_id}_${peak_type} bigDataUrl=https://hpc.nih.gov/~CCBR/ccbr1155/${dir_loc}/bigbed/${complete_sample_id}_fragmentsbased_diffresults.bigbed type=bigBed itemRgb=On\" >> $track_info } # iterate through samples / peaks / methods / dedup for sample_id in ${deg_list[@]}; do for peak_id in ${peak_list[@]}; do for method_id in ${method_list[@]}; do for dedup_id in ${dedup_list[@]}; do run_comparison_tracks $peak_id $method_id $dedup_id $sample_id done done done done","title":"Inputs"},{"location":"UCSC/creating_track_info/#other-tips","text":"Users can also change the colors of the tracks using standard HTML color features. Common colors used are provided below: Red=205,92,92 Blue=65,105,225 Black=0,0,0","title":"Other tips"},{"location":"UCSC/creating_tracks/","text":"Biowulf/Helix hosts its own instance of the UCSC Genome Browser which is behind the NIH firewall. Generating New Tracks \u00b6 Login to VPN Login to the UCSC Browser website Select \"My Data > Custom Tracks\" Select \"Add Custom Tracks\" Paste the track data generated in Creating Track Info into the text box Select \"Submit\" Review the track information. The column \"Error\" should be empty. If there is an error then a hyperlink will display \"Show\" although this often does not contain helpful error information. After troubleshooting any errors select \"Go\" Use the features at the bottom of the page to alter views and/or add additional track information Select \"My Data > My Sessions\" Under \"Save current settings as named session:\" enter a descriptive name of the session Select \"Submit\" This will move the descriptive name entered into the \"session name\" list Select the descriptve name, view your track information as saved Copy the hyperlink for this session and share as needed Editing a Previous Tracks \u00b6 Login to VPN Login to the UCSC Browser website Select \"My Data > My Sessions\" Select the descriptve name of the session you'd like to edit Edit the tracks as needed: If wanting to remove or add tracks, then select \"My Data > Custom Tracks\"; follow steps 5-8 above. If wanting to edit the view of the tracks, follow step 9 above. Select \"My Data > My Sessions\" Under \"Save current settings as named session:\" enter a new descriptive name OR the previous name of the session if you'd like to overwrite it Select \"Submit\" This will move the descriptive name entered into the \"session name\" list Select the descriptve name, view your track information as saved Copy the hyperlink for this session and share as needed TIP: Unindexed file formats like bed and gtf take significantly longer to load in the genome Browser and it is recommended to convert them to indexed formats like bigBed and bigWig prior to adding them to your session.","title":"Creating Tracks"},{"location":"UCSC/creating_tracks/#generating-new-tracks","text":"Login to VPN Login to the UCSC Browser website Select \"My Data > Custom Tracks\" Select \"Add Custom Tracks\" Paste the track data generated in Creating Track Info into the text box Select \"Submit\" Review the track information. The column \"Error\" should be empty. If there is an error then a hyperlink will display \"Show\" although this often does not contain helpful error information. After troubleshooting any errors select \"Go\" Use the features at the bottom of the page to alter views and/or add additional track information Select \"My Data > My Sessions\" Under \"Save current settings as named session:\" enter a descriptive name of the session Select \"Submit\" This will move the descriptive name entered into the \"session name\" list Select the descriptve name, view your track information as saved Copy the hyperlink for this session and share as needed","title":"Generating New Tracks"},{"location":"UCSC/creating_tracks/#editing-a-previous-tracks","text":"Login to VPN Login to the UCSC Browser website Select \"My Data > My Sessions\" Select the descriptve name of the session you'd like to edit Edit the tracks as needed: If wanting to remove or add tracks, then select \"My Data > Custom Tracks\"; follow steps 5-8 above. If wanting to edit the view of the tracks, follow step 9 above. Select \"My Data > My Sessions\" Under \"Save current settings as named session:\" enter a new descriptive name OR the previous name of the session if you'd like to overwrite it Select \"Submit\" This will move the descriptive name entered into the \"session name\" list Select the descriptve name, view your track information as saved Copy the hyperlink for this session and share as needed TIP: Unindexed file formats like bed and gtf take significantly longer to load in the genome Browser and it is recommended to convert them to indexed formats like bigBed and bigWig prior to adding them to your session.","title":"Editing a Previous Tracks"},{"location":"UCSC/overview/","text":"Overview \u00b6 The UCSC Genome Browser allows for visualization of genomic data in an interactive and shareable format. User's must create accounts with their NIH credentials, and have an active Biowulf account to create the tracks. In addition users have to be connect to VPN in order to view and create the tracks. Once bigwig files are generated and stored in a shared data location, genomic tracks can be edited, and permanent links created, accessible for collaborators to view.","title":"Overview"},{"location":"UCSC/overview/#overview","text":"The UCSC Genome Browser allows for visualization of genomic data in an interactive and shareable format. User's must create accounts with their NIH credentials, and have an active Biowulf account to create the tracks. In addition users have to be connect to VPN in order to view and create the tracks. Once bigwig files are generated and stored in a shared data location, genomic tracks can be edited, and permanent links created, accessible for collaborators to view.","title":"Overview"}]}