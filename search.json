[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CCBR How-Tos",
    "section": "",
    "text": "This website is designed to share knowledge between analysts and engineers in CCBR.\nYou’ll find how-to-guides, best practices and tutorials under the pages in the sidebar to the left.\nThis page was created through the contributions of several members in CCBR. If you would like to contribute to its development, please reach out to Vishal Koparde to get added to this repo in order to submit a PR."
  },
  {
    "objectID": "docs/lucidcharts.html",
    "href": "docs/lucidcharts.html",
    "title": "LucidChart",
    "section": "",
    "text": "Remember:",
    "crumbs": [
      "Home",
      "LucidChart"
    ]
  },
  {
    "objectID": "docs/lucidcharts.html#existing-document",
    "href": "docs/lucidcharts.html#existing-document",
    "title": "LucidChart",
    "section": "Existing document:",
    "text": "Existing document:\nFor all existing Lucid Charts documents, transfer ownership to nciccbr@mail.nih.gov. This is a 2-step process:\n\nShare the document with edit permissions with nciccbr@mail.nih.gov. Reach out to Vishal Koparde to get the sharing invite accepted.\nOnce, nciccbr@mail.nih.gov has accepted the invite, transfer the ownership over to nciccbr@mail.nih.gov. For doing so please follow the instructions here.",
    "crumbs": [
      "Home",
      "LucidChart"
    ]
  },
  {
    "objectID": "docs/lucidcharts.html#creating-new-document",
    "href": "docs/lucidcharts.html#creating-new-document",
    "title": "LucidChart",
    "section": "Creating new document:",
    "text": "Creating new document:\nYou can create new documents after logging into lucidcharts with your NIH.gov email account. Then, follow the above instructions to transfer ownership to nciccbr@mail.nih.gov.",
    "crumbs": [
      "Home",
      "LucidChart"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#the-problem",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#the-problem",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "The problem",
    "text": "The problem\nBioinformatics is critical for biological research, but bioinformatics software often does not follow good software engineering practices.\n\n\n\n\nhttps://imgs.xkcd.com/comics/dependency.png",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#implications",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#implications",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "Implications",
    "text": "Implications\n\nerror-prone code can lead to invalid scientific findings\ntechnical debt - makes future changes more difficult",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#causes-of-poor-software-quality-in-bioinformatics",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#causes-of-poor-software-quality-in-bioinformatics",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "Causes of poor software quality in bioinformatics",
    "text": "Causes of poor software quality in bioinformatics\n\nmany bioinformaticians lack training in software development\nacademia credits individual researchers to aid their careers, which deincentivizes teams",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#writing-good-code-is-hard",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#writing-good-code-is-hard",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "Writing good code is hard!",
    "text": "Writing good code is hard!\n\n\n\n\nhttps://imgs.xkcd.com/comics/good_code.png",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#the-trap-of-technical-debt",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#the-trap-of-technical-debt",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "The trap of technical debt",
    "text": "The trap of technical debt\n\n\n\n\nhttps://imgs.xkcd.com/comics/fixing_problems.png",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#the-trap-of-perfect-code",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#the-trap-of-perfect-code",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "The trap of “perfect” code",
    "text": "The trap of “perfect” code\n\n\n\n\nI find that when someone’s taking time to do something right in the present, they’re a perfectionist with no ability to prioritize, whereas when someone took time to do something right in the past, they’re a master artisan of great foresight.\n\n\nhttps://imgs.xkcd.com/comics/the_general_problem.png",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#the-authors-proposed-solution",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#the-authors-proposed-solution",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "The authors’ proposed solution",
    "text": "The authors’ proposed solution\nOrganize bioinformaticians into collaborative teams to facilitate:\n\nsoftware quality seminars\ncode reviews\nresource sharing\n\n(i.e. a learning community)\n\nGoal\nPersuade academic researchers to organize collaborative teams within their institutions to improve software quality",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#lessons-from-the-tech-industry",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#lessons-from-the-tech-industry",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "Lessons from the tech industry",
    "text": "Lessons from the tech industry\n\nthe team is a basic unit: there are no lone geniuses\nteams incentivize collective ownership\nthe team context requires all members to adopt good software practices\n\n\nyou can’t collaborate effectively without version control software. your teammates won’t want to work with hard-to-read code.",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#lessons-from-rock-climbing",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#lessons-from-rock-climbing",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "Lessons from rock climbing",
    "text": "Lessons from rock climbing\nClimbers view the route together and discuss the best way to climb it.\n\n\n\n\n\nhttps://www.youtube.com/live/K2zoh6_4Pvw?feature=shared&t=4344\n\n\n\nstart at 1:12:24\nstop by 1:13:56",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#lessons-from-rock-climbing-1",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#lessons-from-rock-climbing-1",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "Lessons from rock climbing",
    "text": "Lessons from rock climbing\n\n\n\n\nFig 1. Kerenc et al. 2024. 10.1093/bioinformatics/btae632\n\n\ndiscuss most optimal way to reach goal before you start the project",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#improving-software-dev-as-a-team",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#improving-software-dev-as-a-team",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "Improving software dev as a team",
    "text": "Improving software dev as a team\na learning community with:\n\nsoftware quality seminars\ncode reviews\nresource sharing",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#software-quality-seminars",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#software-quality-seminars",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "1. Software quality seminars",
    "text": "1. Software quality seminars\n\npresentations & demos covering new techniques, tools, methods, and theory\ntopics are applicable across multiple projects\nresult: members acquire new knowledge to apply to their projects, community gains a shared vocabulary to discuss their work",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#code-review-sessions",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#code-review-sessions",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "2. Code review sessions",
    "text": "2. Code review sessions\n\nreview a section of code line-by-line to offer feedback\nstandard practice in industry, but seldom implemented in academia\nresult: enforce consistent coding standards, detect bugs, positive peer pressure to write good",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#resource-sharing",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#resource-sharing",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "3. Resource sharing",
    "text": "3. Resource sharing\n\nexternal open access resources + internal resources\nrepositories, packages, libraries, seminar recordings\nresult: encourages re-use and further enables knowledge sharing\n\n\nhttps://ferenckata.github.io/ImprovingSoftwareTogether.github.io/index.html",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#outcomes",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#outcomes",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "Outcomes",
    "text": "Outcomes\norganizing academic bioinformaticians into teams:\n\nfosters collaboration while allowing individuals to retain personal ownership & advance their careers\nlowers the barrier to adapt new technologies and techniques\nimproves software quality to enable deeper trust in the resulting scientific discoveries\nbenefits large & small projects alike",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#future-perspectives",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#future-perspectives",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "Future perspectives",
    "text": "Future perspectives\nChallenges in encouraging good software practices\n\nFunding agencies reward novelty; few grants fund maintenance\nAcademia considers journal publications as the token of success\n\n\nSilver linings\n\nFunding for critical software maintenance: Chan Zuckerberg Initiative, Schmidt Futures\nJournal of Open Source Software: great venue for publishing scientific software",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#group-discussion",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#group-discussion",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "Group discussion",
    "text": "Group discussion\n\nIn what ways do the authors’ ideas apply to us?\nHow are we already applying some of their ideas?\nAre there ways we could improve our teamwork?\n\n\n\nAcademia vs Core resource\nReusable software vs One-time-use analysis code",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/code-club/index.html",
    "href": "docs/code-club/index.html",
    "title": "Code Club",
    "section": "",
    "text": "Journal Club where the idea to have a CCBR Code Club was proposed\nFirst Code Club: Establishing a community of practice",
    "crumbs": [
      "Home",
      "Code Club"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/6_howto_precommit.html",
    "href": "docs/GitHub/guide/6_howto_precommit.html",
    "title": "GitHub HowTo: Pre-Commit",
    "section": "",
    "text": "Pre-commit should be added to all GitHub repositories on Biowulf and any clones created elsewhere to ensure cohesive and informative commit messages. After the creating the repository the following commands should be run in order to initialize the pre-commit hook, and establish the following requirements for all commit messages:\n\n\nPre-commit has been installed as a module on Biowulf. Set up an interactive session, and follow the steps below. A pre-commit configuration file is needed, and can be copied from the CCBR template repo.\n# load module on biowulf\nmodule load precommit\n\n# CD into the GitHub repo\ncd &lt;repo_name&gt;\n\n# install precommit in the repo - this is the only time you need to do this, per local repo location\npre-commit install\n\n# copy and update the precommit config\ntouch .pre-commit.config\n\n\n\nCommits must follow the format listed below, as designated by Angular:\n&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;\n&lt;body&gt;\n&lt;BLANK LINE&gt;\n&lt;footer&gt;\n\n\nThe type must be one of the following options:\n\nbuild: Changes that affect the build system or external dependencies\nci: Changes to our CI configuration files and scripts\ndocs: Documentation only changes\nfeat: A new feature\nfix: A bug fix\nperf: A code change that improves performance\nrefactor: A code change that neither fixes a bug nor adds a feature\nstyle: Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc)\ntest: Adding missing tests or correcting existing tests\n\n\n\n\nThe scope must be one of the following options:\n\nanimations\ncommon\ncompiler\ncompiler-cli\ncore\nelements\nforms\nhttp\nlanguage-service\nplatform-browser\nplatform-browser-dynamic\nplatform-server\nplatform-webworker\nplatform-webworker-dynamic\nrouter\nservice-worker\nupgrade\n\n\n\n\nThe subject must be a succinct description of the change. It should follow the following rules:\n\nuse the imperative, present tense: “change” not “changed” nor “changes”\ndon’t capitalize the first letter\nno dot (.) at the end\n\n\n\n\nThe body should include the motivation for the change and contrast this with previous behavior. It should follow the following rule:\n\nuse the imperative, present tense: “change” not “changed” nor “changes”\n\n\n\n\nThe footer should contain any information about Breaking Changes and is also the place to reference GitHub issues that this commit Closes.\n\nBreaking Changes should start with the word BREAKING CHANGE: with a space or two newlines. The rest of the commit message is then used for this.\nClosed bugs should be listed on a separate line in the footer from Breaking Changes, prefixed with “Closes” keyword (Closes #234 or Closes #123, #245, #992).\n\n\n\n\nBelow are some examples of properly formatted commit messages\n# example\ndocs(changelog): update changelog to beta.5\n\n# example\nfix(release): need to depend on latest rxjs and zone.js\n\n# example\nfeat($browser): onUrlChange event (popstate/hashchange/polling)\nAdded new event to $browser:\n- forward popstate event if available\n- forward hashchange event if popstate not available\n- do polling when neither popstate nor hashchange available\n\nBreaks $browser.onHashChange, which was removed (use onUrlChange instead)\n\n# example\nfix($compile): couple of unit tests for IE9\nOlder IEs serialize html uppercased, but IE9 does not...\nWould be better to expect case insensitive, unfortunately jasmine does\nnot allow to user regexps for throw expectations.\n\nCloses #392\nBreaks foo.bar api, foo.baz should be used instead",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub HowTo: Pre-Commit"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/6_howto_precommit.html#using-precommit",
    "href": "docs/GitHub/guide/6_howto_precommit.html#using-precommit",
    "title": "GitHub HowTo: Pre-Commit",
    "section": "",
    "text": "Pre-commit has been installed as a module on Biowulf. Set up an interactive session, and follow the steps below. A pre-commit configuration file is needed, and can be copied from the CCBR template repo.\n# load module on biowulf\nmodule load precommit\n\n# CD into the GitHub repo\ncd &lt;repo_name&gt;\n\n# install precommit in the repo - this is the only time you need to do this, per local repo location\npre-commit install\n\n# copy and update the precommit config\ntouch .pre-commit.config",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub HowTo: Pre-Commit"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/6_howto_precommit.html#structure-of-commit-messages",
    "href": "docs/GitHub/guide/6_howto_precommit.html#structure-of-commit-messages",
    "title": "GitHub HowTo: Pre-Commit",
    "section": "",
    "text": "Commits must follow the format listed below, as designated by Angular:\n&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;\n&lt;body&gt;\n&lt;BLANK LINE&gt;\n&lt;footer&gt;\n\n\nThe type must be one of the following options:\n\nbuild: Changes that affect the build system or external dependencies\nci: Changes to our CI configuration files and scripts\ndocs: Documentation only changes\nfeat: A new feature\nfix: A bug fix\nperf: A code change that improves performance\nrefactor: A code change that neither fixes a bug nor adds a feature\nstyle: Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc)\ntest: Adding missing tests or correcting existing tests\n\n\n\n\nThe scope must be one of the following options:\n\nanimations\ncommon\ncompiler\ncompiler-cli\ncore\nelements\nforms\nhttp\nlanguage-service\nplatform-browser\nplatform-browser-dynamic\nplatform-server\nplatform-webworker\nplatform-webworker-dynamic\nrouter\nservice-worker\nupgrade\n\n\n\n\nThe subject must be a succinct description of the change. It should follow the following rules:\n\nuse the imperative, present tense: “change” not “changed” nor “changes”\ndon’t capitalize the first letter\nno dot (.) at the end\n\n\n\n\nThe body should include the motivation for the change and contrast this with previous behavior. It should follow the following rule:\n\nuse the imperative, present tense: “change” not “changed” nor “changes”\n\n\n\n\nThe footer should contain any information about Breaking Changes and is also the place to reference GitHub issues that this commit Closes.\n\nBreaking Changes should start with the word BREAKING CHANGE: with a space or two newlines. The rest of the commit message is then used for this.\nClosed bugs should be listed on a separate line in the footer from Breaking Changes, prefixed with “Closes” keyword (Closes #234 or Closes #123, #245, #992).\n\n\n\n\nBelow are some examples of properly formatted commit messages\n# example\ndocs(changelog): update changelog to beta.5\n\n# example\nfix(release): need to depend on latest rxjs and zone.js\n\n# example\nfeat($browser): onUrlChange event (popstate/hashchange/polling)\nAdded new event to $browser:\n- forward popstate event if available\n- forward hashchange event if popstate not available\n- do polling when neither popstate nor hashchange available\n\nBreaks $browser.onHashChange, which was removed (use onUrlChange instead)\n\n# example\nfix($compile): couple of unit tests for IE9\nOlder IEs serialize html uppercased, but IE9 does not...\nWould be better to expect case insensitive, unfortunately jasmine does\nnot allow to user regexps for throw expectations.\n\nCloses #392\nBreaks foo.bar api, foo.baz should be used instead",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub HowTo: Pre-Commit"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/5_basic_actions.html",
    "href": "docs/GitHub/guide/5_basic_actions.html",
    "title": "GitHub Basics: GitHub Actions",
    "section": "",
    "text": "GitHub Basics: GitHub Actions\nThe following describe the minimum GitHub actions that should be deployed with any production pipeline. The actions are automatically provided via the cookiecutter templates: NextFlow and Snakemake.\n\nDocumentation (assumes mkdocs build; required for all repos)\n\nThese rules will automatically update any documentation built with mkdocs for all PR’s.\nRule Name(s): mkdocs_build –&gt; pages-build-and-deployment\n\nLintr (required for CCBR projects and new pipelines)\n\nThis rule will automatically perform a lintr with the data provided in the .test folder of the pipeline. Review the GitHub Best Practices - Test Data page for more information.\n\nDry-run with test sample data for any PR to dev branch (required for CCBR projects and new pipelines)\n\nThis rule will automatically perform a dry-run with the data provided in the .test folder of the pipeline. Review the GitHub Best Practices - Test Data page for more information.\n\nFull-run with full sample data for any PR to main branch (required for CCBR projects and new pipelines)\n\nThis rule will automatically perform a full-run with the data provided in the .test folder of the pipeline. Review the GitHub Best Practices - Test Data page for more information.\n\nAuto pull/push from source (if applicable for CCBR projects and new pipelines)\n\nIf the pipeline is forked from another location and updating this forked pipeline is required, an action will automatically perform a pull from the source location at least once a week.\n\nAdd assigned issues & PRs to user projects.\nWhen an issue or PR is assigned to a CCBR member, this action will automatically add it to their personal GitHub Project, if they have one. This file can be copy and pasted exactly as-is into any CCBR repo from here.",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Basics: GitHub Actions"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/4_basic_docs.html",
    "href": "docs/GitHub/guide/4_basic_docs.html",
    "title": "GitHub Basics: Documentation",
    "section": "",
    "text": "GitHub Pages is quick and easy way to build static websites for your GitHub repositories. Essentially, you write pages in Markdown which are then rendered to HTML and hosted on GitHub, free of cost!\nCCBR has used GitHub pages to provide extensive, legible and organized documentation for our pipelines. Examples are included below:\n\nCARLISLE\nPipeliner\nRNA-seek\n\nMkdocs is the with documentation tool preferred, with the Material theme, for most of the CCBR GitHub Pages websites.\n\n\nmkdocs and the Material for mkdocs theme can be installed using the following:\npip install --upgrade pip\npip install mkdocs\npip install mkdocs-material\nAlso install other common dependencies:\npip install mkdocs-pymdownx-material-extras\npip install mkdocs-git-revision-date-localized-plugin\npip install mkdocs-git-revision-date-plugin\npip install mkdocs-minify-plugin\n\n\n\nGenerally, for GitHub repos with GitHub pages:\n\nThe repository needs to be public (not private)\nThe main/master branch has the markdown documents under a docs folder at the root level\nRendered HTMLs are hosted under a gh-pages branch at root level\n\n\n\n\nThe following steps can be followed to build your first website\n\n\n\nmkdocs.yaml needs to be added to the root of the master branch. A template of this file is available in the cookiecutter template.\ngit clone https://github.com/CCBR/xyz.git\ncd xyz\nvi mkdocs.yaml\ngit add mkdocs.yaml\ngit commit -m \"adding mkdocs.yaml\"\ngit push\nHere is a sample mkdocs.yaml:\n# Project Information\nsite_name: CCBR How Tos\nsite_author: Vishal Koparde, Ph.D.\nsite_description: &gt;-\n  The **DEVIL** is in the **DETAILS**. Step-by-step detailed How To Guides for data management and other CCBR-relevant tasks.\n\n# Repository\nrepo_name: CCBR/HowTos\nrepo_url: https://github.com/CCBR/HowTos\nedit_uri: https://github.com/CCBR/HowTos/edit/main/docs/\n\n# Copyright\ncopyright: Copyright &copy; 2023 CCBR\n\n# Configuration\ntheme:\n  name: material\n  features:\n    - navigation.tabs\n    - navigation.top\n    - navigation.indexes\n    - toc.integrate \n  palette:\n    - scheme: default\n      primary: indigo\n      accent: indigo\n      toggle:\n        icon: material/toggle-switch-off-outline\n        name: Switch to dark mode\n    - scheme: slate\n      primary: red\n      accent: red\n      toggle:\n        icon: material/toggle-switch\n        name: Switch to light mode\n  logo: images/doc-book.svg\n  favicon: images/favicon.png\n\n# Plugins\nplugins:\n  - search\n  - git-revision-date\n  - minify:\n      minify_html: true\n\n\n# Customization\nextra:\n  social:\n    - icon: fontawesome/solid/users\n      link: http://bioinformatics.cancer.gov\n    - icon: fontawesome/brands/github\n      link: https://github.com/CCRGeneticsBranch\n    - icon: fontawesome/brands/docker\n      link: https://hub.docker.com/orgs/nciccbr/repositories\n  version:\n    provider: mike\n\n\n# Extensions\nmarkdown_extensions:\n  - markdown.extensions.admonition\n  - markdown.extensions.attr_list\n  - markdown.extensions.def_list\n  - markdown.extensions.footnotes\n  - markdown.extensions.meta\n  - markdown.extensions.toc:\n      permalink: true\n  - pymdownx.arithmatex:\n      generic: true\n  - pymdownx.betterem:\n      smart_enable: all\n  - pymdownx.caret\n  - pymdownx.critic\n  - pymdownx.details\n  - pymdownx.emoji:\n      emoji_index: !!python/name:materialx.emoji.twemoji\n      emoji_generator: !!python/name:materialx.emoji.to_svg\n  - pymdownx.highlight\n  - pymdownx.inlinehilite\n  - pymdownx.keys\n  - pymdownx.magiclink:\n      repo_url_shorthand: true\n      user: squidfunk\n      repo: mkdocs-material\n  - pymdownx.mark\n  - pymdownx.smartsymbols\n  - pymdownx.snippets:\n      check_paths: true\n  - pymdownx.superfences\n  - pymdownx.tabbed\n  - pymdownx.tasklist:\n      custom_checkbox: true\n  - pymdownx.tilde\n\n# Page Tree\nnav:\n  - Intro : index.md\n\n\n\nCreate docs folder, add your index.md there.\nmkdir docs\necho \"### Testing\" &gt; docs/index.md\ngit add docs/index.md\ngit commit -m \"adding landing page\"\ngit push\n\n\n\nmkdocs can now be used to render .md to HTML\nmkdocs build\nINFO     -  Cleaning site directory\nINFO     -  Building documentation to directory: /Users/$USER/Documents/GitRepos/parkit/site\nINFO     -  Documentation built in 0.32 seconds\nThe above command creates a local site folder which is the root of your “to-be-hosted” website. You can now open the HTMLs in the site folder locally to ensure that that HTML is as per you liking. If not, then you can make edits to the .md files and rebuild the site.\nNOTE: You do not want to push the site folder back to GH and hence it needs to be added to .gitignore file:\necho \"**/site/*\" &gt; .gitignore\ngit add .gitignore\ngit commit -m \"adding .gitignore\"\ngit push\n\n\n\nThe following command with auto-create a gh-pages branch (if it does not exist) and copy the contents of the site folder to the root of that branch. It will also provide you the URL to your newly created website.\nmkdocs gh-deploy\nINFO     -  Cleaning site directory\nINFO     -  Building documentation to directory: /Users/kopardevn/Documents/GitRepos/xyz/site\nINFO     -  Documentation built in 0.34 seconds\nWARNING  -  Version check skipped: No version specified in previous deployment.\nINFO     -  Copying '/Users/kopardevn/Documents/GitRepos/xyz/site' to 'gh-pages' branch and pushing to\n            GitHub.\nEnumerating objects: 51, done.\nCounting objects: 100(51/51), done.\nDelta compression using up to 16 threads\nCompressing objects: 100(47/47), done.\nWriting objects: 100(51/51), 441.71 KiB | 4.29 MiB/s, done.\nTotal 51 (delta 4), reused 0 (delta 0), pack-reused 0\nremote: Resolving deltas: 100(4/4), done.\nremote:\nremote: Create a pull request for 'gh-pages' on GitHub by visiting:\nremote:      https://github.com/CCBR/xyz/pull/new/gh-pages\nremote:\nTo https://github.com/CCBR/xyz.git\n * [new branch]      gh-pages -&gt; gh-pages\nINFO     -  Your documentation should shortly be available at: https://CCBR.github.io/xyz/\nNow if you point your web browser to the URL from gh-deploy command (IE https://CCBR.github.io/xyz/) you will see your HTML hosted on GitHub. After creating your docs, the cookiecutter template includes a GitHub action which will automatically perform the above tasks whenever a push is performed to the main branch.\n\n\n\n\nGo to the main GitHub page of your repository\nOn the top right select the gear icon next to About\nUnder Website, select Use your GitHub Pages website.\nSelect Save Changes",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Basics: Documentation"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/4_basic_docs.html#install-mkdocs-themes",
    "href": "docs/GitHub/guide/4_basic_docs.html#install-mkdocs-themes",
    "title": "GitHub Basics: Documentation",
    "section": "",
    "text": "mkdocs and the Material for mkdocs theme can be installed using the following:\npip install --upgrade pip\npip install mkdocs\npip install mkdocs-material\nAlso install other common dependencies:\npip install mkdocs-pymdownx-material-extras\npip install mkdocs-git-revision-date-localized-plugin\npip install mkdocs-git-revision-date-plugin\npip install mkdocs-minify-plugin",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Basics: Documentation"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/4_basic_docs.html#conventions",
    "href": "docs/GitHub/guide/4_basic_docs.html#conventions",
    "title": "GitHub Basics: Documentation",
    "section": "",
    "text": "Generally, for GitHub repos with GitHub pages:\n\nThe repository needs to be public (not private)\nThe main/master branch has the markdown documents under a docs folder at the root level\nRendered HTMLs are hosted under a gh-pages branch at root level",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Basics: Documentation"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/4_basic_docs.html#create-website",
    "href": "docs/GitHub/guide/4_basic_docs.html#create-website",
    "title": "GitHub Basics: Documentation",
    "section": "",
    "text": "The following steps can be followed to build your first website",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Basics: Documentation"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/4_basic_docs.html#add-mkdocs.yaml",
    "href": "docs/GitHub/guide/4_basic_docs.html#add-mkdocs.yaml",
    "title": "GitHub Basics: Documentation",
    "section": "",
    "text": "mkdocs.yaml needs to be added to the root of the master branch. A template of this file is available in the cookiecutter template.\ngit clone https://github.com/CCBR/xyz.git\ncd xyz\nvi mkdocs.yaml\ngit add mkdocs.yaml\ngit commit -m \"adding mkdocs.yaml\"\ngit push\nHere is a sample mkdocs.yaml:\n# Project Information\nsite_name: CCBR How Tos\nsite_author: Vishal Koparde, Ph.D.\nsite_description: &gt;-\n  The **DEVIL** is in the **DETAILS**. Step-by-step detailed How To Guides for data management and other CCBR-relevant tasks.\n\n# Repository\nrepo_name: CCBR/HowTos\nrepo_url: https://github.com/CCBR/HowTos\nedit_uri: https://github.com/CCBR/HowTos/edit/main/docs/\n\n# Copyright\ncopyright: Copyright &copy; 2023 CCBR\n\n# Configuration\ntheme:\n  name: material\n  features:\n    - navigation.tabs\n    - navigation.top\n    - navigation.indexes\n    - toc.integrate \n  palette:\n    - scheme: default\n      primary: indigo\n      accent: indigo\n      toggle:\n        icon: material/toggle-switch-off-outline\n        name: Switch to dark mode\n    - scheme: slate\n      primary: red\n      accent: red\n      toggle:\n        icon: material/toggle-switch\n        name: Switch to light mode\n  logo: images/doc-book.svg\n  favicon: images/favicon.png\n\n# Plugins\nplugins:\n  - search\n  - git-revision-date\n  - minify:\n      minify_html: true\n\n\n# Customization\nextra:\n  social:\n    - icon: fontawesome/solid/users\n      link: http://bioinformatics.cancer.gov\n    - icon: fontawesome/brands/github\n      link: https://github.com/CCRGeneticsBranch\n    - icon: fontawesome/brands/docker\n      link: https://hub.docker.com/orgs/nciccbr/repositories\n  version:\n    provider: mike\n\n\n# Extensions\nmarkdown_extensions:\n  - markdown.extensions.admonition\n  - markdown.extensions.attr_list\n  - markdown.extensions.def_list\n  - markdown.extensions.footnotes\n  - markdown.extensions.meta\n  - markdown.extensions.toc:\n      permalink: true\n  - pymdownx.arithmatex:\n      generic: true\n  - pymdownx.betterem:\n      smart_enable: all\n  - pymdownx.caret\n  - pymdownx.critic\n  - pymdownx.details\n  - pymdownx.emoji:\n      emoji_index: !!python/name:materialx.emoji.twemoji\n      emoji_generator: !!python/name:materialx.emoji.to_svg\n  - pymdownx.highlight\n  - pymdownx.inlinehilite\n  - pymdownx.keys\n  - pymdownx.magiclink:\n      repo_url_shorthand: true\n      user: squidfunk\n      repo: mkdocs-material\n  - pymdownx.mark\n  - pymdownx.smartsymbols\n  - pymdownx.snippets:\n      check_paths: true\n  - pymdownx.superfences\n  - pymdownx.tabbed\n  - pymdownx.tasklist:\n      custom_checkbox: true\n  - pymdownx.tilde\n\n# Page Tree\nnav:\n  - Intro : index.md",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Basics: Documentation"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/4_basic_docs.html#create-index.md",
    "href": "docs/GitHub/guide/4_basic_docs.html#create-index.md",
    "title": "GitHub Basics: Documentation",
    "section": "",
    "text": "Create docs folder, add your index.md there.\nmkdir docs\necho \"### Testing\" &gt; docs/index.md\ngit add docs/index.md\ngit commit -m \"adding landing page\"\ngit push",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Basics: Documentation"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/4_basic_docs.html#build-site",
    "href": "docs/GitHub/guide/4_basic_docs.html#build-site",
    "title": "GitHub Basics: Documentation",
    "section": "",
    "text": "mkdocs can now be used to render .md to HTML\nmkdocs build\nINFO     -  Cleaning site directory\nINFO     -  Building documentation to directory: /Users/$USER/Documents/GitRepos/parkit/site\nINFO     -  Documentation built in 0.32 seconds\nThe above command creates a local site folder which is the root of your “to-be-hosted” website. You can now open the HTMLs in the site folder locally to ensure that that HTML is as per you liking. If not, then you can make edits to the .md files and rebuild the site.\nNOTE: You do not want to push the site folder back to GH and hence it needs to be added to .gitignore file:\necho \"**/site/*\" &gt; .gitignore\ngit add .gitignore\ngit commit -m \"adding .gitignore\"\ngit push",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Basics: Documentation"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/4_basic_docs.html#deploy-site",
    "href": "docs/GitHub/guide/4_basic_docs.html#deploy-site",
    "title": "GitHub Basics: Documentation",
    "section": "",
    "text": "The following command with auto-create a gh-pages branch (if it does not exist) and copy the contents of the site folder to the root of that branch. It will also provide you the URL to your newly created website.\nmkdocs gh-deploy\nINFO     -  Cleaning site directory\nINFO     -  Building documentation to directory: /Users/kopardevn/Documents/GitRepos/xyz/site\nINFO     -  Documentation built in 0.34 seconds\nWARNING  -  Version check skipped: No version specified in previous deployment.\nINFO     -  Copying '/Users/kopardevn/Documents/GitRepos/xyz/site' to 'gh-pages' branch and pushing to\n            GitHub.\nEnumerating objects: 51, done.\nCounting objects: 100(51/51), done.\nDelta compression using up to 16 threads\nCompressing objects: 100(47/47), done.\nWriting objects: 100(51/51), 441.71 KiB | 4.29 MiB/s, done.\nTotal 51 (delta 4), reused 0 (delta 0), pack-reused 0\nremote: Resolving deltas: 100(4/4), done.\nremote:\nremote: Create a pull request for 'gh-pages' on GitHub by visiting:\nremote:      https://github.com/CCBR/xyz/pull/new/gh-pages\nremote:\nTo https://github.com/CCBR/xyz.git\n * [new branch]      gh-pages -&gt; gh-pages\nINFO     -  Your documentation should shortly be available at: https://CCBR.github.io/xyz/\nNow if you point your web browser to the URL from gh-deploy command (IE https://CCBR.github.io/xyz/) you will see your HTML hosted on GitHub. After creating your docs, the cookiecutter template includes a GitHub action which will automatically perform the above tasks whenever a push is performed to the main branch.",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Basics: Documentation"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/4_basic_docs.html#add-to-the-github-page",
    "href": "docs/GitHub/guide/4_basic_docs.html#add-to-the-github-page",
    "title": "GitHub Basics: Documentation",
    "section": "",
    "text": "Go to the main GitHub page of your repository\nOn the top right select the gear icon next to About\nUnder Website, select Use your GitHub Pages website.\nSelect Save Changes",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Basics: Documentation"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/0_overview.html",
    "href": "docs/GitHub/guide/0_overview.html",
    "title": "Overview of GitHub Topics",
    "section": "",
    "text": "Preparing your environment:\n\nDescribes how to create a PAT, add GH to your bash profile and use password-less login features\n\nBasic Commands:\n\nDescribes basic functions to be used with this SOP",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "Overview of GitHub Topics"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/0_overview.html#getting-set-up-and-familiar-with-github",
    "href": "docs/GitHub/guide/0_overview.html#getting-set-up-and-familiar-with-github",
    "title": "Overview of GitHub Topics",
    "section": "",
    "text": "Preparing your environment:\n\nDescribes how to create a PAT, add GH to your bash profile and use password-less login features\n\nBasic Commands:\n\nDescribes basic functions to be used with this SOP",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "Overview of GitHub Topics"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/0_overview.html#basics",
    "href": "docs/GitHub/guide/0_overview.html#basics",
    "title": "Overview of GitHub Topics",
    "section": "Basics",
    "text": "Basics\n\nCreating your GitHub repo:\n\nProvides information on how to setup a GitHub repository under CCBR and the use of templates\n\nCreating your Documentation:\n\nProvides information on how to setup documentation under your repository; provided with all template repos\n\nGitHub Actions:\n\nProvides information for using GitHub Actions under your repository; provided with all template repos",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "Overview of GitHub Topics"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/0_overview.html#best-practices",
    "href": "docs/GitHub/guide/0_overview.html#best-practices",
    "title": "Overview of GitHub Topics",
    "section": "Best Practices",
    "text": "Best Practices\n\nUse Pre-commit Hooks\nCCBR Projects, new Pipelines\nTechDev",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "Overview of GitHub Topics"
    ]
  },
  {
    "objectID": "docs/GitHub/github-actions-demo.html",
    "href": "docs/GitHub/github-actions-demo.html",
    "title": "GitHub Actions demo",
    "section": "",
    "text": "Demo: http://sovacool.dev/gh-actions-demo/\nExample code and worfklow files used in the demo: https://github.com/kelly-sovacool/gh-actions-sandbox\nPresented as a 1-hour live demo for the CCR Bioinformatics Training and Education Program\n\nrecording link: https://cbiit.webex.com/cbiit/ldr.php?RCID=3c728d9bd6f2efc3ecad6b72a807a7c2",
    "crumbs": [
      "Home",
      "GitHub",
      "GitHub Actions demo"
    ]
  },
  {
    "objectID": "docs/containers/singularity-biowulf.html",
    "href": "docs/containers/singularity-biowulf.html",
    "title": "How to use containers with singularity on biowulf",
    "section": "",
    "text": "Log in to biowulf and start an interactive session.\nLoad the singularity module\nPull the container image from dockerhub:\nSingularity will convert the docker image to a SIF file and write it in your current directory.\nRun the container in an interactive bash shell:\nAlternatively, you can pull the container and run it interactively in one command like this:\nRun any commands you wish to while inside the container.\nYou can exit the container by typing exit and pressing enter.",
    "crumbs": [
      "Home",
      "Containers",
      "How to use containers with singularity on biowulf"
    ]
  },
  {
    "objectID": "docs/containers/singularity-biowulf.html#cache-directory",
    "href": "docs/containers/singularity-biowulf.html#cache-directory",
    "title": "How to use containers with singularity on biowulf",
    "section": "Cache directory",
    "text": "Cache directory\nSingularity caches images in a directory set by the SINGULARITY_CACHEDIR environment variable. By default, this is set to $HOME/.singularity. However, there is limited space in user’s home directories. Instead, we recommend setting it to a location with more disk space, such as /data/$USER/.singularity.\nYou can set this custom cache directory location by pasting the following line in your ~/.bashrc or ~/.zshrc:\nexport SINGULARITY_CACHEDIR=/data/$USER/.singularity\nThis will take effect if you log out and log back in, or source your rc file. You can check the cache directory variable like so:\necho $SINGULARITY_CACHEDIR\n\n/data/$USER/.singularity\n\n(Your own username will appear instead of $USER.)",
    "crumbs": [
      "Home",
      "Containers",
      "How to use containers with singularity on biowulf"
    ]
  },
  {
    "objectID": "docs/containers/singularity-biowulf.html#package-versions",
    "href": "docs/containers/singularity-biowulf.html#package-versions",
    "title": "How to use containers with singularity on biowulf",
    "section": "Package versions",
    "text": "Package versions\n\nR\nYou can find out the versions of R packages installed in a container like so:\nsingularity exec docker://nciccbr/carlisle_r:v2 R -e \"readr::write_tsv(tibble::as_tibble(installed.packages()), 'r-packages.tsv')\"\nThis command runs R code in the container to write the installed packages to a TSV file called r-packages.tsv.\n\n\nPython\nRun this command to find out the versions of Python packages installed in a container:\nsingularity exec docker://nciccbr/ccbr_ubuntu_22.04:v4 pip list &gt; pip_list.txt\nThe packages & versions are written to pip_list.txt.",
    "crumbs": [
      "Home",
      "Containers",
      "How to use containers with singularity on biowulf"
    ]
  },
  {
    "objectID": "docs/R-package/create-from-conda/0_setup.html",
    "href": "docs/R-package/create-from-conda/0_setup.html",
    "title": "setup",
    "section": "",
    "text": "You will first need to set up Conda in order to use the Conda tools for creating your Conda package.\nThe documentation for getting started can be found here, including installation guidelines.\n\n\n\n\nIn a space shared with other users that may use Conda, your personal Conda cache needs to be specified. To edit how your cache is saved perform the following steps:\n\nCreate a new directory where you would like to store the conda cache called ‘conda-cache’\n\nmkdir conda/conda-cache\n\nIn your home directory, create the file .condarc\n\ntouch ~/.condarc\n\nOpen the new file .condarc and add the following sections:\n\n\npkgs_dirs\nenvs_dirs\nconda-build\nchannels\n\nIn each section you will add the path to the directories you would like to use for each section.\nExample:\nkgs_dirs:\n  - /rstudio-files/ccbr-data/users/$USER/conda-cache\nenvs_dirs:\n  - /rstudio-files/ccbr-data/users/$USER/conda-envs\nconda-build:\n    root-dir: /rstudio-files/ccbr-data/users/$USER/conda-bld\n    build_folder: /rstudio-files/ccbr-data/users/$USER/conda-bld\n    conda-build\n  output_folder: /rstudio-files/ccbr-data/users/$USER/conda-bld/conda-output\nchannels:\n  - file://rstudio-files/RH/ccbr-projects/Conda_package_tutorial/local_channel/channel\n  - conda-forge\n  - bioconda\n  - defaults\n\n\n\nTo check that conda has been setup with the specified paths from .condarc start conda:\nconda activate\nThen check the conda info:\nconda info\n\n\nTo build a Conda package, ‘channels’ are needed to supply the dependencies that are specified in the DESCRIPTION and meta.yaml files (discussed below). These R packages in these channels are also Conda packages that have been previously built as a specific version of that package and given a ‘build string’, a unique indentifier for the build of that specific conda package.\nFor channels to be available to you when you build your own conda package, you first need to add them. To add a Conda channel run:\nconfig –add channels \nFor , some examples are:\n\nfile://rstudio-files/RH/ccbr-projects/Conda_package_tutorial/local_channel/channel\nconda-forge\nbioconda\ndefaults\n\n\n\n\n\nTo create a package with Conda, you first need to make a new release and tag in the github repo of the R package you would like to create into a Conda Package.\nFor more information on best practices in R package creation, see:\nhttps://r-pkgs.org/whole-game.html\nBefore creating the release on github, please check for proper dependencies listed in the files NAMESPACE and DESCRIPTION.\nThese files should have the same list of dependencies, and the version numbers for dependencies can be specified in the DESCRIPTION file. The DESCRIPTION file must be editted manually, while the NAMESPACE file should not be editted manually, but rather created automatically using the document() function.\nThe DESCRITPION file must also be correctly formatted. For more information, see:\nhttps://r-pkgs.org/description.html\n\n\nTo download the most recent release from the most recent tag on Github, activate Conda then use Conda skeleton like so:\nconda activate \n\nconda skeleton cran $githubURL\nReplace $githubURL with the URL to your R package’s github repo.\nA folder is then created for the downloaded release, for example running the following:\nconda skeleton cran https://github.com/NIDAP-Community/DSPWorkflow\ncreates the folder\nr-dspworkflow\nWithin this newly created folder is a file named “meta.yaml”. You will need to edit this file to include the channels and edit any information on the the package version number or dependency version numbers.\nHere is an example of the top of the “meta.yaml” file with the channels section added:\n{% set version = '0.9.5.2' %}\n\n{% set posix = 'm2-' if win else '' %}\n{% set native = 'm2w64-' if win else '' %}\n\npackage:\n  name: r-dspworkflow\n  version: {{ version|replace(\"-\", \"_\") }}\n  \nchannels:\n  - conda-forge\n  - bioconda\n  - default\n  - file://rstudio-files/RH/ccbr-projects/Conda_package_tutorial/local_channel/channel\n\nsource:\n\n  git_url: https://github.com/NIDAP-Community/DSPWorkflow\n  git_tag: 0.9.5\n\nbuild:\n  merge_build_host: True  # [win]\n  # If this is a new build for the same version, increment the build number.\n  number: 0\n  # no skip\n\n  # This is required to make R link correctly on Linux.\n  rpaths:\n    - lib/R/lib/\n    - lib/\n\n    # Suggests: testthat (== 3.1.4)\nrequirements:\n  build:\n    - {{ posix }}filesystem        # [win]\n    - {{ posix }}git\n    - {{ posix }}zip               # [win]\nHere is an example of the sections for specifying dependency versions from the “meta.yaml” file:\n  host:\n    - r-base =4.1.3=h2f963a2_5\n    - bioconductor-biobase =2.54.0=r41hc0cfd56_2\n    - bioconductor-biocgenerics =0.40.0=r41hdfd78af_0\n    - bioconductor-geomxtools =3.1.1=r41hdfd78af_0\n    - bioconductor-nanostringnctools =1.2.0\n    - bioconductor-spatialdecon =1.4.3\n    - bioconductor-complexheatmap =2.10.0=r41hdfd78af_0\n    - r-cowplot =1.1.1=r41hc72bb7e_1\n    - r-dplyr =1.0.9=r41h7525677_0\n    - r-ggforce =0.3.4=r41h7525677_0\n    - r-ggplot2 =3.3.6=r41hc72bb7e_1\n    - r-gridextra =2.3=r41hc72bb7e_1004\n    - r-gtable =0.3.0=r41hc72bb7e_3\n    - r-knitr =1.40=r41hc72bb7e_1\n    - r-patchwork =1.1.2=r41hc72bb7e_1\n    - r-reshape2 =1.4.4=r41h7525677_2\n    - r-scales =1.2.1=r41hc72bb7e_1\n    - r-tibble =3.1.8=r41h06615bd_1\n    - r-tidyr =1.2.1=r41h7525677_1\n    - r-umap =0.2.9.0=r41h7525677_1\n    - r-rtsne =0.16=r41h37cf8d7_1\n    - r-magrittr =2.0.3=r41h06615bd_1\n    - r-rlang =1.1.0=r41h38f115c_0\n    \n  run:\n    - r-base =4.1.3=h2f963a2_5\n    - bioconductor-biobase =2.54.0=r41hc0cfd56_2\n    - bioconductor-biocgenerics =0.40.0=r41hdfd78af_0\n    - bioconductor-geomxtools =3.1.1=r41hdfd78af_0\n    - bioconductor-nanostringnctools =1.2.0\n    - bioconductor-spatialdecon =1.4.3\n    - bioconductor-complexheatmap =2.10.0=r41hdfd78af_0\n    - r-cowplot =1.1.1=r41hc72bb7e_1\n    - r-dplyr =1.0.9=r41h7525677_0\n    - r-ggforce =0.3.4=r41h7525677_0\n    - r-ggplot2 =3.3.6=r41hc72bb7e_1\n    - r-gridextra =2.3=r41hc72bb7e_1004\n    - r-gtable =0.3.0=r41hc72bb7e_3\n    - r-knitr =1.40=r41hc72bb7e_1\n    - r-patchwork =1.1.2=r41hc72bb7e_1\n    - r-reshape2 =1.4.4=r41h7525677_2\n    - r-scales =1.2.1=r41hc72bb7e_1\n    - r-tibble =3.1.8=r41h06615bd_1\n    - r-tidyr =1.2.1=r41h7525677_1\n    - r-umap =0.2.9.0=r41h7525677_1\n    - r-rtsne =0.16=r41h37cf8d7_1\n    - r-magrittr =2.0.3=r41h06615bd_1\n    - r-rlang =1.1.0=r41h38f115c_0\nIn the above example, each of the dependencies has been assigned a conda build string, so that when conda builds a conda package, it will only use that specific build of the dependency from the listed conda channels. The above example is very restrictive, the dependencies can also be listed in the “meta.yaml” file to be more open–it will choose a conda build string that fits in with the other resolved dependency build strings based on what is available in the channels.\nAlso note that the “host” section matches the “run” section.\nHere is some examples of a more open setup for these dependencies:\n  host:\n    - r-base &gt;=4.1.3\n    - bioconductor-biobase &gt;=2.54.0\n    - bioconductor-biocgenerics &gt;=0.40.0\n    - bioconductor-geomxtools &gt;=3.1.1\n    - bioconductor-nanostringnctools &gt;=1.2.0\n    - bioconductor-spatialdecon =1.4.3\n    - bioconductor-complexheatmap &gt;=2.10.0\n    - r-cowplot &gt;=1.1.1\n    - r-dplyr &gt;=1.0.9\n    - r-ggforce &gt;=0.3.4\n    - r-ggplot2 &gt;=3.3.6\n    - r-gridextra &gt;=2.3\n    - r-gtable &gt;=0.3.0\n    - r-knitr &gt;=1.40\n    - r-patchwork &gt;=1.1.2\n    - r-reshape2 &gt;=1.4.4\n    - r-scales &gt;=1.2.1\n    - r-tibble &gt;=3.1.8\n    - r-tidyr &gt;=1.2.1\n    - r-umap &gt;=0.2.9.0\n    - r-rtsne &gt;=0.16\n    - r-magrittr &gt;=2.0.3\n    - r-rlang &gt;=1.1.0\n\n\n\nWhen the “meta.yaml” has been prepared, you can now build the Conda package.\nTo do so, run the command:\nconda-build $r-package 2&gt;&1 | tee $build_log_name.log\nReplace $r-package with the name of the R package folder that was created after running conda skeleton (the folder where the meta.yaml is located).\nExample: r-dspworkflow\nReplace $build_log_name.log with the name for the log file, such as the date, time, and initials.\nExample: 05_12_23_330_nc.log\nThe log file will list how conda has built the package, including what dependencies version numbers and corresponding build strings were used to resolve the conda environment. These dependencies are what we specified in the “meta.yaml” file. The log file will be useful troubleshooting a failed build.\nBe aware, the build can take anywhere from several minutes to an hour to complete, depending on the size of the package and the number of dependencies.\nThe conda package will be built as a tar.bz2 file.\n\n\n\n\n\n\nAn important consideration for Conda builds is the list of dependencies, specified versions, and compatibility with each of the other dependencies.\nIf the meta.yaml and DESCRIPTION file specify specific package versions, Conda’s ability to resolve the Conda environment also becomes more limited.\nFor example, if the Conda package we are building has the following requirements:\nDependency A version == 1.0\nDependency B version &gt;= 2.5\nAnd the Dependencies located in our Conda channel have the following dependencies:\nDependency A version 1.0\n  - Dependency C version == 0.5\n\nDependency A version 1.2\n- Dependency C version &gt;= 0.7\n\nDependency B version 2.7\n  - Dependency C version &gt;= 0.7\nAs you can see, the Conda build will not be able to resolve the environment because Dependency A verion 1.0 needs an old version of Dependency C, while Dependency B version 2.7 needs a newer version.\nIn this case, if we changed our package’s DESCRIPTION and meta.yaml file to be:\nDependency A version &gt;= 1.0\nDependency B version &gt;= 2.5\nThe conda build will be able to resolve. This is a simplied version of a what are more often complex dependency structures, but it is an important concept in conda package building that will inevitably arise as a package’s dependencies become more specific.\nTo check on the versions of packages that are available in a Conda channel, use the command:\nconda search $dependency\nReplace $dependency with the name of package you would like to investigate. There are more optional commands for this function which can be found here:\nhttps://docs.conda.io/projects/conda/en/latest/commands/search.html\nTo check the dependencies of packages that exist in your Conda cache, go to the folder specified for your conda cache (that we specified earlier). In case you need to find that path you can use:\nconda info\nHere there will be a folder for each of the packages that has been used in a conda build (including the dependencies). In each folder is another folder called “info” and a file called “index.json” that lists information, such as depends for the package.\nHere is an example:\ncat /rstudio-files/ccbr-data/users/$USER/conda-cache/r-ggplot2-3.3.6-r41hc72bb7e_1/info/index.json\n\n{\n \"arch\": null,\n \"build\": \"r41hc72bb7e_1\",\n \"build_number\": 1,\n \"depends\": [\n   \"r-base &gt;=4.1,&lt;4.2.0a0\",\n   \"r-digest\",\n   \"r-glue\",\n   \"r-gtable &gt;=0.1.1\",\n   \"r-isoband\",\n   \"r-mass\",\n   \"r-mgcv\",\n   \"r-rlang &gt;=0.3.0\",\n   \"r-scales &gt;=0.5.0\",\n   \"r-tibble\",\n   \"r-withr &gt;=2.0.0\"\n ],\n \"license\": \"GPL-2.0-only\",\n \"license_family\": \"GPL2\",\n \"name\": \"r-ggplot2\",\n \"noarch\": \"generic\",\n \"platform\": null,\n \"subdir\": \"noarch\",\n \"timestamp\": 1665515494942,\n \"version\": \"3.3.6\"\n}\nIf you would like to specify an exact package to use in a conda channel for your conda build, specify the build string in your “meta.yaml” file. In the above example for ggplot version 3.3.6, the build string is listed in the folder name for package as well as in the index.json file for “build:”r41hc72bb7e_1”.",
    "crumbs": [
      "Home",
      "R Package",
      "Create from Conda",
      "setup"
    ]
  },
  {
    "objectID": "docs/R-package/create-from-conda/0_setup.html#set-up-conda-tools-installation-if-needed",
    "href": "docs/R-package/create-from-conda/0_setup.html#set-up-conda-tools-installation-if-needed",
    "title": "setup",
    "section": "",
    "text": "You will first need to set up Conda in order to use the Conda tools for creating your Conda package.\nThe documentation for getting started can be found here, including installation guidelines.",
    "crumbs": [
      "Home",
      "R Package",
      "Create from Conda",
      "setup"
    ]
  },
  {
    "objectID": "docs/R-package/create-from-conda/0_setup.html#set-up-conda-cache",
    "href": "docs/R-package/create-from-conda/0_setup.html#set-up-conda-cache",
    "title": "setup",
    "section": "",
    "text": "In a space shared with other users that may use Conda, your personal Conda cache needs to be specified. To edit how your cache is saved perform the following steps:\n\nCreate a new directory where you would like to store the conda cache called ‘conda-cache’\n\nmkdir conda/conda-cache\n\nIn your home directory, create the file .condarc\n\ntouch ~/.condarc\n\nOpen the new file .condarc and add the following sections:\n\n\npkgs_dirs\nenvs_dirs\nconda-build\nchannels\n\nIn each section you will add the path to the directories you would like to use for each section.\nExample:\nkgs_dirs:\n  - /rstudio-files/ccbr-data/users/$USER/conda-cache\nenvs_dirs:\n  - /rstudio-files/ccbr-data/users/$USER/conda-envs\nconda-build:\n    root-dir: /rstudio-files/ccbr-data/users/$USER/conda-bld\n    build_folder: /rstudio-files/ccbr-data/users/$USER/conda-bld\n    conda-build\n  output_folder: /rstudio-files/ccbr-data/users/$USER/conda-bld/conda-output\nchannels:\n  - file://rstudio-files/RH/ccbr-projects/Conda_package_tutorial/local_channel/channel\n  - conda-forge\n  - bioconda\n  - defaults",
    "crumbs": [
      "Home",
      "R Package",
      "Create from Conda",
      "setup"
    ]
  },
  {
    "objectID": "docs/R-package/create-from-conda/0_setup.html#check-conda-setup",
    "href": "docs/R-package/create-from-conda/0_setup.html#check-conda-setup",
    "title": "setup",
    "section": "",
    "text": "To check that conda has been setup with the specified paths from .condarc start conda:\nconda activate\nThen check the conda info:\nconda info\n\n\nTo build a Conda package, ‘channels’ are needed to supply the dependencies that are specified in the DESCRIPTION and meta.yaml files (discussed below). These R packages in these channels are also Conda packages that have been previously built as a specific version of that package and given a ‘build string’, a unique indentifier for the build of that specific conda package.\nFor channels to be available to you when you build your own conda package, you first need to add them. To add a Conda channel run:\nconfig –add channels \nFor , some examples are:\n\nfile://rstudio-files/RH/ccbr-projects/Conda_package_tutorial/local_channel/channel\nconda-forge\nbioconda\ndefaults",
    "crumbs": [
      "Home",
      "R Package",
      "Create from Conda",
      "setup"
    ]
  },
  {
    "objectID": "docs/R-package/create-from-conda/0_setup.html#conda-r-package-creation",
    "href": "docs/R-package/create-from-conda/0_setup.html#conda-r-package-creation",
    "title": "setup",
    "section": "",
    "text": "To create a package with Conda, you first need to make a new release and tag in the github repo of the R package you would like to create into a Conda Package.\nFor more information on best practices in R package creation, see:\nhttps://r-pkgs.org/whole-game.html\nBefore creating the release on github, please check for proper dependencies listed in the files NAMESPACE and DESCRIPTION.\nThese files should have the same list of dependencies, and the version numbers for dependencies can be specified in the DESCRIPTION file. The DESCRIPTION file must be editted manually, while the NAMESPACE file should not be editted manually, but rather created automatically using the document() function.\nThe DESCRITPION file must also be correctly formatted. For more information, see:\nhttps://r-pkgs.org/description.html\n\n\nTo download the most recent release from the most recent tag on Github, activate Conda then use Conda skeleton like so:\nconda activate \n\nconda skeleton cran $githubURL\nReplace $githubURL with the URL to your R package’s github repo.\nA folder is then created for the downloaded release, for example running the following:\nconda skeleton cran https://github.com/NIDAP-Community/DSPWorkflow\ncreates the folder\nr-dspworkflow\nWithin this newly created folder is a file named “meta.yaml”. You will need to edit this file to include the channels and edit any information on the the package version number or dependency version numbers.\nHere is an example of the top of the “meta.yaml” file with the channels section added:\n{% set version = '0.9.5.2' %}\n\n{% set posix = 'm2-' if win else '' %}\n{% set native = 'm2w64-' if win else '' %}\n\npackage:\n  name: r-dspworkflow\n  version: {{ version|replace(\"-\", \"_\") }}\n  \nchannels:\n  - conda-forge\n  - bioconda\n  - default\n  - file://rstudio-files/RH/ccbr-projects/Conda_package_tutorial/local_channel/channel\n\nsource:\n\n  git_url: https://github.com/NIDAP-Community/DSPWorkflow\n  git_tag: 0.9.5\n\nbuild:\n  merge_build_host: True  # [win]\n  # If this is a new build for the same version, increment the build number.\n  number: 0\n  # no skip\n\n  # This is required to make R link correctly on Linux.\n  rpaths:\n    - lib/R/lib/\n    - lib/\n\n    # Suggests: testthat (== 3.1.4)\nrequirements:\n  build:\n    - {{ posix }}filesystem        # [win]\n    - {{ posix }}git\n    - {{ posix }}zip               # [win]\nHere is an example of the sections for specifying dependency versions from the “meta.yaml” file:\n  host:\n    - r-base =4.1.3=h2f963a2_5\n    - bioconductor-biobase =2.54.0=r41hc0cfd56_2\n    - bioconductor-biocgenerics =0.40.0=r41hdfd78af_0\n    - bioconductor-geomxtools =3.1.1=r41hdfd78af_0\n    - bioconductor-nanostringnctools =1.2.0\n    - bioconductor-spatialdecon =1.4.3\n    - bioconductor-complexheatmap =2.10.0=r41hdfd78af_0\n    - r-cowplot =1.1.1=r41hc72bb7e_1\n    - r-dplyr =1.0.9=r41h7525677_0\n    - r-ggforce =0.3.4=r41h7525677_0\n    - r-ggplot2 =3.3.6=r41hc72bb7e_1\n    - r-gridextra =2.3=r41hc72bb7e_1004\n    - r-gtable =0.3.0=r41hc72bb7e_3\n    - r-knitr =1.40=r41hc72bb7e_1\n    - r-patchwork =1.1.2=r41hc72bb7e_1\n    - r-reshape2 =1.4.4=r41h7525677_2\n    - r-scales =1.2.1=r41hc72bb7e_1\n    - r-tibble =3.1.8=r41h06615bd_1\n    - r-tidyr =1.2.1=r41h7525677_1\n    - r-umap =0.2.9.0=r41h7525677_1\n    - r-rtsne =0.16=r41h37cf8d7_1\n    - r-magrittr =2.0.3=r41h06615bd_1\n    - r-rlang =1.1.0=r41h38f115c_0\n    \n  run:\n    - r-base =4.1.3=h2f963a2_5\n    - bioconductor-biobase =2.54.0=r41hc0cfd56_2\n    - bioconductor-biocgenerics =0.40.0=r41hdfd78af_0\n    - bioconductor-geomxtools =3.1.1=r41hdfd78af_0\n    - bioconductor-nanostringnctools =1.2.0\n    - bioconductor-spatialdecon =1.4.3\n    - bioconductor-complexheatmap =2.10.0=r41hdfd78af_0\n    - r-cowplot =1.1.1=r41hc72bb7e_1\n    - r-dplyr =1.0.9=r41h7525677_0\n    - r-ggforce =0.3.4=r41h7525677_0\n    - r-ggplot2 =3.3.6=r41hc72bb7e_1\n    - r-gridextra =2.3=r41hc72bb7e_1004\n    - r-gtable =0.3.0=r41hc72bb7e_3\n    - r-knitr =1.40=r41hc72bb7e_1\n    - r-patchwork =1.1.2=r41hc72bb7e_1\n    - r-reshape2 =1.4.4=r41h7525677_2\n    - r-scales =1.2.1=r41hc72bb7e_1\n    - r-tibble =3.1.8=r41h06615bd_1\n    - r-tidyr =1.2.1=r41h7525677_1\n    - r-umap =0.2.9.0=r41h7525677_1\n    - r-rtsne =0.16=r41h37cf8d7_1\n    - r-magrittr =2.0.3=r41h06615bd_1\n    - r-rlang =1.1.0=r41h38f115c_0\nIn the above example, each of the dependencies has been assigned a conda build string, so that when conda builds a conda package, it will only use that specific build of the dependency from the listed conda channels. The above example is very restrictive, the dependencies can also be listed in the “meta.yaml” file to be more open–it will choose a conda build string that fits in with the other resolved dependency build strings based on what is available in the channels.\nAlso note that the “host” section matches the “run” section.\nHere is some examples of a more open setup for these dependencies:\n  host:\n    - r-base &gt;=4.1.3\n    - bioconductor-biobase &gt;=2.54.0\n    - bioconductor-biocgenerics &gt;=0.40.0\n    - bioconductor-geomxtools &gt;=3.1.1\n    - bioconductor-nanostringnctools &gt;=1.2.0\n    - bioconductor-spatialdecon =1.4.3\n    - bioconductor-complexheatmap &gt;=2.10.0\n    - r-cowplot &gt;=1.1.1\n    - r-dplyr &gt;=1.0.9\n    - r-ggforce &gt;=0.3.4\n    - r-ggplot2 &gt;=3.3.6\n    - r-gridextra &gt;=2.3\n    - r-gtable &gt;=0.3.0\n    - r-knitr &gt;=1.40\n    - r-patchwork &gt;=1.1.2\n    - r-reshape2 &gt;=1.4.4\n    - r-scales &gt;=1.2.1\n    - r-tibble &gt;=3.1.8\n    - r-tidyr &gt;=1.2.1\n    - r-umap &gt;=0.2.9.0\n    - r-rtsne &gt;=0.16\n    - r-magrittr &gt;=2.0.3\n    - r-rlang &gt;=1.1.0\n\n\n\nWhen the “meta.yaml” has been prepared, you can now build the Conda package.\nTo do so, run the command:\nconda-build $r-package 2&gt;&1 | tee $build_log_name.log\nReplace $r-package with the name of the R package folder that was created after running conda skeleton (the folder where the meta.yaml is located).\nExample: r-dspworkflow\nReplace $build_log_name.log with the name for the log file, such as the date, time, and initials.\nExample: 05_12_23_330_nc.log\nThe log file will list how conda has built the package, including what dependencies version numbers and corresponding build strings were used to resolve the conda environment. These dependencies are what we specified in the “meta.yaml” file. The log file will be useful troubleshooting a failed build.\nBe aware, the build can take anywhere from several minutes to an hour to complete, depending on the size of the package and the number of dependencies.\nThe conda package will be built as a tar.bz2 file.",
    "crumbs": [
      "Home",
      "R Package",
      "Create from Conda",
      "setup"
    ]
  },
  {
    "objectID": "docs/R-package/create-from-conda/0_setup.html#common-issues",
    "href": "docs/R-package/create-from-conda/0_setup.html#common-issues",
    "title": "setup",
    "section": "",
    "text": "An important consideration for Conda builds is the list of dependencies, specified versions, and compatibility with each of the other dependencies.\nIf the meta.yaml and DESCRIPTION file specify specific package versions, Conda’s ability to resolve the Conda environment also becomes more limited.\nFor example, if the Conda package we are building has the following requirements:\nDependency A version == 1.0\nDependency B version &gt;= 2.5\nAnd the Dependencies located in our Conda channel have the following dependencies:\nDependency A version 1.0\n  - Dependency C version == 0.5\n\nDependency A version 1.2\n- Dependency C version &gt;= 0.7\n\nDependency B version 2.7\n  - Dependency C version &gt;= 0.7\nAs you can see, the Conda build will not be able to resolve the environment because Dependency A verion 1.0 needs an old version of Dependency C, while Dependency B version 2.7 needs a newer version.\nIn this case, if we changed our package’s DESCRIPTION and meta.yaml file to be:\nDependency A version &gt;= 1.0\nDependency B version &gt;= 2.5\nThe conda build will be able to resolve. This is a simplied version of a what are more often complex dependency structures, but it is an important concept in conda package building that will inevitably arise as a package’s dependencies become more specific.\nTo check on the versions of packages that are available in a Conda channel, use the command:\nconda search $dependency\nReplace $dependency with the name of package you would like to investigate. There are more optional commands for this function which can be found here:\nhttps://docs.conda.io/projects/conda/en/latest/commands/search.html\nTo check the dependencies of packages that exist in your Conda cache, go to the folder specified for your conda cache (that we specified earlier). In case you need to find that path you can use:\nconda info\nHere there will be a folder for each of the packages that has been used in a conda build (including the dependencies). In each folder is another folder called “info” and a file called “index.json” that lists information, such as depends for the package.\nHere is an example:\ncat /rstudio-files/ccbr-data/users/$USER/conda-cache/r-ggplot2-3.3.6-r41hc72bb7e_1/info/index.json\n\n{\n \"arch\": null,\n \"build\": \"r41hc72bb7e_1\",\n \"build_number\": 1,\n \"depends\": [\n   \"r-base &gt;=4.1,&lt;4.2.0a0\",\n   \"r-digest\",\n   \"r-glue\",\n   \"r-gtable &gt;=0.1.1\",\n   \"r-isoband\",\n   \"r-mass\",\n   \"r-mgcv\",\n   \"r-rlang &gt;=0.3.0\",\n   \"r-scales &gt;=0.5.0\",\n   \"r-tibble\",\n   \"r-withr &gt;=2.0.0\"\n ],\n \"license\": \"GPL-2.0-only\",\n \"license_family\": \"GPL2\",\n \"name\": \"r-ggplot2\",\n \"noarch\": \"generic\",\n \"platform\": null,\n \"subdir\": \"noarch\",\n \"timestamp\": 1665515494942,\n \"version\": \"3.3.6\"\n}\nIf you would like to specify an exact package to use in a conda channel for your conda build, specify the build string in your “meta.yaml” file. In the above example for ggplot version 3.3.6, the build string is listed in the folder name for package as well as in the index.json file for “build:”r41hc72bb7e_1”.",
    "crumbs": [
      "Home",
      "R Package",
      "Create from Conda",
      "setup"
    ]
  },
  {
    "objectID": "docs/UCSC/1_creating_inputs.html",
    "href": "docs/UCSC/1_creating_inputs.html",
    "title": "CCBR How-Tos",
    "section": "",
    "text": "In order to use the genomic broswer features, sample files must be created.\n\n\nFor individual samples, where peak density is to be observed, bigwig formatted files must be generated. If using the CCBR pipelines these are automatically generated as outputs of the pipeline (WORKDIR/results/bigwig). In many cases, scaling or normalization of bigwig is required to visualize multiple samples in comparison with each other. See various deeptools options for details/ideas. If not using CCBR pipelines, example code is provided below for the file generation.\nmodue load ucsc\n        \nfragments_bed=\"/path/to/sample1.fragments.bed\"\nbw=\"/path/to/sample1.bigwig\"\ngenome_len=\"numeric_genome_length\"\nbg=\"/path/to/sample1.bedgraph\"\nbw=\"/path/to/sample2.bigwig\"\n\n# if using a spike-in scale, the scaling factor should be applied\n# while not required, it is recommended for CUT&RUN experiements\nspikein_scale=\"spike_in_value\"\n\n# create bed file\nbedtools genomecov -bg -scale $spikein_scale -i $fragments_bed -g $genome_len &gt; $bg\n\n# create bigwig file\nbedGraphToBigWig $bg $genome_len $bw\n\n\n\nFor contrasts, where peak differences are to be observed, bigbed formatted files must be generated. If using the CCBR/CARLISLE pipeline these are automatically generated as outputs of the pipeline (WORKDIR/results/peaks/contrasts/contrast_id/). If not using this pipeline, example code is provided below for the file generation.\nmodule load ucsc\n\nbed=\"/path/to/sample1_vs_sample2_fragmentsbased_diffresults.bed\"\nbigbed=\"/path/to/output/sample1_vs_sample2_fragmentsbased_diffresults.bigbed\"\ngenome_len=\"numeric_genome_length\"\n\n# create bigbed file\nbedToBigBed -type=bed9 $bed $genome_len $bigbed",
    "crumbs": [
      "Home",
      "UCSC",
      "Generating Inputs"
    ]
  },
  {
    "objectID": "docs/UCSC/1_creating_inputs.html#generating-inputs",
    "href": "docs/UCSC/1_creating_inputs.html#generating-inputs",
    "title": "CCBR How-Tos",
    "section": "",
    "text": "In order to use the genomic broswer features, sample files must be created.\n\n\nFor individual samples, where peak density is to be observed, bigwig formatted files must be generated. If using the CCBR pipelines these are automatically generated as outputs of the pipeline (WORKDIR/results/bigwig). In many cases, scaling or normalization of bigwig is required to visualize multiple samples in comparison with each other. See various deeptools options for details/ideas. If not using CCBR pipelines, example code is provided below for the file generation.\nmodue load ucsc\n        \nfragments_bed=\"/path/to/sample1.fragments.bed\"\nbw=\"/path/to/sample1.bigwig\"\ngenome_len=\"numeric_genome_length\"\nbg=\"/path/to/sample1.bedgraph\"\nbw=\"/path/to/sample2.bigwig\"\n\n# if using a spike-in scale, the scaling factor should be applied\n# while not required, it is recommended for CUT&RUN experiements\nspikein_scale=\"spike_in_value\"\n\n# create bed file\nbedtools genomecov -bg -scale $spikein_scale -i $fragments_bed -g $genome_len &gt; $bg\n\n# create bigwig file\nbedGraphToBigWig $bg $genome_len $bw\n\n\n\nFor contrasts, where peak differences are to be observed, bigbed formatted files must be generated. If using the CCBR/CARLISLE pipeline these are automatically generated as outputs of the pipeline (WORKDIR/results/peaks/contrasts/contrast_id/). If not using this pipeline, example code is provided below for the file generation.\nmodule load ucsc\n\nbed=\"/path/to/sample1_vs_sample2_fragmentsbased_diffresults.bed\"\nbigbed=\"/path/to/output/sample1_vs_sample2_fragmentsbased_diffresults.bigbed\"\ngenome_len=\"numeric_genome_length\"\n\n# create bigbed file\nbedToBigBed -type=bed9 $bed $genome_len $bigbed",
    "crumbs": [
      "Home",
      "UCSC",
      "Generating Inputs"
    ]
  },
  {
    "objectID": "docs/UCSC/1_creating_inputs.html#sharing-data",
    "href": "docs/UCSC/1_creating_inputs.html#sharing-data",
    "title": "CCBR How-Tos",
    "section": "Sharing data",
    "text": "Sharing data\nFor all sample types, data must be stored on a shared directory. It is recommended that symlnks be created from the source location to this shared directory to ensure that minial disc space is being used. Example code for creating symlinks is provided below.\n\nsingle sample\n# single sample\n## set source file location\nsource_loc=\"/WORKDIR/results/bigwig/sample1.bigwig \"\n\n## set destination link location\nlink_loc=\"/SHAREDDIR/bigwig/sample1.bigwig\"\n\n## create hard links\nln $source_loc $link_loc\n\n\ncontrast sample\n# contrast\n## set source file location\nsource_loc=\"WORKDIR/results/peaks/contrasts/sample1_vs_sample2/sample1_vs_sample2_fragmentsbased_diffresults.bigbed \"\n\n## set destination link location\nlink_loc=\"/SHAREDDIR/bigbed/sample1_vs_sample2.bigbed\"\n\n## create hard links\nln $source_loc $link_loc\nOnce the links have been generated, the data folder must be open to read and write access.\n## set destination link location\nlink_loc=\"/SHAREDDIR/bigbed/\"\n\n# open dir\nchmod -R a+rX $link_loc",
    "crumbs": [
      "Home",
      "UCSC",
      "Generating Inputs"
    ]
  },
  {
    "objectID": "docs/UCSC/2_creating_track_info.html",
    "href": "docs/UCSC/2_creating_track_info.html",
    "title": "Generating Track information",
    "section": "",
    "text": "It’s recommended to create a text file of all sample track information to ease in editing and submission to the UCSC browser website. A single line of code is needed for each sample which will provide the track location, sample name, description of the sample, whether to autoscale the samples, max height of the samples, view limits, and color. An example is provided below.\ntrack type=bigWig bigDataUrl=https://hpc.nih.gov/~CCBR/ccbr1155/${dir_loc}/bigwig/${complete_sample_id}.bigwig name=${sample_id} description=${sample_id} visibility=full autoScale=off maxHeightPixels=128:30:1 viewLimits=1:120 color=65,105,225\nUsers may find it helpful to create a single script which would create this text file for all samples. An example of this is listed below, which assumes that input files were generated using the CARLISLE pipeline. It can be edited to adapt to other output files, as needed.\nGenerally, each “track” line should have at least the following key value pairs: - name : label for the track - description : defines the center lable displayed - type : BAM, BED, bigBed, bigWig, etc. - bigDataUrl : URL of the data file - for other options see here\n\n\n\n\nsamples_list.txt: a single column text file with sampleID’s\ntrack_dir: path to the linked files\ntrack_output: path to output file\npeak_list: all peak types to be included\nmethod_list: what method to be included\ndedupe_list: type of duplication to be included\n\n# input arguments\nsample_list_input=/\"path/to/samples.txt\"\ntrack_dir=\"/path/to/shared/dir/\"\ntrack_output=\"/path/to/output/file/tracks.txt\npeak_list=(\"norm.relaxed.bed\" \"narrowPeak\" \"broadGo_peaks.bed\" \"narrowGo_peaks.bed\")\nmethod_list=(\"fragmentsbased\")\ndedup_list=(\"dedup\")\n\n# read sample file\nIFS=$'\\n' read -d '' -r -a sample_list &lt; $sample_list_input\n\nrun_sample_tracks (){\n    sample_id=$1\n    dedup_id=$2\n    \n    # sample name\n    # eg siNC_H3K27Ac_1.dedup.bigwig\n    complete_sample_id=\"${sample_id}.${dedup_id}\"\n\n    # set link location\n    link_loc=\"${track_dir}/bigwig/${complete_sample_id}.bigwig\"\n\n    # echo track info\n    echo \"track type=bigWig bigDataUrl=https://hpc.nih.gov/~CCBR/ccbr1155/${dir_loc}/bigwig/${complete_sample_id}.bigwig name=${sample_id} description=${sample_id} visibility=full autoScale=off maxHeightPixels=128:30:1 viewLimits=1:120 color=65,105,225\" &gt;&gt; $track_output\n}\n\n# iterate through samples\n# at the sample level only DEDUP matters\nfor sample_id in ${sample_list[@]}; do\n    for dedup_id in ${dedup_list[@]}; do\n        run_sample_tracks $sample_id $dedup_id\n    done\ndone",
    "crumbs": [
      "Home",
      "UCSC",
      "Generating Track information"
    ]
  },
  {
    "objectID": "docs/UCSC/2_creating_track_info.html#single-samples",
    "href": "docs/UCSC/2_creating_track_info.html#single-samples",
    "title": "Generating Track information",
    "section": "",
    "text": "It’s recommended to create a text file of all sample track information to ease in editing and submission to the UCSC browser website. A single line of code is needed for each sample which will provide the track location, sample name, description of the sample, whether to autoscale the samples, max height of the samples, view limits, and color. An example is provided below.\ntrack type=bigWig bigDataUrl=https://hpc.nih.gov/~CCBR/ccbr1155/${dir_loc}/bigwig/${complete_sample_id}.bigwig name=${sample_id} description=${sample_id} visibility=full autoScale=off maxHeightPixels=128:30:1 viewLimits=1:120 color=65,105,225\nUsers may find it helpful to create a single script which would create this text file for all samples. An example of this is listed below, which assumes that input files were generated using the CARLISLE pipeline. It can be edited to adapt to other output files, as needed.\nGenerally, each “track” line should have at least the following key value pairs: - name : label for the track - description : defines the center lable displayed - type : BAM, BED, bigBed, bigWig, etc. - bigDataUrl : URL of the data file - for other options see here",
    "crumbs": [
      "Home",
      "UCSC",
      "Generating Track information"
    ]
  },
  {
    "objectID": "docs/UCSC/2_creating_track_info.html#inputs",
    "href": "docs/UCSC/2_creating_track_info.html#inputs",
    "title": "Generating Track information",
    "section": "",
    "text": "samples_list.txt: a single column text file with sampleID’s\ntrack_dir: path to the linked files\ntrack_output: path to output file\npeak_list: all peak types to be included\nmethod_list: what method to be included\ndedupe_list: type of duplication to be included\n\n# input arguments\nsample_list_input=/\"path/to/samples.txt\"\ntrack_dir=\"/path/to/shared/dir/\"\ntrack_output=\"/path/to/output/file/tracks.txt\npeak_list=(\"norm.relaxed.bed\" \"narrowPeak\" \"broadGo_peaks.bed\" \"narrowGo_peaks.bed\")\nmethod_list=(\"fragmentsbased\")\ndedup_list=(\"dedup\")\n\n# read sample file\nIFS=$'\\n' read -d '' -r -a sample_list &lt; $sample_list_input\n\nrun_sample_tracks (){\n    sample_id=$1\n    dedup_id=$2\n    \n    # sample name\n    # eg siNC_H3K27Ac_1.dedup.bigwig\n    complete_sample_id=\"${sample_id}.${dedup_id}\"\n\n    # set link location\n    link_loc=\"${track_dir}/bigwig/${complete_sample_id}.bigwig\"\n\n    # echo track info\n    echo \"track type=bigWig bigDataUrl=https://hpc.nih.gov/~CCBR/ccbr1155/${dir_loc}/bigwig/${complete_sample_id}.bigwig name=${sample_id} description=${sample_id} visibility=full autoScale=off maxHeightPixels=128:30:1 viewLimits=1:120 color=65,105,225\" &gt;&gt; $track_output\n}\n\n# iterate through samples\n# at the sample level only DEDUP matters\nfor sample_id in ${sample_list[@]}; do\n    for dedup_id in ${dedup_list[@]}; do\n        run_sample_tracks $sample_id $dedup_id\n    done\ndone",
    "crumbs": [
      "Home",
      "UCSC",
      "Generating Track information"
    ]
  },
  {
    "objectID": "docs/UCSC/2_creating_track_info.html#inputs-1",
    "href": "docs/UCSC/2_creating_track_info.html#inputs-1",
    "title": "Generating Track information",
    "section": "Inputs",
    "text": "Inputs\n\nsamples_list.txt: a single column text file with sampleID’s\ntrack_dir: path to the linked files\ntrack_output: path to output file\npeak_list: all peak types to be included\nmethod_list: what method to be included\ndedupe_list: type of duplication to be included\n\n# input arguments\nsample_list_input=/\"path/to/samples.txt\"\ntrack_dir=\"/path/to/shared/dir/\"\ntrack_output=\"/path/to/output/file/tracks.txt\npeak_list=(\"norm.relaxed.bed\" \"narrowPeak\" \"broadGo_peaks.bed\" \"narrowGo_peaks.bed\")\nmethod_list=(\"fragmentsbased\")\ndedup_list=(\"dedup\")\n\n# read sample file\nIFS=$'\\n' read -d '' -r -a deg_list &lt; $deg_list_input\n\nrun_comparison_tracks (){\n    peak_type=$1\n    method_type=$2\n    dedup_type=$3\n    sample_id=$4\n    \n    # sample name\n    # eg siSmyd3_2m_Smyd3_0.25HCHO_500K_vs_siNC_2m_Smyd3_0.25HCHO_500K__no_dedup__norm.relaxed\n    complete_sample_id=\"${sample_id}__${dedup_type}__${peak_type}\"\n    \n    # set link location\n    link_loc=\"${track_dir}/bigbed/${complete_sample_id}_${method_type}_diffresults.bigbed\"\n\n    # echo track info\n    echo \"track name=${sample_id}_${peak_type} bigDataUrl=https://hpc.nih.gov/~CCBR/ccbr1155/${dir_loc}/bigbed/${complete_sample_id}_fragmentsbased_diffresults.bigbed type=bigBed itemRgb=On\" &gt;&gt; $track_info\n}\n\n# iterate through samples / peaks / methods / dedup\nfor sample_id in ${deg_list[@]}; do\n    for peak_id in ${peak_list[@]}; do\n        for method_id in ${method_list[@]}; do\n            for dedup_id in ${dedup_list[@]}; do\n                run_comparison_tracks $peak_id $method_id $dedup_id $sample_id\n            done\n        done\n    done\ndone",
    "crumbs": [
      "Home",
      "UCSC",
      "Generating Track information"
    ]
  },
  {
    "objectID": "docs/datashare.html",
    "href": "docs/datashare.html",
    "title": "Datashare",
    "section": "",
    "text": "Host data on Helix or Biowulf, that is publicly accessible through a URL",
    "crumbs": [
      "Home",
      "Datashare"
    ]
  },
  {
    "objectID": "docs/datashare.html#setup",
    "href": "docs/datashare.html#setup",
    "title": "Datashare",
    "section": "Setup",
    "text": "Setup\n\nAccess Login\n# Helix\nssh -Y username@helix.nih.gov\n\n# Biowulf\nssh -Y username@biowulf.nih.gov\nCreate a new dir (tutorial) in the datashare path:\ncd /data/CCBR/datashare/\nmkdir tutorial",
    "crumbs": [
      "Home",
      "Datashare"
    ]
  },
  {
    "objectID": "docs/datashare.html#processing",
    "href": "docs/datashare.html#processing",
    "title": "Datashare",
    "section": "Processing",
    "text": "Processing\n\nMake a directory in the datashare folder\nNOTE: For all steps below, an example is shown for Helix, but the same process is applicable for Biowulf, after changing the helix.nih.gov to biowulf.nih.gov\nNow you can transfer your data to the new directory. One method is to use scp to copy data from your local machine to Helix.\nHere is an example of using scp to copy the file file.txt from a local directory to Helix.\nscp /data/$USER/file.txt username@helix.nih.gov:/data/CCBR/datashare/tutorial/\nTo copy multiple directories recursively, you can also include the -r command with scp and from the top level directory:\nscp -r /data/$USER/ username@helix.nih.gov:/data/CCBR/datashare/tutorial/\n\n\nCreate public permissions for data\nWhen the data has been successully copied, we need to open the permissions.\nNOTE: This will give open access to anyone with the link. Ensure this is appropriate for the data type\n# cd to the shared dir\ncd /data/CCBR/datashare/\n\n# run CHMOD, twice\nchmod -R 777 tutorial\nchmod -R 777 tutorial/*\n\n# run SETFACL\nsetfacl -m u:webcpu:r-x tutorial/*",
    "crumbs": [
      "Home",
      "Datashare"
    ]
  },
  {
    "objectID": "docs/datashare.html#public-access",
    "href": "docs/datashare.html#public-access",
    "title": "Datashare",
    "section": "Public Access",
    "text": "Public Access\nNOTE: You must be logged into HPC in order to access these files from a web browser.\nFiles will be available for access through a browser, via tools like wget and UCSC genome track browser via the following format:\nhttp://hpc.nih.gov/~CCBR/tutorial/file.txt\nFor more information and a tutorial for creating UCSC tracks, visit the CCBR HowTo Page.",
    "crumbs": [
      "Home",
      "Datashare"
    ]
  },
  {
    "objectID": "docs/HPCDME/transfer.html",
    "href": "docs/HPCDME/transfer.html",
    "title": "File Transfers from Biowulf",
    "section": "",
    "text": "🛑 STOP: dm_register_dataobject_multipart does not work via SLURM. Hence, file transfers are NOT working when initiated from BIOWULF. We recommend transferring files from Helix.",
    "crumbs": [
      "Home",
      "HPCDME",
      "File Transfers from Biowulf"
    ]
  },
  {
    "objectID": "docs/HPCDME/transfer.html#background",
    "href": "docs/HPCDME/transfer.html#background",
    "title": "File Transfers from Biowulf",
    "section": "Background",
    "text": "Background\nRawdata or Project folders from Biowulf can be parked at a secure location after the analysis has reached an endpoint. Traditionally, CCBR analysts have been using GridFTP Globus Archive for doing this. But, this Globus Archive has been running relatively full lately and it is hard to estimate how much space is left there as the volume is shared among multiple groups.",
    "crumbs": [
      "Home",
      "HPCDME",
      "File Transfers from Biowulf"
    ]
  },
  {
    "objectID": "docs/HPCDME/transfer.html#parkit",
    "href": "docs/HPCDME/transfer.html#parkit",
    "title": "File Transfers from Biowulf",
    "section": "parkit",
    "text": "parkit\nparkit is designed to assist analysts in archiving project data from the NIH’s Biowulf/Helix systems to the HPC-DME storage platform. It provides functionalities to package and store data such as raw FastQ files or processed data from bioinformatics pipelines. Users can automatically:\n\ncreate tarballs of their data (including .filelist and .md5sum files),\ngenerate metadata,\ncreate collections on HPC-DME, and\ndeposit tar files into the system for long-term storage.\n\nparkit also features comprehensive workflows that support both folder-based and tarball-based archiving. These workflows are integrated with the SLURM job scheduler, enabling efficient execution of archival tasks on the Biowulf HPC cluster. This integration ensures that bioinformatics project data is securely archived and well-organized, allowing for seamless long-term storage.\n\n❗ NOTE: HPC DME API CLUs should already be setup as per these instructions in order to use parkit\n\n\n❗ NOTE: HPC_DM_UTILS environment variable should be set to point to the utils folder under the HPC_DME_APIs repo setup. Please see these instructions.\n\nprojark is the preferred parkit command to completely archive an entire folder as a tarball on HPCDME using SLURM.\n\nprojark usage\n\nload conda env\n# source conda\n. \"/data/CCBR_Pipeliner/db/PipeDB/Conda/etc/profile.d/conda.sh\"\n# activate parkit or parkit_dev environment\nconda activate parkit\n# check version of parkit\nparkit --version\nprojark --version\n\n\nExpected sample output\n\nv2.0.2-dev\nprojark is using the following parkit version:\nv2.0.2-dev\n\n\n\nprojark help\nprojark --help\n\n\nExpected sample output\n\nusage: projark [-h] --folder FOLDER --projectnumber PROJECTNUMBER\n               [--executor EXECUTOR] [--rawdata] [--cleanup]\n\nWrapper for folder2hpcdme for quick CCBR project archiving!\n\noptions:\n  -h, --help            show this help message and exit\n  --folder FOLDER       Input folder path to archive\n  --projectnumber PROJECTNUMBER\n                        CCBR project number.. destination will be\n                        /CCBR_Archive/GRIDFTP/Project_CCBR-&lt;projectnumber&gt;\n  --executor EXECUTOR   slurm or local\n  --rawdata             If tarball is rawdata and needs to go under folder\n                        Rawdata\n  --cleanup             post transfer step to delete local files\n\n\n\n\nprojark testing\n\nget dummy data\n# make a tmp folder\nmkdir -p /data/$USER/parkit_tmp\n# copy dummy project folder into the tmp folder\ncp -r /data/CCBR/projects/CCBR-12345 /data/$USER/parkit_tmp/CCBR-12345-$USER\n# check if HPC_DM_UTILS has been set\necho $HPC_DM_UTILS\n\n\nrun projark\nprojark --folder /data/$USER/parkit_tmp/CCBR-12345-$USER --projectnumber 12345-$USER --executor local\n\n\nExpected sample output\n\nSOURCE_CONDA_CMD is set to: . \"/data/CCBR_Pipeliner/db/PipeDB/Conda/etc/profile.d/conda.sh\"\nHPC_DM_UTILS is set to: /data/kopardevn/GitRepos/HPC_DME_APIs/utils\nparkit_folder2hpcdme --folder \"/data/$USER/parkit_tmp/CCBR-12345-$USER\" --dest \"/CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\" --projecttitle \"CCBR-12345-kopardevn\" --projectdesc \"CCBR-12345-kopardevn\" --executor \"local\" --hpcdmutilspath /data/kopardevn/GitRepos/HPC_DME_APIs/utils --makereadme\n################ Running createtar #############################\nparkit createtar --folder \"/data/$USER/parkit_tmp/CCBR-12345-$USER\"\ntar cvf /data/$USER/parkit_tmp/CCBR-12345-$USER.tar /data/$USER/parkit_tmp/CCBR-12345-$USER &gt; /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist\ncreatemetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar file was created!\ncreatemetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist file was created!\ncreatemetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.md5 file was created!\ncreatemetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist.md5 file was created!\n################################################################\n############ Running createemptycollection ######################\nparkit createemptycollection --dest \"/CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\" --projectdesc \"CCBR-12345-kopardevn\" --projecttitle \"CCBR-12345-kopardevn\"\nmodule load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_collection /dev/shm/a213dedc-9363-44ec-8a7a-d29f2345a0b5.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\ncat /dev/shm/a213dedc-9363-44ec-8a7a-d29f2345a0b5.json && rm -f /dev/shm/a213dedc-9363-44ec-8a7a-d29f2345a0b5.json\nmodule load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_collection /dev/shm/cabf7826-81b5-4b6a-addd-09fbcf279591.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn/Analysis\nmodule load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_collection /dev/shm/cabf7826-81b5-4b6a-addd-09fbcf279591.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn/Rawdata\ncat /dev/shm/cabf7826-81b5-4b6a-addd-09fbcf279591.json && rm -f /dev/shm/cabf7826-81b5-4b6a-addd-09fbcf279591.json\n################################################################\n########### Running createmetadata ##############################\nparkit createmetadata --tarball \"/data/$USER/parkit_tmp/CCBR-12345-$USER.tar\" --dest \"/CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\"\ncreatemetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.metadata.json file was created!\ncreatemetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist.metadata.json file was created!\n################################################################\n############# Running deposittar ###############################\nparkit deposittar --tarball \"/data/$USER/parkit_tmp/CCBR-12345-$USER.tar\" --dest \"/CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\"\nmodule load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_dataobject /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist.metadata.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn/Analysis/CCBR-12345.tar.filelist /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist\nmodule load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_dataobject_multipart /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.metadata.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn/Analysis/CCBR-12345.tar /data/$USER/parkit_tmp/CCBR-12345-$USER.tar\n################################################################\n\n\n❗ NOTE: remove --executor local from the command when running on real data (not test data) to submit jobs through SLURM\n\n\n❗ NOTE: add --rawdata when folder contains raw fastqs\n\n\n\nverify transfer\nTransfer can be verified by logging into HPC DME web interface.\n\n\n\nalt text\n\n\n\n\ncleanup\nDelete unwanted collection from HPC DME.\n# load java\nmodule load java\n# load dm_ commands\nsource $HPC_DM_UTILS/functions\n# delete collection recursively\ndm_delete_collection -r /CCBR_Archive/GRIDFTP/Project_CCBR-12345-$USER\n\n\nExpected sample output\n\nReading properties from /data/kopardevn/GitRepos/HPC_DME_APIs/utils/hpcdme.properties\nWARNING: You have requested recursive delete of the collection. This will delete all files and sub-collections within it recursively. Are you sure you want to proceed? (Y/N):\nY\nWould you like to see the list of files to delete ?\nN\nThe collection /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn and all files and sub-collections within it will be recursively deleted. Proceed with deletion ? (Y/N):\nY\nExecuting: https://hpcdmeapi.nci.nih.gov:8080/collection\nWrote results into /data/kopardevn/HPCDMELOG/tmp/getCollections_Records20241010.txt\nCmd process Completed\nOct 10, 2024 4:43:09 PM org.springframework.shell.core.AbstractShell handleExecutionResult\nINFO: CLI_SUCCESS\n\n\n⚠️ Reach out to Vishal Koparde in case you run into issues.",
    "crumbs": [
      "Home",
      "HPCDME",
      "File Transfers from Biowulf"
    ]
  },
  {
    "objectID": "docs/snakemake.html",
    "href": "docs/snakemake.html",
    "title": "Snakemake",
    "section": "",
    "text": "Step-by-step guide for setting up and learning to use Snakemake, with examples and use cases\n\nhttps://CCBR.github.io/snakemake_tutorial/",
    "crumbs": [
      "Home",
      "Snakemake"
    ]
  },
  {
    "objectID": "contributors.html",
    "href": "contributors.html",
    "title": "Contributors",
    "section": "",
    "text": "Samantha\n\n\n\n\n\n\n\nVishal Koparde, PhD\n\n\n\n\n\n\n\nKelly Sovacool, PhD\n\n\n\n\n\n\n\nNed Cauley\n\n\n\n\n\nView the contributors graph on GitHub for more details."
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "MIT License",
    "section": "",
    "text": "MIT License\nCopyright (c) 2025 HowTos authors\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "contributions.html",
    "href": "contributions.html",
    "title": "CCBR How-Tos",
    "section": "",
    "text": "The following individuals were the main contributors to these HowTo manuals:\n\nGitHub: Samantha Sevilla & Vishal Koparde\nTechDev: Samantha Sevilla\nHPCDME: Vishal Koparde\nUSCS Tracks: Samantha Sevilla\nRpackage: Ned Cauley\nOther How To’s: Ned Cauley\nTutorials: Samantha Sevilla"
  },
  {
    "objectID": "docs/HPCDME/transferFromHelix.html",
    "href": "docs/HPCDME/transferFromHelix.html",
    "title": "File Transfer from HELIX",
    "section": "",
    "text": "Rawdata or Project folders from Helix can be parked at a secure location after the analysis has reached an endpoint. Traditionally, CCBR analysts have been using GridFTP Globus Archive for doing this. But, this Globus Archive has been running relatively full lately and it is hard to estimate how much space is left there as the volume is shared among multiple groups.",
    "crumbs": [
      "Home",
      "HPCDME",
      "File Transfer from HELIX"
    ]
  },
  {
    "objectID": "docs/HPCDME/transferFromHelix.html#background",
    "href": "docs/HPCDME/transferFromHelix.html#background",
    "title": "File Transfer from HELIX",
    "section": "",
    "text": "Rawdata or Project folders from Helix can be parked at a secure location after the analysis has reached an endpoint. Traditionally, CCBR analysts have been using GridFTP Globus Archive for doing this. But, this Globus Archive has been running relatively full lately and it is hard to estimate how much space is left there as the volume is shared among multiple groups.",
    "crumbs": [
      "Home",
      "HPCDME",
      "File Transfer from HELIX"
    ]
  },
  {
    "objectID": "docs/HPCDME/transferFromHelix.html#parkit",
    "href": "docs/HPCDME/transferFromHelix.html#parkit",
    "title": "File Transfer from HELIX",
    "section": "parkit",
    "text": "parkit\nparkit is designed to assist analysts in archiving project data from the NIH’s Helix/Helix systems to the HPC-DME storage platform. It provides functionalities to package and store data such as raw FastQ files or processed data from bioinformatics pipelines. Users can automatically: - create tarballs of their data (including .filelist and .md5sum files), - generate metadata, - create collections on HPC-DME, and - deposit tar files into the system for long-term storage. parkit also features comprehensive workflows that support both folder-based and tarball-based archiving. This integration ensures that bioinformatics project data is securely archived and well-organized, allowing for seamless long-term storage.\n\n❗ NOTE: HPC DME API CLUs should already be setup as per these instructions in order to use parkit\n\n\n❗ NOTE: HPC_DM_UTILS environment variable should be set to point to the utils folder under the HPC_DME_APIs repo setup. Please see these instructions.\n\n\n❗ NOTE: If it has been a few months since you last used HPC_DME_APIs or parkit or projark, then please run the following commands before you start using parkit or projark:\ncd $HPC_DM_UTILS\ngit pull\nsource $HPC_DM_UTILS/functions\ndm_generate_token\n\nprojark is the preferred parkit command to completely archive an entire folder as a tarball on HPCDME. SLURM is not available on Helix and it could take a few hours to upload large files. Hence, it is recommended to use “tmux” or “screen” command for projark to continue running even when you log out of Helix.\n\n❗ NOTE: Run the following commands inside a screen or tmux session.\n\n\nprojark usage\n\nload conda env\n# source conda\n. \"/data/CCBR_Pipeliner/db/PipeDB/Conda/etc/profile.d/conda.sh\"\n# activate parkit or parkit_dev environment\nconda activate parkit\n# check version of parkit\nparkit --version\nprojark --version\n\n\nExpected sample output\n\nv2.1.2\nprojark is using the following parkit version:\nv2.1.2\n\n\n\nprojark help\nprojark --help\n\n\nExpected sample output\n\nusage: projark [-h] --folder FOLDER --projectnumber PROJECTNUMBER\n               [--executor EXECUTOR] [--rawdata] [--cleanup]\n\nWrapper for folder2hpcdme for quick CCBR project archiving!\n\noptions:\n  -h, --help            show this help message and exit\n  --folder FOLDER       Input folder path to archive\n  --projectnumber PROJECTNUMBER\n                        CCBR project number.. destination will be\n                        /CCBR_Archive/GRIDFTP/Project_CCBR-&lt;projectnumber&gt;\n  --executor EXECUTOR   slurm or local\n  --rawdata             If tarball is rawdata and needs to go under folder\n                        Rawdata\n  --cleanup             post transfer step to delete local files\n\n\n\n\nprojark testing\n\nget dummy data\n# make a tmp folder\nmkdir -p /data/$USER/parkit_tmp\n# copy dummy project folder into the tmp folder\ncp -r /data/CCBR/projects/CCBR-12345 /data/$USER/parkit_tmp/CCBR-12345-$USER\n# check if HPC_DM_UTILS has been set\necho $HPC_DM_UTILS\n\n\nrun projark\nprojark --folder /data/$USER/parkit_tmp/CCBR-12345-$USER --projectnumber 12345-$USER --executor local\n\n❗ NOTE: --executor slurm will not work on Helix\n\n\n\nExpected sample output\n\nSOURCE_CONDA_CMD is set to: . \"/data/CCBR_Pipeliner/db/PipeDB/Conda/etc/profile.d/conda.sh\"\nHPC_DM_UTILS is set to: /data/kopardevn/GitRepos/HPC_DME_APIs/utils\nparkit_folder2hpcdme --folder \"/data/$USER/parkit_tmp/CCBR-12345-$USER\" --dest \"/CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\" --projecttitle \"CCBR-12345-kopardevn\" --projectdesc \"CCBR-12345-kopardevn\" --executor \"local\" --hpcdmutilspath /data/kopardevn/GitRepos/HPC_DME_APIs/utils --makereadme\n################ Running createtar #############################\nparkit createtar --folder \"/data/$USER/parkit_tmp/CCBR-12345-$USER\"\ntar cvf /data/$USER/parkit_tmp/CCBR-12345-$USER.tar /data/$USER/parkit_tmp/CCBR-12345-$USER &gt; /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist\ncreatemetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar file was created!\ncreatemetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist file was created!\ncreatemetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.md5 file was created!\ncreatemetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist.md5 file was created!\n################################################################\n############ Running createemptycollection ######################\nparkit createemptycollection --dest \"/CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\" --projectdesc \"CCBR-12345-kopardevn\" --projecttitle \"CCBR-12345-kopardevn\"\nmodule load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_collection /dev/shm/a213dedc-9363-44ec-8a7a-d29f2345a0b5.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\ncat /dev/shm/a213dedc-9363-44ec-8a7a-d29f2345a0b5.json && rm -f /dev/shm/a213dedc-9363-44ec-8a7a-d29f2345a0b5.json\nmodule load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_collection /dev/shm/cabf7826-81b5-4b6a-addd-09fbcf279591.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn/Analysis\nmodule load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_collection /dev/shm/cabf7826-81b5-4b6a-addd-09fbcf279591.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn/Rawdata\ncat /dev/shm/cabf7826-81b5-4b6a-addd-09fbcf279591.json && rm -f /dev/shm/cabf7826-81b5-4b6a-addd-09fbcf279591.json\n################################################################\n########### Running createmetadata ##############################\nparkit createmetadata --tarball \"/data/$USER/parkit_tmp/CCBR-12345-$USER.tar\" --dest \"/CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\"\ncreatemetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.metadata.json file was created!\ncreatemetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist.metadata.json file was created!\n################################################################\n############# Running deposittar ###############################\nparkit deposittar --tarball \"/data/$USER/parkit_tmp/CCBR-12345-$USER.tar\" --dest \"/CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\"\nmodule load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_dataobject /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist.metadata.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn/Analysis/CCBR-12345.tar.filelist /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist\nmodule load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_dataobject_multipart /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.metadata.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn/Analysis/CCBR-12345.tar /data/$USER/parkit_tmp/CCBR-12345-$USER.tar\n################################################################\n\n\n❗ NOTE: add --rawdata when folder contains raw fastqs\n\n\n\nverify transfer\nTransfer can be verified by logging into HPC DME web interface.\n\n\n\nalt text\n\n\n\n\ncleanup\nDelete unwanted collection from HPC DME.\n# load java\nmodule load java\n# load dm_ commands\nsource $HPC_DM_UTILS/functions\n# delete collection recursively\ndm_delete_collection -r /CCBR_Archive/GRIDFTP/Project_CCBR-12345-$USER\n\n\nExpected sample output\n\nReading properties from /data/kopardevn/GitRepos/HPC_DME_APIs/utils/hpcdme.properties\nWARNING: You have requested recursive delete of the collection. This will delete all files and sub-collections within it recursively. Are you sure you want to proceed? (Y/N):\nY\nWould you like to see the list of files to delete ?\nN\nThe collection /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn and all files and sub-collections within it will be recursively deleted. Proceed with deletion ? (Y/N):\nY\nExecuting: https://hpcdmeapi.nci.nih.gov:8080/collection\nWrote results into /data/kopardevn/HPCDMELOG/tmp/getCollections_Records20241010.txt\nCmd process Completed\nOct 10, 2024 4:43:09 PM org.springframework.shell.core.AbstractShell handleExecutionResult\nINFO: CLI_SUCCESS\n\n\n⚠️ Reach out to Vishal Koparde in case you run into issues.",
    "crumbs": [
      "Home",
      "HPCDME",
      "File Transfer from HELIX"
    ]
  },
  {
    "objectID": "docs/HPCDME/setup.html",
    "href": "docs/HPCDME/setup.html",
    "title": "HPCDME Setup",
    "section": "",
    "text": "Background\nHPC_DME_APIs provides command line utilities or CLUs to interface with HPCDME. This document describes some of the initial setup steps to get the CLUs working on Biowulf.\n\n\nSetup steps:\n\nClone repo:\nThe repo can be cloned at a location accessible to you:\ncd /data/$USER/\ngit clone https://github.com/CBIIT/HPC_DME_APIs.git\n\n\nCreate dirs, log files needed for HPCMDE\nmkdir -p /data/$USER/HPCDMELOG/tmp\ntouch /data/$USER/HPCDMELOG/tmp/hpc-cli.log\n\n\nCopy properties template\nhpcdme.properties is the file that all CLUs look into for various parameters like authentication password, file size limits, number of CPUs, etc. Make a copy of the template provided and prepare it for customization.\ncd /data/$USER/HPC_DME_APIs/utils\ncp hpcdme.properties-sample hpcdme.properties\n\n\nCustomize properties file\nSome of the parameters in this file have become obsolete over the course of time and are commmented out. Change paths and default values, as needed.\nNOTES\n\nReplace $USER with your actual username in the properties file. Bash variables will not be interpolated.\nLeave hpc.ssl.keystore.password=changeit as-is. changeit is not a variable but the actual password for our team.\nBe sure to set the proxy server URL as below (hpc.server.proxy.url=10.1.200.75) when running on biowulf/helix.\n\n#HPC DME Server URL\n#Production server settings\nhpc.server.url=https://hpcdmeapi.nci.nih.gov:8080\nhpc.ssl.keystore.path=hpc-client/keystore/keystore-prod.jks\n#hpc.ssl.keystore.password=hpcdmncif\nhpc.ssl.keystore.password=changeit\n\n#UAT server settings\n#hpc.server.url=https://fr-s-hpcdm-uat-p.ncifcrf.gov:7738/hpc-server\n#hpc.ssl.keystore.path=hpc-client/keystore/keystore-uat.jks\n#hpc.ssl.keystore.password=hpc-server-store-pwd\n\n#Proxy Settings\nhpc.server.proxy.url=10.1.200.75\nhpc.server.proxy.port=3128\n\nhpc.user=$USER\n\n#Globus settings\n#default globus endpoint to be used in registration and download\nhpc.globus.user=$USER\nhpc.default.globus.endpoint=ea6c8fd6-4810-11e8-8ee3-0a6d4e044368\n\n#Log files directory\nhpc.error-log.dir=/data/$USER/HPCDMELOG/tmp\n\n###HPC CLI Logging START####\n#ERROR, WARN, INFO, DEBUG\nhpc.log.level=ERROR\nhpc.log.file=/data/$USER/HPCDMELOG/tmp/hpc-cli.log\n###HPC CLI Logging END####\n\n#############################################################################\n# Please use caution changing following properties. They don't change usually\n#############################################################################\n#hpc.collection.service=collection\n#hpc.dataobject.service=dataObject\n#Log files directory\n#hpc.error-log.dir=.\n\n#Number of thread to run data file import from a CSV file\nhpc.job.thread.count=1\n\nupload.buffer.size=10000000\n\n#Retry count and backoff period for registerFromFilePath (Fixed backoff)\nhpc.retry.max.attempts=3\n#hpc.retry.backoff.period=5000\n\n#Multi-part upload thread pool, threshold and part size configuration\n#hpc.multipart.threadpoolsize=10\n#hpc.multipart.threshold=1074790400\n#hpc.multipart.chunksize=1073741824\n\n#globus.nexus.url=nexus.api.globusonline.org\n#globus.url=www.globusonline.org\n\n#HPC DME Login token file location\nhpc.login.token=tokens/hpcdme-auth.txt\n\n#Globus Login token file location\n#hpc.globus.login.token=tokens/globus-auth.txt\n#validate.md5.checksum=false\n\n# JAR version\n#hpc.jar.version=hpc-cli-1.4.0.jar\n\nNOTE: The current java version used is:\njava -version\nopenjdk version \"1.8.0_181\"\nOpenJDK Runtime Environment (build 1.8.0_181-b13)\nOpenJDK 64-Bit Server VM (build 25.181-b13, mixed mode)\n\n\n\nEdit ~/.bashrc\nAdd the CLUs to PATH by adding the following to ~/.bashrc file\n# export environment variable HPC_DM_UTILS pointing to directory where \n# HPC DME client utilities are, then source functions script in there \nexport HPC_DM_UTILS=/data/$USER/HPC_DME_APIs/utils\nsource $HPC_DM_UTILS/functions\nNext, source it\nsource ~/.bashrc\n\n\nGenerate token\nNow, you are all set to generate a token. This prevents from re-entering your password everytime.\ndm_generate_token\nIf the token generation takes longer than 45 seconds, check the connection:\nping hpcdmeapi.nci.nih.gov\nIf the connection responds, try to export the following proxy, and then re-run the dm_generate_tokens command:\nexport https_proxy=http://dtn01-e0:3128\nDone! You are now all set to use CLUs.\n\n\n\nReferences and Links\n\nHPC_DME_APIs repo\nUser guides\nWiki pages\nYuri Dinh",
    "crumbs": [
      "Home",
      "HPCDME",
      "HPCDME Setup"
    ]
  },
  {
    "objectID": "docs/UCSC/0_overview.html",
    "href": "docs/UCSC/0_overview.html",
    "title": "Overview",
    "section": "",
    "text": "Overview\nThe UCSC Genome Browser allows for visualization of genomic data in an interactive and shareable format. User’s must create accounts with their NIH credentials, and have an active Biowulf account to create the tracks. In addition users have to be connect to VPN in order to view and create the tracks. Once bigwig files are generated and stored in a shared data location, genomic tracks can be edited, and permanent links created, accessible for collaborators to view.",
    "crumbs": [
      "Home",
      "UCSC",
      "Overview"
    ]
  },
  {
    "objectID": "docs/UCSC/3_creating_tracks.html",
    "href": "docs/UCSC/3_creating_tracks.html",
    "title": "Generating New Tracks",
    "section": "",
    "text": "Generating New Tracks\nBiowulf/Helix hosts its own instance of the UCSC Genome Browser which is behind the NIH firewall.\n\nLogin to VPN\nLogin to the UCSC Browser website\nSelect “My Data &gt; Custom Tracks”\nSelect “Add Custom Tracks”\nPaste the track data generated in Creating Track Info into the text box\nSelect “Submit”\nReview the track information. The column “Error” should be empty. If there is an error then a hyperlink will display “Show” although this often does not contain helpful error information.\nAfter troubleshooting any errors select “Go”\nUse the features at the bottom of the page to alter views and/or add additional track information\nSelect “My Data &gt; My Sessions”\nUnder “Save current settings as named session:” enter a descriptive name of the session\nSelect “Submit”\nThis will move the descriptive name entered into the “session name” list\nSelect the descriptve name, view your track information as saved\nCopy the hyperlink for this session and share as needed\n\n\n\nEditing a Previous Tracks\n\nLogin to VPN\nLogin to the UCSC Browser website\nSelect “My Data &gt; My Sessions”\nSelect the descriptve name of the session you’d like to edit\nEdit the tracks as needed:\n\nIf wanting to remove or add tracks, then select “My Data &gt; Custom Tracks”; follow steps 5-8 above.\nIf wanting to edit the view of the tracks, follow step 9 above.\n\nSelect “My Data &gt; My Sessions”\nUnder “Save current settings as named session:” enter a new descriptive name OR the previous name of the session if you’d like to overwrite it\nSelect “Submit”\nThis will move the descriptive name entered into the “session name” list\nSelect the descriptve name, view your track information as saved\nCopy the hyperlink for this session and share as needed\n\n\nTIP: Unindexed file formats like bed and gtf take significantly longer to load in the genome Browser and it is recommended to convert them to indexed formats like bigBed and bigWig prior to adding them to your session.",
    "crumbs": [
      "Home",
      "UCSC",
      "Generating New Tracks"
    ]
  },
  {
    "objectID": "docs/R-package/create-from-conda/1_build_pkg.html",
    "href": "docs/R-package/create-from-conda/1_build_pkg.html",
    "title": "Creating the package",
    "section": "",
    "text": "To create a package with Conda, you first need to make a new release and tag in the github repo of the R package you would like to create into a Conda Package.\nFor more information on best practices in R package creation, review this documentation.\nBefore creating the release on github, please check for proper dependencies listed in the files NAMESPACE and DESCRIPTION.\nThese files should have the same list of dependencies, and the version numbers for dependencies can be specified in the DESCRIPTION file. The DESCRIPTION file must be edited manually, while the NAMESPACE file should not be edited manually, but rather created automatically using the document() function.\nThe DESCRIPTION file must also be correctly formatted. For more information, see the following website.",
    "crumbs": [
      "Home",
      "R Package",
      "Create from Conda",
      "Creating the package"
    ]
  },
  {
    "objectID": "docs/R-package/create-from-conda/1_build_pkg.html#overview",
    "href": "docs/R-package/create-from-conda/1_build_pkg.html#overview",
    "title": "Creating the package",
    "section": "",
    "text": "To create a package with Conda, you first need to make a new release and tag in the github repo of the R package you would like to create into a Conda Package.\nFor more information on best practices in R package creation, review this documentation.\nBefore creating the release on github, please check for proper dependencies listed in the files NAMESPACE and DESCRIPTION.\nThese files should have the same list of dependencies, and the version numbers for dependencies can be specified in the DESCRIPTION file. The DESCRIPTION file must be edited manually, while the NAMESPACE file should not be edited manually, but rather created automatically using the document() function.\nThe DESCRIPTION file must also be correctly formatted. For more information, see the following website.",
    "crumbs": [
      "Home",
      "R Package",
      "Create from Conda",
      "Creating the package"
    ]
  },
  {
    "objectID": "docs/R-package/create-from-conda/1_build_pkg.html#download-the-r-package-release-from-github",
    "href": "docs/R-package/create-from-conda/1_build_pkg.html#download-the-r-package-release-from-github",
    "title": "Creating the package",
    "section": "Download the R package release from Github",
    "text": "Download the R package release from Github\nTo download the most recent release from the most recent tag on Github, activate Conda then use Conda skeleton to pull the correct URL. In the example below, replace $githubURL with the URL to your R package’s github repo.\nconda activate \n\nconda skeleton cran $githubURL\nA folder is then created for the downloaded release. Ror example running the following:\nconda skeleton cran https://github.com/NIDAP-Community/DSPWorkflow\nCreates the folder\nr-dspworkflow\nWithin this newly created folder is a file named meta.yaml. You will need to edit this file to include the channels and edit any information on the the package version number or dependency version numbers.\nHere is an example of the top of the meta.yaml file with the channels section added:\n{% set version = '0.9.5.2' %}\n\n{% set posix = 'm2-' if win else '' %}\n{% set native = 'm2w64-' if win else '' %}\n\npackage:\n  name: r-dspworkflow\n  version: {{ version|replace(\"-\", \"_\") }}\n  \nchannels:\n  - conda-forge\n  - bioconda\n  - default\n  - file://rstudio-files/RH/ccbr-projects/Conda_package_tutorial/local_channel/channel\n\nsource:\n\n  git_url: https://github.com/NIDAP-Community/DSPWorkflow\n  git_tag: 0.9.5\n\nbuild:\n  merge_build_host: True  # [win]\n  # If this is a new build for the same version, increment the build number.\n  number: 0\n  # no skip\n\n  # This is required to make R link correctly on Linux.\n  rpaths:\n    - lib/R/lib/\n    - lib/\n\n    # Suggests: testthat (== 3.1.4)\nrequirements:\n  build:\n    - {{ posix }}filesystem        # [win]\n    - {{ posix }}git\n    - {{ posix }}zip               # [win]\nHere is an example of the sections for specifying dependency versions from the meta.yaml file:\n  host:\n    - r-base =4.1.3=h2f963a2_5\n    - bioconductor-biobase =2.54.0=r41hc0cfd56_2\n    - bioconductor-biocgenerics =0.40.0=r41hdfd78af_0\n    - bioconductor-geomxtools =3.1.1=r41hdfd78af_0\n    - bioconductor-nanostringnctools =1.2.0\n    - bioconductor-spatialdecon =1.4.3\n    - bioconductor-complexheatmap =2.10.0=r41hdfd78af_0\n    - r-cowplot =1.1.1=r41hc72bb7e_1\n    - r-dplyr =1.0.9=r41h7525677_0\n    - r-ggforce =0.3.4=r41h7525677_0\n    - r-ggplot2 =3.3.6=r41hc72bb7e_1\n    - r-gridextra =2.3=r41hc72bb7e_1004\n    - r-gtable =0.3.0=r41hc72bb7e_3\n    - r-knitr =1.40=r41hc72bb7e_1\n    - r-patchwork =1.1.2=r41hc72bb7e_1\n    - r-reshape2 =1.4.4=r41h7525677_2\n    - r-scales =1.2.1=r41hc72bb7e_1\n    - r-tibble =3.1.8=r41h06615bd_1\n    - r-tidyr =1.2.1=r41h7525677_1\n    - r-umap =0.2.9.0=r41h7525677_1\n    - r-rtsne =0.16=r41h37cf8d7_1\n    - r-magrittr =2.0.3=r41h06615bd_1\n    - r-rlang =1.1.0=r41h38f115c_0\n    \n  run:\n    - r-base =4.1.3=h2f963a2_5\n    - bioconductor-biobase =2.54.0=r41hc0cfd56_2\n    - bioconductor-biocgenerics =0.40.0=r41hdfd78af_0\n    - bioconductor-geomxtools =3.1.1=r41hdfd78af_0\n    - bioconductor-nanostringnctools =1.2.0\n    - bioconductor-spatialdecon =1.4.3\n    - bioconductor-complexheatmap =2.10.0=r41hdfd78af_0\n    - r-cowplot =1.1.1=r41hc72bb7e_1\n    - r-dplyr =1.0.9=r41h7525677_0\n    - r-ggforce =0.3.4=r41h7525677_0\n    - r-ggplot2 =3.3.6=r41hc72bb7e_1\n    - r-gridextra =2.3=r41hc72bb7e_1004\n    - r-gtable =0.3.0=r41hc72bb7e_3\n    - r-knitr =1.40=r41hc72bb7e_1\n    - r-patchwork =1.1.2=r41hc72bb7e_1\n    - r-reshape2 =1.4.4=r41h7525677_2\n    - r-scales =1.2.1=r41hc72bb7e_1\n    - r-tibble =3.1.8=r41h06615bd_1\n    - r-tidyr =1.2.1=r41h7525677_1\n    - r-umap =0.2.9.0=r41h7525677_1\n    - r-rtsne =0.16=r41h37cf8d7_1\n    - r-magrittr =2.0.3=r41h06615bd_1\n    - r-rlang =1.1.0=r41h38f115c_0\nIn the above example, each of the dependencies has been assigned a conda build string, so that when conda builds a conda package, it will only use that specific build of the dependency from the listed conda channels. The above example is very restrictive, the dependencies can also be listed in the “meta.yaml” file to be more open–it will choose a conda build string that fits in with the other resolved dependency build strings based on what is available in the channels.\nAlso note that the “host” section matches the “run” section.\nHere is some examples of a more open setup for these dependencies:\n  host:\n    - r-base &gt;=4.1.3\n    - bioconductor-biobase &gt;=2.54.0\n    - bioconductor-biocgenerics &gt;=0.40.0\n    - bioconductor-geomxtools &gt;=3.1.1\n    - bioconductor-nanostringnctools &gt;=1.2.0\n    - bioconductor-spatialdecon =1.4.3\n    - bioconductor-complexheatmap &gt;=2.10.0\n    - r-cowplot &gt;=1.1.1\n    - r-dplyr &gt;=1.0.9\n    - r-ggforce &gt;=0.3.4\n    - r-ggplot2 &gt;=3.3.6\n    - r-gridextra &gt;=2.3\n    - r-gtable &gt;=0.3.0\n    - r-knitr &gt;=1.40\n    - r-patchwork &gt;=1.1.2\n    - r-reshape2 &gt;=1.4.4\n    - r-scales &gt;=1.2.1\n    - r-tibble &gt;=3.1.8\n    - r-tidyr &gt;=1.2.1\n    - r-umap &gt;=0.2.9.0\n    - r-rtsne &gt;=0.16\n    - r-magrittr &gt;=2.0.3\n    - r-rlang &gt;=1.1.0",
    "crumbs": [
      "Home",
      "R Package",
      "Create from Conda",
      "Creating the package"
    ]
  },
  {
    "objectID": "docs/R-package/create-from-conda/1_build_pkg.html#build-the-conda-package",
    "href": "docs/R-package/create-from-conda/1_build_pkg.html#build-the-conda-package",
    "title": "Creating the package",
    "section": "Build the Conda package",
    "text": "Build the Conda package\nWhen the meta.yaml has been prepared, you can now build the Conda package. To do so, run the command, replacing\n\n$r-package with the name of the R package folder that was created after running conda skeleton (the folder where the meta.yaml is located).\n$build_log_name.log with the name for the log file, such as the date, time, and initials.\n\nconda-build $r-package 2&gt;&1|tee $build_log_name.log\nExample\nconda-build r-dspworkflow 2&gt;&1|tee 05_12_23_330_nc.log\n\nThe log file will list how conda has built the package, including what dependencies version numbers and corresponding build strings were used to resolve the conda environment. These dependencies are what we specified in the “meta.yaml” file. The log file will be useful troubleshooting a failed build.\nBe aware, the build can take anywhere from several minutes to an hour to complete, depending on the size of the package and the number of dependencies.\nThe conda package will be built as a tar.bz2 file.",
    "crumbs": [
      "Home",
      "R Package",
      "Create from Conda",
      "Creating the package"
    ]
  },
  {
    "objectID": "docs/R-package/create-from-conda/common_issues.html",
    "href": "docs/R-package/create-from-conda/common_issues.html",
    "title": "common issues",
    "section": "",
    "text": "An important consideration for Conda builds is the list of dependencies, specified versions, and compatibility with each of the other dependencies.\nIf the meta.yaml and DESCRIPTION file specify specific package versions, Conda’s ability to resolve the Conda environment also becomes more limited.\nFor example, if the Conda package we are building has the following requirements:\nDependency A version == 1.0\nDependency B version &gt;= 2.5\nAnd the Dependencies located in our Conda channel have the following dependencies:\nDependency A version 1.0\n  - Dependency C version == 0.5\n\nDependency A version 1.2\n- Dependency C version &gt;= 0.7\n\nDependency B version 2.7\n  - Dependency C version &gt;= 0.7\nAs you can see, the Conda build will not be able to resolve the environment because Dependency A version 1.0 needs an old version of Dependency C, while Dependency B version 2.7 needs a newer version.\nIn this case, if we changed our package’s DESCRIPTION and meta.yaml file to be:\nDependency A version &gt;= 1.0\nDependency B version &gt;= 2.5\nThe conda build will be able to resolve. This is a simplified version of a what are more often complex dependency structures, but it is an important concept in conda package building that will inevitably arise as a package’s dependencies become more specific.\nTo check on the versions of packages that are available in a Conda channel, use the command:\nconda search $dependency\nReplace $dependency with the name of package you would like to investigate. There are more optional commands for this function which can be found here\nTo check the dependencies of packages that exist in your Conda cache, go to the folder specified for your conda cache (that we specified earlier). In case you need to find that path you can use:\nconda info\nHere there will be a folder for each of the packages that has been used in a conda build (including the dependencies). In each folder is another folder called “info” and a file called “index.json” that lists information, such as depends for the package.\nHere is an example:\ncat /rstudio-files/ccbr-data/users/Ned/conda-cache/r-ggplot2-3.3.6-r41hc72bb7e_1/info/index.json\n\n{\n \"arch\": null,\n \"build\": \"r41hc72bb7e_1\",\n \"build_number\": 1,\n \"depends\": [\n   \"r-base &gt;=4.1,&lt;4.2.0a0\",\n   \"r-digest\",\n   \"r-glue\",\n   \"r-gtable &gt;=0.1.1\",\n   \"r-isoband\",\n   \"r-mass\",\n   \"r-mgcv\",\n   \"r-rlang &gt;=0.3.0\",\n   \"r-scales &gt;=0.5.0\",\n   \"r-tibble\",\n   \"r-withr &gt;=2.0.0\"\n ],\n \"license\": \"GPL-2.0-only\",\n \"license_family\": \"GPL2\",\n \"name\": \"r-ggplot2\",\n \"noarch\": \"generic\",\n \"platform\": null,\n \"subdir\": \"noarch\",\n \"timestamp\": 1665515494942,\n \"version\": \"3.3.6\"\n}\nIf you would like to specify an exact package to use in a conda channel for your conda build, specify the build string in your “meta.yaml” file. In the above example for ggplot version 3.3.6, the build string is listed in the folder name for package as well as in the index.json file for “build:”r41hc72bb7e_1”.",
    "crumbs": [
      "Home",
      "R Package",
      "Create from Conda",
      "common issues"
    ]
  },
  {
    "objectID": "docs/R-package/create-from-conda/common_issues.html#package-dependency-issues",
    "href": "docs/R-package/create-from-conda/common_issues.html#package-dependency-issues",
    "title": "common issues",
    "section": "",
    "text": "An important consideration for Conda builds is the list of dependencies, specified versions, and compatibility with each of the other dependencies.\nIf the meta.yaml and DESCRIPTION file specify specific package versions, Conda’s ability to resolve the Conda environment also becomes more limited.\nFor example, if the Conda package we are building has the following requirements:\nDependency A version == 1.0\nDependency B version &gt;= 2.5\nAnd the Dependencies located in our Conda channel have the following dependencies:\nDependency A version 1.0\n  - Dependency C version == 0.5\n\nDependency A version 1.2\n- Dependency C version &gt;= 0.7\n\nDependency B version 2.7\n  - Dependency C version &gt;= 0.7\nAs you can see, the Conda build will not be able to resolve the environment because Dependency A version 1.0 needs an old version of Dependency C, while Dependency B version 2.7 needs a newer version.\nIn this case, if we changed our package’s DESCRIPTION and meta.yaml file to be:\nDependency A version &gt;= 1.0\nDependency B version &gt;= 2.5\nThe conda build will be able to resolve. This is a simplified version of a what are more often complex dependency structures, but it is an important concept in conda package building that will inevitably arise as a package’s dependencies become more specific.\nTo check on the versions of packages that are available in a Conda channel, use the command:\nconda search $dependency\nReplace $dependency with the name of package you would like to investigate. There are more optional commands for this function which can be found here\nTo check the dependencies of packages that exist in your Conda cache, go to the folder specified for your conda cache (that we specified earlier). In case you need to find that path you can use:\nconda info\nHere there will be a folder for each of the packages that has been used in a conda build (including the dependencies). In each folder is another folder called “info” and a file called “index.json” that lists information, such as depends for the package.\nHere is an example:\ncat /rstudio-files/ccbr-data/users/Ned/conda-cache/r-ggplot2-3.3.6-r41hc72bb7e_1/info/index.json\n\n{\n \"arch\": null,\n \"build\": \"r41hc72bb7e_1\",\n \"build_number\": 1,\n \"depends\": [\n   \"r-base &gt;=4.1,&lt;4.2.0a0\",\n   \"r-digest\",\n   \"r-glue\",\n   \"r-gtable &gt;=0.1.1\",\n   \"r-isoband\",\n   \"r-mass\",\n   \"r-mgcv\",\n   \"r-rlang &gt;=0.3.0\",\n   \"r-scales &gt;=0.5.0\",\n   \"r-tibble\",\n   \"r-withr &gt;=2.0.0\"\n ],\n \"license\": \"GPL-2.0-only\",\n \"license_family\": \"GPL2\",\n \"name\": \"r-ggplot2\",\n \"noarch\": \"generic\",\n \"platform\": null,\n \"subdir\": \"noarch\",\n \"timestamp\": 1665515494942,\n \"version\": \"3.3.6\"\n}\nIf you would like to specify an exact package to use in a conda channel for your conda build, specify the build string in your “meta.yaml” file. In the above example for ggplot version 3.3.6, the build string is listed in the folder name for package as well as in the index.json file for “build:”r41hc72bb7e_1”.",
    "crumbs": [
      "Home",
      "R Package",
      "Create from Conda",
      "common issues"
    ]
  },
  {
    "objectID": "docs/containers/build-docker-google-cloud.html",
    "href": "docs/containers/build-docker-google-cloud.html",
    "title": "Build docker containers interactively with Google Cloud Shell",
    "section": "",
    "text": "As of this writing, Docker Desktop is not approved for use on NCI laptops. If you want to build and push docker containers using the docker CLI, you can use the Google Cloud Shell: https://console.cloud.google.com/\nYou can create an account with your @nih.gov email. Certain aspects of Google Cloud cost money, but the free tier offers more than enough usage for building containers with the Google Cloud Shell.",
    "crumbs": [
      "Home",
      "Containers",
      "Build docker containers interactively with Google Cloud Shell"
    ]
  },
  {
    "objectID": "docs/containers/build-docker-google-cloud.html#create-a-dockerfile",
    "href": "docs/containers/build-docker-google-cloud.html#create-a-dockerfile",
    "title": "Build docker containers interactively with Google Cloud Shell",
    "section": "Create a Dockerfile",
    "text": "Create a Dockerfile\nFor this tutorial, we’ll create a minimal Dockerfile with these contents:\nFROM python:3.11\n\nRUN mkdir -p /data2/\nRUN echo 'hello world' &gt; /data2/file.txt\n\nUpload\nIf you already have a Dockerfile written, you can upload it to Cloud Shell.\n\n\n\nIn the file explorer tab, right-click and select “Upload”.\n\n\n\n\nWrite\nCreate a new text file:\n\n\n\nClick File &gt; New Text File and write the contents of your Dockerfile.\n\n\nSave it with the name “Dockerfile”.",
    "crumbs": [
      "Home",
      "Containers",
      "Build docker containers interactively with Google Cloud Shell"
    ]
  },
  {
    "objectID": "docs/containers/build-docker-google-cloud.html#build-the-container",
    "href": "docs/containers/build-docker-google-cloud.html#build-the-container",
    "title": "Build docker containers interactively with Google Cloud Shell",
    "section": "Build the container",
    "text": "Build the container\ndocker build -t minimal:v1 -f ./Dockerfile .\n\n-t, --tag - Name and optional tag (format: “name:tag”)\n-f, --file - Name of the Dockerfile (default: “PATH/Dockerfile”)\n\nRun docker build --help for more options.\n\n\n\nThe build completed successfully.",
    "crumbs": [
      "Home",
      "Containers",
      "Build docker containers interactively with Google Cloud Shell"
    ]
  },
  {
    "objectID": "docs/containers/build-docker-google-cloud.html#test-the-container-image",
    "href": "docs/containers/build-docker-google-cloud.html#test-the-container-image",
    "title": "Build docker containers interactively with Google Cloud Shell",
    "section": "Test the container image",
    "text": "Test the container image\nRun the container interactively with the bash shell:\ndocker run -it minimal:v1 bash\nRun any commands you wish inside the container to test that everything you need was installed correctly.\ncat /data2/file.txt\nwhich python\npython --version\n\n\n\nThis container should have python installed and should have the “hello world” file.\n\n\nClose the container with the keyboard shortcut Control-DControl-D or type exit and press enter.\nIf the container doesn’t have the software you expected, something went wrong during the build process. Try editing the Dockerfile and re-building the container.",
    "crumbs": [
      "Home",
      "Containers",
      "Build docker containers interactively with Google Cloud Shell"
    ]
  },
  {
    "objectID": "docs/containers/build-docker-google-cloud.html#push-the-container-to-dockerhub",
    "href": "docs/containers/build-docker-google-cloud.html#push-the-container-to-dockerhub",
    "title": "Build docker containers interactively with Google Cloud Shell",
    "section": "Push the container to dockerhub",
    "text": "Push the container to dockerhub\nIf your container was built successfully, you can push it to a container registry such as dockerhub so you can pull it for use on other platforms.\n\nLog in to dockerhub\ndocker login\n\n\n\nOpen your web browser and type in the confirmation code. This step goes very smoothly if you’re already logged in to dockerhub in your web browser.\n\n\nIf it worked, you’ll see “Login Succeeded” in the Terminal.\n\n\nTag & push the container image\nAdd a tag to the image with your username or the namespace on dockerhub (e.g. nciccbr) that you want to push the container to.\nChange NAMESPACE in the following command to your username or namespace.\ndocker image tag minimal:v1 NAMESPACE/minimal:v1\nPush the container image to dockerhub:\ndocker push NAMESPACE/minimal:v1\n\n\n\nIn these examples, the namespace is kellysovacool (my username)\n\n\n\n\n\nIf the push succeeded, you’ll see it in your list of repositories on dockerhub\n\n\n\n\n\nOn the repository page, you can see the list of tags that have been pushed and optionally add a description, category, overview, etc.",
    "crumbs": [
      "Home",
      "Containers",
      "Build docker containers interactively with Google Cloud Shell"
    ]
  },
  {
    "objectID": "docs/containers/build-docker-google-cloud.html#pull-any-public-container",
    "href": "docs/containers/build-docker-google-cloud.html#pull-any-public-container",
    "title": "Build docker containers interactively with Google Cloud Shell",
    "section": "Pull any public container",
    "text": "Pull any public container\nYou can pull any public container from dockerhub (including the one you just pushed).\nHere is the base image that we use for building most other CCBR containers. You can see details about it on dockerhub:\n\n\n\n&lt;https://hub.docker.com/repository/docker/nciccbr/ccbr_ubuntu_22.04/general&gt;\n\n\n\n\n\nCCBR base image overview with list of installed tools\n\n\nHere’s an example of pulling this container from the CCBR dockerhub account:\ndocker pull nciccbr/ccbr_ubuntu_22.04:v4\nThe namespace is nciccbr, the container name is ccbr_ubuntu_22.04, and the version tag is v4.\n\nRun the container interactively:\ndocker run -it nciccbr/ccbr_ubuntu_22.04:v4 bash",
    "crumbs": [
      "Home",
      "Containers",
      "Build docker containers interactively with Google Cloud Shell"
    ]
  },
  {
    "objectID": "docs/containers/build-docker-google-cloud.html#docker-docs",
    "href": "docs/containers/build-docker-google-cloud.html#docker-docs",
    "title": "Build docker containers interactively with Google Cloud Shell",
    "section": "Docker Docs",
    "text": "Docker Docs\nView the Docker getting started tutorial for more detailed information: https://docs.docker.com/get-started/introduction/build-and-push-first-image/",
    "crumbs": [
      "Home",
      "Containers",
      "Build docker containers interactively with Google Cloud Shell"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/7_sop_projpipes.html",
    "href": "docs/GitHub/guide/7_sop_projpipes.html",
    "title": "GitHub Best Practices: Projects and Pipelines",
    "section": "",
    "text": "Users should follow these links to learn more about setting up the repository, before reviewing the best practices below:\n\nPreparing your environment\nBasic Commands\nCreating your GitHub repo\nCreating your Documentation\nGitHub Actions\n\n\n\n\n\nAll pipelines should provide users with documentation for usage, test data, expected outputs, and troubleshooting information. Mkdocs is the recommended tool to perform this action, however, other tools may be utilized. The template’s (NextFlow, Snakemake) were written for mkdocs, and provide basic yaml markdown files provided for this use. They should be edited according to the pipelines function and user needs. Examples of the requirements for each page are provided in the templates.\n\nBackground\n\nInformation on who the pipeline was developed for, and a statement if it’s only been tested on Biowulf.\nAlso include a workflow image to summarize the pipeline.\n\nGetting Started\n\nThis should set the stage for all of the pipeline requirements. This should include the following pages:\n\nIntroduction\nSetup Dependencies\nLogin to the cluster\nLoad an interactive session\n\n\nPreparing Files\n\nThis should include the following pages:\n\nConfigs\n\nCluster Config\nTools Config\nConfig YAML\n\nUser Parameters\nReferences\n\n\nPreparing Manifests\n\nSamples Manifest\n\n\n\nRunning the Pipeline\n\nThis should include all information about the various run commands provided within the pipeline. This should include the following pages:\n\nPipeline Overview\nCommands explained\nTypical Workflow\n\n\nExpected Output\n\nThis should include all pertinent information about output files, including extensions that differentiate files.\n\nRunning Test Data\n\nThis should walk the user through the steps of running the pipeline using test data. This should include the following pages:\n\nGetting Started\nAbout the test data\nSubmit the test data\nReview outputs\n\n\n\n\n\n\n\n\n\n\nAdminTeam should have a minimum of maintain role to the repo\nBoth the develop and master branch must be protected (IE have to have a PR to be changed)\nRepo visibility should be set to Private or Internal and made Public only when required.\n\n\n\n\n\n\n\nAll repositories should follow the strategy outlined in the Creating your GitHub repo\n\n\n\n\n\nAll repositories should include a minimum of two branches at any time:\n\nmain ( or master )\ndev\n\nAdditional branches should be created as needed. These would include feature branches, developed using individual, feature specific addition and hotfix branches, developed using individual, bug specific fixes.\nUtilization of these branches should follow the documentation below.\n\n\n\n\n\nWe encourage the use of the Git Flow tools for some actions, available on Biowulf (module load gitflow). Our current branching strategy is based off of the Git Flow strategy shown below :\n\n ref:https://nvie.com/posts/a-successful-git-branching-model/\n\nMaster (named main or master)\n\nbranch that contains the current release / tagged version of the pipeline\nmerges from Dev branch or hotfix branch allowed\nmerges require actions_master_branch pass from GitHub actions. See GitHub actions #4 for more information testing requirements for merge\n\nDevelop (named dev or activeDev)\n\nbranch that contains current dev\nmerges from feature branch allowed\nmerges require actions_dev_branch pass from GitHub actions. See GitHub actions #3 for more information testing requirements for merge\n\nFeature (named feature/unique_feature_name)\n\nbranch to develop new features that branches off the develop branch\nrecommended usages of git flow feature start unique_feature_name followed by git flow feature publish unique_feature_name\nno merges into this branch are expected\n\nHotfix (named unique_hotfix_name)\n\nbranches arise from a bug that has been discovered and must be resolved; it enables developers to keep working on their own changes on the develop branch while the bug is being fixed\nrecommended usage of git flow hotfix start unique_hotfix_name\nno merges into this branch are expected\n\n\n\n💡 Note\nWhile the git flow feature start command is recommended for feature branch creation, the git flow feature finish command is not. Using the finish command will automatically merge the feature branch into the dev branch without any testing and regardless of any divergence that may have occurred during feature development.\n\n\n\n\n\nAssuming that you have already cloned the repo and initiated git flow with git flow init\nCreate a new feature and publish it git flow feature start unique_feature_name and then git flow feature publish unique_feature_name. The unique_feature_name will be created from the develop branch.\nCode normally, make frequent commits, push frequently to remote. These commits are added to the unique_feature_name branch.\nWhen you are ready to add your feature enhancements back to the develop branch, you may have to make sure that the develop branch has not marched forward while you were working on the unique_feature_name feature (This is possible as other may have added their PRs in the interim). If it has, then you may have to pull in changes made to develop branch into your unique_feature_name feature branch. This can be achieved using the GitHub web interface… merge develop &lt;– unique_feature_name. Resolve conflicts if any.\nRetest your unique_feature_name branch.\nNow send in a pull request using the GitHub web interface to pull your unique_feature_name into develop branch.\nOccasionally, we may create a new release branch from the develop branch and perform thorough E2E testing with fixes. Once, everything is working as expected, we pull the release branch back into develop and also push it to main with a version tag.\n\n\n\n\n\nThe following format of versioning should be followed:\nvX.Y.Z\nThe following rules should be applies when determining the version release:\n\nX is major; non-backward compatible (dependent on the amount of changes; from dev)\nY is minor; backwards compatible (dependent on the amount of changes; from dev)\nZ is patches; backwards compatible (bugs; hot fixes)\n\nOther notes:\n\nX,Y,Z must be numeric only\nAll updates to the main (master) branch must be tagged and versioned using the parameters above\nUpdates to the dev branch can be tagged, but should not be versioned\nIf the pipeline is available locally (IE on Biowulf), version changes should be added for use\n\n\n\n\n\n\nThe following information is meant to outline test_data requirements for all pipelines, however, should be altered to fit the needs of the specific pipeline or project developed.\n\n\n\nLocation of data\n\nTest data sets should be stored within a .test directory, as found in all templates.\n\nDocumentation\n\nReview information on the documentation page, which will provide basic information on test data used within the project/pipeline.\nA README file should be created under the .test directory, to include the following information:\n\nDate of implementation\nInformation on species (IE Homo Sapiens) and data type (IE RNA-Seq)\nInformation on the references to be used (IE hg38)\nMetadata manifests required by the pipeline\nThe source of the files\nLink to scripts used in created the partial test data\n\n\nChoosing a test data set\n\nAt a minimum three test sets are recommended to be available:\n\nShould include sub-sampled inputs, to test the pipelines functionality, and to be used as the tutorial test set.\nShould include full-sample inputs, of high quality, to test the robustness of the pipelines resources\nShould include full-sample inputs, of expected project-level quality, to test the robustness of the pipelines error handling\n\nTest data should come from a CCBR project or a publicly available source. Care should be taken when choosing test data sets, to ensure that the robustness of the pipeline will be tested, as well as the ability of the pipeline to handle both high and low quality data. Multiple test sets may need to be created to meet these goals. On BIOWULF, these test data files can be stored under /data/CCBR_Pipeliner/testdata for easy access by all users of the pipeline(s).",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Best Practices: Projects and Pipelines"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/7_sop_projpipes.html#pipeline-documentation",
    "href": "docs/GitHub/guide/7_sop_projpipes.html#pipeline-documentation",
    "title": "GitHub Best Practices: Projects and Pipelines",
    "section": "",
    "text": "All pipelines should provide users with documentation for usage, test data, expected outputs, and troubleshooting information. Mkdocs is the recommended tool to perform this action, however, other tools may be utilized. The template’s (NextFlow, Snakemake) were written for mkdocs, and provide basic yaml markdown files provided for this use. They should be edited according to the pipelines function and user needs. Examples of the requirements for each page are provided in the templates.\n\nBackground\n\nInformation on who the pipeline was developed for, and a statement if it’s only been tested on Biowulf.\nAlso include a workflow image to summarize the pipeline.\n\nGetting Started\n\nThis should set the stage for all of the pipeline requirements. This should include the following pages:\n\nIntroduction\nSetup Dependencies\nLogin to the cluster\nLoad an interactive session\n\n\nPreparing Files\n\nThis should include the following pages:\n\nConfigs\n\nCluster Config\nTools Config\nConfig YAML\n\nUser Parameters\nReferences\n\n\nPreparing Manifests\n\nSamples Manifest\n\n\n\nRunning the Pipeline\n\nThis should include all information about the various run commands provided within the pipeline. This should include the following pages:\n\nPipeline Overview\nCommands explained\nTypical Workflow\n\n\nExpected Output\n\nThis should include all pertinent information about output files, including extensions that differentiate files.\n\nRunning Test Data\n\nThis should walk the user through the steps of running the pipeline using test data. This should include the following pages:\n\nGetting Started\nAbout the test data\nSubmit the test data\nReview outputs",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Best Practices: Projects and Pipelines"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/7_sop_projpipes.html#repository-management",
    "href": "docs/GitHub/guide/7_sop_projpipes.html#repository-management",
    "title": "GitHub Best Practices: Projects and Pipelines",
    "section": "",
    "text": "AdminTeam should have a minimum of maintain role to the repo\nBoth the develop and master branch must be protected (IE have to have a PR to be changed)\nRepo visibility should be set to Private or Internal and made Public only when required.\n\n\n\n\n\n\n\nAll repositories should follow the strategy outlined in the Creating your GitHub repo\n\n\n\n\n\nAll repositories should include a minimum of two branches at any time:\n\nmain ( or master )\ndev\n\nAdditional branches should be created as needed. These would include feature branches, developed using individual, feature specific addition and hotfix branches, developed using individual, bug specific fixes.\nUtilization of these branches should follow the documentation below.\n\n\n\n\n\nWe encourage the use of the Git Flow tools for some actions, available on Biowulf (module load gitflow). Our current branching strategy is based off of the Git Flow strategy shown below :\n\n ref:https://nvie.com/posts/a-successful-git-branching-model/\n\nMaster (named main or master)\n\nbranch that contains the current release / tagged version of the pipeline\nmerges from Dev branch or hotfix branch allowed\nmerges require actions_master_branch pass from GitHub actions. See GitHub actions #4 for more information testing requirements for merge\n\nDevelop (named dev or activeDev)\n\nbranch that contains current dev\nmerges from feature branch allowed\nmerges require actions_dev_branch pass from GitHub actions. See GitHub actions #3 for more information testing requirements for merge\n\nFeature (named feature/unique_feature_name)\n\nbranch to develop new features that branches off the develop branch\nrecommended usages of git flow feature start unique_feature_name followed by git flow feature publish unique_feature_name\nno merges into this branch are expected\n\nHotfix (named unique_hotfix_name)\n\nbranches arise from a bug that has been discovered and must be resolved; it enables developers to keep working on their own changes on the develop branch while the bug is being fixed\nrecommended usage of git flow hotfix start unique_hotfix_name\nno merges into this branch are expected\n\n\n\n💡 Note\nWhile the git flow feature start command is recommended for feature branch creation, the git flow feature finish command is not. Using the finish command will automatically merge the feature branch into the dev branch without any testing and regardless of any divergence that may have occurred during feature development.\n\n\n\n\n\nAssuming that you have already cloned the repo and initiated git flow with git flow init\nCreate a new feature and publish it git flow feature start unique_feature_name and then git flow feature publish unique_feature_name. The unique_feature_name will be created from the develop branch.\nCode normally, make frequent commits, push frequently to remote. These commits are added to the unique_feature_name branch.\nWhen you are ready to add your feature enhancements back to the develop branch, you may have to make sure that the develop branch has not marched forward while you were working on the unique_feature_name feature (This is possible as other may have added their PRs in the interim). If it has, then you may have to pull in changes made to develop branch into your unique_feature_name feature branch. This can be achieved using the GitHub web interface… merge develop &lt;– unique_feature_name. Resolve conflicts if any.\nRetest your unique_feature_name branch.\nNow send in a pull request using the GitHub web interface to pull your unique_feature_name into develop branch.\nOccasionally, we may create a new release branch from the develop branch and perform thorough E2E testing with fixes. Once, everything is working as expected, we pull the release branch back into develop and also push it to main with a version tag.\n\n\n\n\n\nThe following format of versioning should be followed:\nvX.Y.Z\nThe following rules should be applies when determining the version release:\n\nX is major; non-backward compatible (dependent on the amount of changes; from dev)\nY is minor; backwards compatible (dependent on the amount of changes; from dev)\nZ is patches; backwards compatible (bugs; hot fixes)\n\nOther notes:\n\nX,Y,Z must be numeric only\nAll updates to the main (master) branch must be tagged and versioned using the parameters above\nUpdates to the dev branch can be tagged, but should not be versioned\nIf the pipeline is available locally (IE on Biowulf), version changes should be added for use\n\n\n\n\n\n\nThe following information is meant to outline test_data requirements for all pipelines, however, should be altered to fit the needs of the specific pipeline or project developed.\n\n\n\nLocation of data\n\nTest data sets should be stored within a .test directory, as found in all templates.\n\nDocumentation\n\nReview information on the documentation page, which will provide basic information on test data used within the project/pipeline.\nA README file should be created under the .test directory, to include the following information:\n\nDate of implementation\nInformation on species (IE Homo Sapiens) and data type (IE RNA-Seq)\nInformation on the references to be used (IE hg38)\nMetadata manifests required by the pipeline\nThe source of the files\nLink to scripts used in created the partial test data\n\n\nChoosing a test data set\n\nAt a minimum three test sets are recommended to be available:\n\nShould include sub-sampled inputs, to test the pipelines functionality, and to be used as the tutorial test set.\nShould include full-sample inputs, of high quality, to test the robustness of the pipelines resources\nShould include full-sample inputs, of expected project-level quality, to test the robustness of the pipelines error handling\n\nTest data should come from a CCBR project or a publicly available source. Care should be taken when choosing test data sets, to ensure that the robustness of the pipeline will be tested, as well as the ability of the pipeline to handle both high and low quality data. Multiple test sets may need to be created to meet these goals. On BIOWULF, these test data files can be stored under /data/CCBR_Pipeliner/testdata for easy access by all users of the pipeline(s).",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Best Practices: Projects and Pipelines"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/1_howto_setup.html",
    "href": "docs/GitHub/guide/1_howto_setup.html",
    "title": "GitHub Setup: Preparing the Environment",
    "section": "",
    "text": "The gh is installed on Biowulf at /data/CCBR_Pipeliner/db/PipeDB/bin/gh_1.7.0_linux_amd64/bin/gh. You can run the following lines to edit your ~/.bashrc file to add gh to your $PATH:\necho \"export PATH=$PATH:/data/CCBR_Pipeliner/db/PipeDB/bin/gh_1.7.0_linux_amd64/bin\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\nAlternatively, you can use the git commands provided through a Biowulf module\nmodule load git\n\n\n\nPersonal Access Token (PAT) is required to access GitHub (GH) without having to authenticate by other means (like password) every single time. You will need gh cli installed on your laptop or use /data/CCBR_Pipeliner/db/PipeDB/bin/gh_1.7.0_linux_amd64/bin/gh on Biowulf, as described above. You can create a PAT by going here. Then you can copy the PAT and save it into a file on Biowulf (say ~/gh_token). Next, you can run the following command to set everything up correctly on Biowulf (or your laptop)\ngh auth login --with-token &lt; ~/git_token\n\n\n\nIf you hate to re-enter (username and) password every time you push/pull to/from github (or mkdocs gh-deploy), then it is totally worthwhile to spend a couple minutes to set up SSH keys for auto-authentication. The instructions to do this are available here.",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Setup: Preparing the Environment"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/1_howto_setup.html#using-github-cli",
    "href": "docs/GitHub/guide/1_howto_setup.html#using-github-cli",
    "title": "GitHub Setup: Preparing the Environment",
    "section": "",
    "text": "The gh is installed on Biowulf at /data/CCBR_Pipeliner/db/PipeDB/bin/gh_1.7.0_linux_amd64/bin/gh. You can run the following lines to edit your ~/.bashrc file to add gh to your $PATH:\necho \"export PATH=$PATH:/data/CCBR_Pipeliner/db/PipeDB/bin/gh_1.7.0_linux_amd64/bin\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\nAlternatively, you can use the git commands provided through a Biowulf module\nmodule load git",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Setup: Preparing the Environment"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/1_howto_setup.html#creating-pat-for-gh",
    "href": "docs/GitHub/guide/1_howto_setup.html#creating-pat-for-gh",
    "title": "GitHub Setup: Preparing the Environment",
    "section": "",
    "text": "Personal Access Token (PAT) is required to access GitHub (GH) without having to authenticate by other means (like password) every single time. You will need gh cli installed on your laptop or use /data/CCBR_Pipeliner/db/PipeDB/bin/gh_1.7.0_linux_amd64/bin/gh on Biowulf, as described above. You can create a PAT by going here. Then you can copy the PAT and save it into a file on Biowulf (say ~/gh_token). Next, you can run the following command to set everything up correctly on Biowulf (or your laptop)\ngh auth login --with-token &lt; ~/git_token",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Setup: Preparing the Environment"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/1_howto_setup.html#password-less-login",
    "href": "docs/GitHub/guide/1_howto_setup.html#password-less-login",
    "title": "GitHub Setup: Preparing the Environment",
    "section": "",
    "text": "If you hate to re-enter (username and) password every time you push/pull to/from github (or mkdocs gh-deploy), then it is totally worthwhile to spend a couple minutes to set up SSH keys for auto-authentication. The instructions to do this are available here.",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Setup: Preparing the Environment"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/8_sop_techdev.html",
    "href": "docs/GitHub/guide/8_sop_techdev.html",
    "title": "GitHub Best Practices: TechDev projects",
    "section": "",
    "text": "Users should follow these links to learn more about setting up the repository, before reviewing the best practices below:\n\nPreparing your environment\nBasic Commands\nCreating your GitHub repo\nCreating your Documentation\nGitHub Actions",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Best Practices: TechDev projects"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/8_sop_techdev.html#github-best-practices",
    "href": "docs/GitHub/guide/8_sop_techdev.html#github-best-practices",
    "title": "GitHub Best Practices: TechDev projects",
    "section": "",
    "text": "Users should follow these links to learn more about setting up the repository, before reviewing the best practices below:\n\nPreparing your environment\nBasic Commands\nCreating your GitHub repo\nCreating your Documentation\nGitHub Actions",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Best Practices: TechDev projects"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/8_sop_techdev.html#techdev-documentation",
    "href": "docs/GitHub/guide/8_sop_techdev.html#techdev-documentation",
    "title": "GitHub Best Practices: TechDev projects",
    "section": "TechDev Documentation",
    "text": "TechDev Documentation\n\nAll pipelines should provide users with:\n\ndocumentation for usage\ntest data\nexpected outputs and reports\n\ntroubleshooting information\n\nMarkdown pages can be hosted directly within the repo using GH Pages. Mkdocs is the recommended tool to perform this action, however, other tools may be utilized. The templates (TechDev) template’s (written for mkdocs) provided have basic yaml markdown files provided for this use, and should be edited according to the pipelines function and user needs. Also, track blockers/hurdles using GitHub Issues.\n\nOverview\n\nInformation on the goal of the TechDev project.\n\nAnalysis\n2.1. Background - Provide any relevant background to the project to be completed. This might include information on: - problem that was encountered leading to this techdev - new feature or tool developed to be benchmarked - relevant biological or statistical information needed to perform this analysis\n2.2. Resources\n\nThis page should include any relevant tools that were used for testing. If these tools were loaded via Biowulf, include all version numbers. If they were installed locally, provide information on installation, source location, and any reference documents used.\n\n2.3. Test Data\n\nThis will include information on the test data included within the project. Species information, source information, and references should be included. Any manipulation performed on the source samples should also be included. Manifest information should also be outlined. Provide location to toy dataset or real dataset or both. It is best to park these datasets at a commonly accessible location on Biowulf and share that location here. Also, make the location read-only to prevent accidental edits by other users.\n\n2.4. Analysis Plan\n\nProvide a summary of the plan of action.\nIf benchmarking tools, include the dataset used, where the source material was obtained, and all relevant metadata.\nInclude the location of any relevant config / data files used in this analysis\nProvide information on the limits of the analysis - IE this was only tested on Biowulf, this was only performed in human samples, etc.\n\n2.5. Results\n\nWhat were the findings of this TechDev exercise? Include links to where intermediate files may be located. Include tables, plots, etc. This will serve as a permanent record of the TechDev efforts to be shared within the team and beyond.\n\n2.6 Conclusions\n\nProvide brief, concise conclusion drawn from the analysis, including any alterations being made to pipelines or analysis.\n\nContributions\n\nProvide any names that contributed to this work.",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Best Practices: TechDev projects"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/8_sop_techdev.html#techdev-repository-management",
    "href": "docs/GitHub/guide/8_sop_techdev.html#techdev-repository-management",
    "title": "GitHub Best Practices: TechDev projects",
    "section": "TechDev Repository Management",
    "text": "TechDev Repository Management\n\nSecurity settings\n\nTwo members of CCBR (creator and one manager) should be granted full administrative privileges to the repository to ensure the source code can be accessed by other members, as needed\nBoth the develop and master branch must be protected (IE have to have a PR to be changed)\n\n\n\nCCBR Branch Strategy\n\nBranch Naming\n\nAll repositories should follow the strategy outlined in the Creating your GitHub repo\n\n\n\nBranch Overview\n\nAll repositories should include a minimum of two branches at any time:\n\nmain (master)\ndev\n\nUtilization of these branches should follow the documentation below.\n\n\n\nBranch Strategy\n\nWe encourage the use of the Git Flow tools for some actions, available on Biowulf. Our current branching strategy is based off of the Git Flow strategy shown below :\n\n\n\nImage title\n\n\n\n\nMaster (named main or master)\n\nbranch that contains the current release / tagged version of the pipeline\nmerges from Dev branch or hotfix branch allowed\nmerges require actions_master_branch pass from GitHub actions. See GitHub actions #4 for more information testing requirements for merge\n\nDevelop (named dev or activeDev)\n\nbranch that contains current dev\nmerges from feature branch allowed\nmerges require actions_dev_branch pass from GitHub actions. See GitHub actions #3 for more information testing requirements for merge",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Best Practices: TechDev projects"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/8_sop_techdev.html#techdev-test-data",
    "href": "docs/GitHub/guide/8_sop_techdev.html#techdev-test-data",
    "title": "GitHub Best Practices: TechDev projects",
    "section": "TechDev Test Data",
    "text": "TechDev Test Data\n\nRequirements\n\nLocation of data\n\nTest data sets should be stored within a .test directory, as found in all templates.\n\nDocumentation\n\nReview information on the documentation page, which will provide basic information on test data used within the project/pipeline.\nA README file should be created under the .test directory, to include the following information:\n\nDate of implementation\nInformation on species (IE Homo Sapiens) and data type (IE RNA-Seq)\nInformation on the references to be used (IE hg38)\nMetadata manifests required by the pipeline\nThe source of the files\nLink to scripts used in created the partial test data\n\n\nChoosing a test data set\n\nTest data should come from a CCBR project or a a publicly available source. Care should be taken when choosing test data sets, to ensure that the data matches the goals of the techdev effort.",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Best Practices: TechDev projects"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/3_basic_repo.html",
    "href": "docs/GitHub/guide/3_basic_repo.html",
    "title": "GitHub Basics: Repository",
    "section": "",
    "text": "All CCBR developed pipelines, and techdev efforts should be created under CCBR’s GitHub Org Account and all CCBR team members should have a minimal of read-only permission to the repository.\n\n\n\n\nAll CCBR developed pipelines should be created from the appropriate templates:\n\nNextflow Pipelines: https://github.com/CCBR/CCBR_NextflowTemplate\nSnakemake Pipelines: https://github.com/CCBR/CCBR_SnakemakeTemplate\nTechDev Projects: https://github.com/CCBR/CCBR_TechDevTemplate\n\n\n\nNote: The above templates are themselves under active development! As we continue building a multitude of analysis pipelines, we keep expanding on the list of “commonalities” between these analysis pipelines which need to be added to the template itself. Hence, templates are updated from time-to-time.\n\n\n\n\nTo create a new repository on Github using gh cli, you can run the following command on Biowulf after you update the new repository name (&lt;ADD NEW REPO NAME&gt;) and the repository description (&lt;ADD REPO DESCRIPTION&gt;) commands below.\nNaming Nomenclature: - All Repositories: Do not remove the CCBR/ leading the repository name, as this will correctly place the repository under the CCBR organization account. - CCBR Projects: These should be named with the CCBR#. IE CCBR/CCBR1155 - CCBR Pipelines: These should be descriptive of the pipelines main process; acronyms are encouraged. IE CCBR/CARLISLE - TechDev projects: These should be descriptive of the techDev project to be performed and should begin with techdev_. IE CCBR/techdev_peakcall_benchmarking\n\n\ngh repo create CCBR/&lt;ADD NEW REPO NAME&gt; \\\n--template CCBR/CCBR_NextflowTemplate \\\n--description \"&lt;ADD REPO DESCRIPTION&gt;\" \\\n--public \\\n--confirm\ngh repo create CCBR/&lt;ADD NEW REPO NAME&gt; \\\n--template CCBR/CCBR_SnakemakeTemplate \\\n--description \"&lt;ADD REPO DESCRIPTION&gt;\" \\\n--public \\\n--confirm\n\n\n\ngh repo create CCBR/techdev_&lt;ADD NEW REPO NAME&gt; \\\n--template CCBR/CCBR_TechDevTemplate \\\n--description \"&lt;ADD REPO DESCRIPTION&gt;\" \\\n--public \\\n--confirm\nOnce the repo is created, then you can clone a local copy of the new repository:\ngh repo clone CCBR/&lt;reponame&gt;.git\n\n\n\n\nIf you start from one of the above templates, you’ll have these files already. However, if you’re updating an established repository, you may need to add some of these manually.\n\nCHANGELOG.md\nThe changelog file should be at the top level of your repo. One exception is if your repo is an R package, it should be called NEWS.md instead. You can see an example changelog of a pipeline in active development here.\nVERSION\nThe version file should be at the top level of your repo. If your repo is a Python package, it can be at at the package root instead, e.g. src/pkg/VERSION. If your repo is an R package, the version should be inside the DESCRIPTION file instead. Every time a PR is opened, the PR owner should add a line to the changelog to describe any user-facing changes such as new features added, bugs fixed, documentation updates, or performance improvements.\nYou will also need a CLI command to print the version. The implementation will be different depending on your repo’s language and structure.\nCITATION.cff\nThe citation file must be at the top level of your repo. See template here.\nYou will also need a CLI command to print the citation. The implementation will be different depending on your repo’s language and structure.\nPre-commit config files\nPre-commit hooks provide an automated way to style code, draw attention to typos, and validate commit messages. Learn more about pre-commit here. These files can be customized depending on the needs of the repo. For example, you don’t need the hook for formatting R code if your repo does not and will never contain any R code.\n\n.pre-commit-config.yaml\n.pretterignore\n.prettierrc\n\n.github/PULL_REQUEST_TEMPLATE.md\nThe Pull Request template file helps developers and collaborators remember to write descriptive PR comments, link any relevant issues that the PR resolves, write unit tests, update the docs, and update the changelog. You can customize the Checklist in the template depending on the needs of the repo.\nIssue templates (optional)\nIssue templates help users know how they can best communicate with maintainers to report bugs and request new features. These are helpful but not required.\n\n.github/ISSUE_TEMPLATE/bug_report.yml\n.github/ISSUE_TEMPLATE/config.yml\n.github/ISSUE_TEMPLATE/feature_request.yml\n\nGitHub Actions\nGitHub Actions are automated workflows that run on GitHub’s servers to execute unit tests, render documentation, build docker containers, etc. Most Actions need to be customized for each repo. Learn more about them here.",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Basics: Repository"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/3_basic_repo.html#repository-location",
    "href": "docs/GitHub/guide/3_basic_repo.html#repository-location",
    "title": "GitHub Basics: Repository",
    "section": "",
    "text": "All CCBR developed pipelines, and techdev efforts should be created under CCBR’s GitHub Org Account and all CCBR team members should have a minimal of read-only permission to the repository.",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Basics: Repository"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/3_basic_repo.html#use-of-cookiecutter-templates",
    "href": "docs/GitHub/guide/3_basic_repo.html#use-of-cookiecutter-templates",
    "title": "GitHub Basics: Repository",
    "section": "",
    "text": "All CCBR developed pipelines should be created from the appropriate templates:\n\nNextflow Pipelines: https://github.com/CCBR/CCBR_NextflowTemplate\nSnakemake Pipelines: https://github.com/CCBR/CCBR_SnakemakeTemplate\nTechDev Projects: https://github.com/CCBR/CCBR_TechDevTemplate\n\n\n\nNote: The above templates are themselves under active development! As we continue building a multitude of analysis pipelines, we keep expanding on the list of “commonalities” between these analysis pipelines which need to be added to the template itself. Hence, templates are updated from time-to-time.",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Basics: Repository"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/3_basic_repo.html#creating-a-new-repository",
    "href": "docs/GitHub/guide/3_basic_repo.html#creating-a-new-repository",
    "title": "GitHub Basics: Repository",
    "section": "",
    "text": "To create a new repository on Github using gh cli, you can run the following command on Biowulf after you update the new repository name (&lt;ADD NEW REPO NAME&gt;) and the repository description (&lt;ADD REPO DESCRIPTION&gt;) commands below.\nNaming Nomenclature: - All Repositories: Do not remove the CCBR/ leading the repository name, as this will correctly place the repository under the CCBR organization account. - CCBR Projects: These should be named with the CCBR#. IE CCBR/CCBR1155 - CCBR Pipelines: These should be descriptive of the pipelines main process; acronyms are encouraged. IE CCBR/CARLISLE - TechDev projects: These should be descriptive of the techDev project to be performed and should begin with techdev_. IE CCBR/techdev_peakcall_benchmarking\n\n\ngh repo create CCBR/&lt;ADD NEW REPO NAME&gt; \\\n--template CCBR/CCBR_NextflowTemplate \\\n--description \"&lt;ADD REPO DESCRIPTION&gt;\" \\\n--public \\\n--confirm\ngh repo create CCBR/&lt;ADD NEW REPO NAME&gt; \\\n--template CCBR/CCBR_SnakemakeTemplate \\\n--description \"&lt;ADD REPO DESCRIPTION&gt;\" \\\n--public \\\n--confirm\n\n\n\ngh repo create CCBR/techdev_&lt;ADD NEW REPO NAME&gt; \\\n--template CCBR/CCBR_TechDevTemplate \\\n--description \"&lt;ADD REPO DESCRIPTION&gt;\" \\\n--public \\\n--confirm\nOnce the repo is created, then you can clone a local copy of the new repository:\ngh repo clone CCBR/&lt;reponame&gt;.git",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Basics: Repository"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/3_basic_repo.html#minimal-helper-components",
    "href": "docs/GitHub/guide/3_basic_repo.html#minimal-helper-components",
    "title": "GitHub Basics: Repository",
    "section": "",
    "text": "If you start from one of the above templates, you’ll have these files already. However, if you’re updating an established repository, you may need to add some of these manually.\n\nCHANGELOG.md\nThe changelog file should be at the top level of your repo. One exception is if your repo is an R package, it should be called NEWS.md instead. You can see an example changelog of a pipeline in active development here.\nVERSION\nThe version file should be at the top level of your repo. If your repo is a Python package, it can be at at the package root instead, e.g. src/pkg/VERSION. If your repo is an R package, the version should be inside the DESCRIPTION file instead. Every time a PR is opened, the PR owner should add a line to the changelog to describe any user-facing changes such as new features added, bugs fixed, documentation updates, or performance improvements.\nYou will also need a CLI command to print the version. The implementation will be different depending on your repo’s language and structure.\nCITATION.cff\nThe citation file must be at the top level of your repo. See template here.\nYou will also need a CLI command to print the citation. The implementation will be different depending on your repo’s language and structure.\nPre-commit config files\nPre-commit hooks provide an automated way to style code, draw attention to typos, and validate commit messages. Learn more about pre-commit here. These files can be customized depending on the needs of the repo. For example, you don’t need the hook for formatting R code if your repo does not and will never contain any R code.\n\n.pre-commit-config.yaml\n.pretterignore\n.prettierrc\n\n.github/PULL_REQUEST_TEMPLATE.md\nThe Pull Request template file helps developers and collaborators remember to write descriptive PR comments, link any relevant issues that the PR resolves, write unit tests, update the docs, and update the changelog. You can customize the Checklist in the template depending on the needs of the repo.\nIssue templates (optional)\nIssue templates help users know how they can best communicate with maintainers to report bugs and request new features. These are helpful but not required.\n\n.github/ISSUE_TEMPLATE/bug_report.yml\n.github/ISSUE_TEMPLATE/config.yml\n.github/ISSUE_TEMPLATE/feature_request.yml\n\nGitHub Actions\nGitHub Actions are automated workflows that run on GitHub’s servers to execute unit tests, render documentation, build docker containers, etc. Most Actions need to be customized for each repo. Learn more about them here.",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Basics: Repository"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/2_howto_functions.html",
    "href": "docs/GitHub/guide/2_howto_functions.html",
    "title": "GitHub HowTo: Basic Functions",
    "section": "",
    "text": "GitHub HowTo: Basic Functions\nThe following outlines basic GitHub function to push and pull from your repository. It also includes information on creating a new branch and deleting a branch. These commands should be used in line with guidance on GitHub Repo Management.\n\nPushing local changes to remote\nCheck which files have been changed.\ngit status\nStage files that need to be pushed\ngit add &lt;thisfile&gt;\ngit add &lt;thatfile&gt;\nPush changes to branch named new_feature\ngit push origin new_feature\n\n\nPulling remote changes to local\nPull changes from branch new_feature into your branch old_feature\ngit checkout old_feature\ngit pull new_feature\nIf you have non-compatible changes in the old_feature branch, there are two options: 1) ignore local changes and pull remote anyways. This will delete the changes you’ve made to your remote respository.\ngit reset --hard\ngit pull\n\ntemporarily stash changes away, pull and reapply changes after.\n\ngit stash\ngit pull\ngit stash pop\n\n\nCreating a new branch\nThis is a two step process.\n\nCreate the branch locally\n\ngit checkout -b &lt;newbranch&gt;\n\nPush the branch to remote\n\ngit push -u origin &lt;newbranch&gt;\nOR\ngit push -u origin HEAD\nThis is a shortcut to push the current branch to a branch of the same name on origin and track it so that you don’t need to specify origin HEAD in the future.\n\n\nDeleting branches\n\nLocally\ngit branch -d &lt;BranchName&gt;\n\n\non GitHub\ngit push origin --delete &lt;BranchName&gt;",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub HowTo: Basic Functions"
    ]
  },
  {
    "objectID": "docs/code-club/2025-08-26_establishing-code-club/slides.html#agenda",
    "href": "docs/code-club/2025-08-26_establishing-code-club/slides.html#agenda",
    "title": "Establishing a Community of Practice",
    "section": "Agenda",
    "text": "Agenda\n\nEstablishing a Community of Practice with Code Clubs\nTutorial on building docker containers",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 08 26 Establishing Code Club",
      "Establishing a Community of Practice"
    ]
  },
  {
    "objectID": "docs/code-club/2025-08-26_establishing-code-club/slides.html#survey",
    "href": "docs/code-club/2025-08-26_establishing-code-club/slides.html#survey",
    "title": "Establishing a Community of Practice",
    "section": "Survey",
    "text": "Survey\nPlease fill out this survey if you haven’t already!\nhttps://forms.gle/yT5LWko7LhAmGpZM6",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 08 26 Establishing Code Club",
      "Establishing a Community of Practice"
    ]
  },
  {
    "objectID": "docs/code-club/2025-08-26_establishing-code-club/slides.html#recap",
    "href": "docs/code-club/2025-08-26_establishing-code-club/slides.html#recap",
    "title": "Establishing a Community of Practice",
    "section": "Recap",
    "text": "Recap\nJournal club in February 2025",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 08 26 Establishing Code Club",
      "Establishing a Community of Practice"
    ]
  },
  {
    "objectID": "docs/code-club/2025-08-26_establishing-code-club/slides.html#recap-1",
    "href": "docs/code-club/2025-08-26_establishing-code-club/slides.html#recap-1",
    "title": "Establishing a Community of Practice",
    "section": "Recap",
    "text": "Recap\n\n\n\n\nThe improvement process in software dev is like rock climbing",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 08 26 Establishing Code Club",
      "Establishing a Community of Practice"
    ]
  },
  {
    "objectID": "docs/code-club/2025-08-26_establishing-code-club/slides.html#journal-club-for-code",
    "href": "docs/code-club/2025-08-26_establishing-code-club/slides.html#journal-club-for-code",
    "title": "Establishing a Community of Practice",
    "section": "Journal club for code",
    "text": "Journal club for code\nJust like a regular journal club seminar helps scientists keep up with the literature in their field, code club helps computational scientists keep up with best practices in software development.\n\nHagan et al. 2020. PLOS Comp Biol.\nhttps://doi.org/10.1371/journal.pcbi.1008119",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 08 26 Establishing Code Club",
      "Establishing a Community of Practice"
    ]
  },
  {
    "objectID": "docs/code-club/2025-08-26_establishing-code-club/slides.html#code-club-as-a-community-of-practice",
    "href": "docs/code-club/2025-08-26_establishing-code-club/slides.html#code-club-as-a-community-of-practice",
    "title": "Establishing a Community of Practice",
    "section": "Code Club as a community of practice",
    "text": "Code Club as a community of practice\n\nCommunities of practice are groups of people who share a concern or a passion for something they do and learn how to do it better as they interact regularly.\n\n\nWenger-Trayner, E. and Wenger-Trayner, B. 2015.\nhttps://www.wenger-trayner.com/introduction-to-communities-of-practice.",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 08 26 Establishing Code Club",
      "Establishing a Community of Practice"
    ]
  },
  {
    "objectID": "docs/code-club/2025-08-26_establishing-code-club/slides.html#goals-of-code-club",
    "href": "docs/code-club/2025-08-26_establishing-code-club/slides.html#goals-of-code-club",
    "title": "Establishing a Community of Practice",
    "section": "Goals of Code Club",
    "text": "Goals of Code Club\n\nAdopt good practices in software development\nImprove the quality of our code\nLearn from each other\nFocus on CCBR-specific practices, ways of working",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 08 26 Establishing Code Club",
      "Establishing a Community of Practice"
    ]
  },
  {
    "objectID": "docs/code-club/2025-08-26_establishing-code-club/slides.html#formats-for-code-club",
    "href": "docs/code-club/2025-08-26_establishing-code-club/slides.html#formats-for-code-club",
    "title": "Establishing a Community of Practice",
    "section": "Formats for code club",
    "text": "Formats for code club\n\nTraditional powerpoint slides\nLive demos\nHands-on activities\nPeer code review\n\nExperimentation is encouraged!",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 08 26 Establishing Code Club",
      "Establishing a Community of Practice"
    ]
  },
  {
    "objectID": "docs/code-club/2025-08-26_establishing-code-club/slides.html#resource-sharing",
    "href": "docs/code-club/2025-08-26_establishing-code-club/slides.html#resource-sharing",
    "title": "Establishing a Community of Practice",
    "section": "Resource Sharing",
    "text": "Resource Sharing\nCode Club resources can be shared in the CCBR HowTos repo.\n\npresentation slides\ntutorial documents\nlinks for further learning\n\n\n\n\n\nhttps://github.com/CCBR/HowTos",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 08 26 Establishing Code Club",
      "Establishing a Community of Practice"
    ]
  },
  {
    "objectID": "docs/code-club/2025-08-26_establishing-code-club/slides.html#discussion",
    "href": "docs/code-club/2025-08-26_establishing-code-club/slides.html#discussion",
    "title": "Establishing a Community of Practice",
    "section": "Discussion",
    "text": "Discussion\nQuestions, comments, suggestions?",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 08 26 Establishing Code Club",
      "Establishing a Community of Practice"
    ]
  },
  {
    "objectID": "docs/code-club/2025-08-26_establishing-code-club/slides.html#next-docker-tutorial",
    "href": "docs/code-club/2025-08-26_establishing-code-club/slides.html#next-docker-tutorial",
    "title": "Establishing a Community of Practice",
    "section": "Next: docker tutorial",
    "text": "Next: docker tutorial",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 08 26 Establishing Code Club",
      "Establishing a Community of Practice"
    ]
  },
  {
    "objectID": "docs/conda_to_mamba.html",
    "href": "docs/conda_to_mamba.html",
    "title": "Move from Conda to Mamba",
    "section": "",
    "text": "Migrating from Anaconda/Miniconda (conda) to Miniforge (mamba)\n  \n  1. List the conda environments\n  2. Download the Python script to fix exported YAML\n  3. Create mamba compatible exported YAML files\n  4. Uninstall conda (Anaconda/Miniconda)\n  5. Install mamba (Miniforge)\n  6. Clear cache\n  7. Enforce strict channel policy\n  8. Verify channel configuration\n  9. Rebuild old env with mamba\n  Appendix A: fix_yaml.py",
    "crumbs": [
      "Home",
      "Move from Conda to Mamba"
    ]
  },
  {
    "objectID": "docs/conda_to_mamba.html#list-the-conda-environments",
    "href": "docs/conda_to_mamba.html#list-the-conda-environments",
    "title": "Move from Conda to Mamba",
    "section": "1. List the conda environments",
    "text": "1. List the conda environments\nUse your current conda to list environments before migrating.\nconda env list\n# or (equivalent)\nconda info --envs",
    "crumbs": [
      "Home",
      "Move from Conda to Mamba"
    ]
  },
  {
    "objectID": "docs/conda_to_mamba.html#download-the-python-script-to-fix-exported-yaml",
    "href": "docs/conda_to_mamba.html#download-the-python-script-to-fix-exported-yaml",
    "title": "Move from Conda to Mamba",
    "section": "2. Download the Python script to fix exported YAML",
    "text": "2. Download the Python script to fix exported YAML\nThis script: - removes duplicate packages, - drops prefix: and other install-specific metadata, - forces conda-forge, bioconda and defaults as the only channels, - prepares the file for mamba environment creation.\n\n\n\n\n\n\nImportant\n\n\n\nCopy the script from Appendix A into fix_yaml.py.",
    "crumbs": [
      "Home",
      "Move from Conda to Mamba"
    ]
  },
  {
    "objectID": "docs/conda_to_mamba.html#create-mamba-compatible-exported-yaml-files",
    "href": "docs/conda_to_mamba.html#create-mamba-compatible-exported-yaml-files",
    "title": "Move from Conda to Mamba",
    "section": "3. Create mamba compatible exported YAML files",
    "text": "3. Create mamba compatible exported YAML files\nExport a single environment named xyz:\nconda env export --name xyz | python3 fix_yaml.py - -o xyz.yml\n\n\n\n\n\n\nNote\n\n\n\nYou can export all environments at once:\nfor env in $(conda env list | grep -v \"^#\" | awk 'NF&gt;1 {print $1}'); do\n  conda env export --name \"$env\" | python3 fix_yaml.py - -o \"${env}.yml\"\ndone\n\n\n\n\n\n\n\n\nCaution\n\n\n\nIf you have path-only environments (created from arbitrary folders), conda env list may show full paths instead of simple names. Consider exporting those individually to ensure correct name: in the YAML.",
    "crumbs": [
      "Home",
      "Move from Conda to Mamba"
    ]
  },
  {
    "objectID": "docs/conda_to_mamba.html#uninstall-conda-anacondaminiconda",
    "href": "docs/conda_to_mamba.html#uninstall-conda-anacondaminiconda",
    "title": "Move from Conda to Mamba",
    "section": "4. Uninstall conda (Anaconda/Miniconda)",
    "text": "4. Uninstall conda (Anaconda/Miniconda)\nFollow vendor instructions (macOS/Linux):\nhttps://www.anaconda.com/docs/getting-started/anaconda/uninstall#macos-or-linux\nThen remove common leftovers:\n# If Miniconda\nrm -rf ~/miniconda ~/miniconda3\n\n# If Anaconda\nrm -rf ~/anaconda ~/anaconda3\n\nrm -rf ~/.conda\nrm -rf ~/.condarc\nrm -rf ~/.continuum\nrm -rf ~/.conda_envs_dir\nrm -rf ~/.conda_envs_dir_test\nEdit your shell init and delete the conda init block:\n# &gt;&gt;&gt; conda initialize &gt;&gt;&gt;\n...\n# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;\nFiles to check (depending on your shell): ~/.zshrc, ~/.bashrc, ~/.bash_profile.\nRestart your terminal.\n\n\n\n\n\n\nWarning\n\n\n\nMake sure conda is no longer on your PATH:\nwhich conda || echo \"conda not found (expected)\"",
    "crumbs": [
      "Home",
      "Move from Conda to Mamba"
    ]
  },
  {
    "objectID": "docs/conda_to_mamba.html#install-mamba-miniforge",
    "href": "docs/conda_to_mamba.html#install-mamba-miniforge",
    "title": "Move from Conda to Mamba",
    "section": "5. Install mamba (Miniforge)",
    "text": "5. Install mamba (Miniforge)\nDownload mamba for macOS:\n# Apple Silicon (M1/M2/M3):\ncurl -L -O https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge-MacOSX-arm64.sh\n\n# Intel Macs:\ncurl -L -O https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge-MacOSX-x86_64.sh\n\n# Install (interactive)\nbash Miniforge-MacOSX-*.sh\nVerify:\nmamba --version\nmamba info | sed -n '1,60p'\n\n\n\n\n\n\nNote\n\n\n\nOnce mamba installation is completed then delete the install script.\nrm -f Miniforge-MacOSX-*.sh",
    "crumbs": [
      "Home",
      "Move from Conda to Mamba"
    ]
  },
  {
    "objectID": "docs/conda_to_mamba.html#clear-cache",
    "href": "docs/conda_to_mamba.html#clear-cache",
    "title": "Move from Conda to Mamba",
    "section": "6. Clear cache",
    "text": "6. Clear cache\n# Safe to run; ignores errors if conda is gone\nconda clean --all -y || true\nmamba clean --all -y || true",
    "crumbs": [
      "Home",
      "Move from Conda to Mamba"
    ]
  },
  {
    "objectID": "docs/conda_to_mamba.html#enforce-strict-channel-policy",
    "href": "docs/conda_to_mamba.html#enforce-strict-channel-policy",
    "title": "Move from Conda to Mamba",
    "section": "7. Enforce strict channel policy",
    "text": "7. Enforce strict channel policy\n# Start fresh (ignore error if key doesn't exist)\nmamba config remove-key channels 2&gt;/dev/null || true\n\n# Add exactly the three you want\nmamba config prepend channels conda-forge\nmamba config append  channels bioconda\nmamba config append  channels defaults\n\n# Recommended\nmamba config set channel_priority strict\nThis: - forces to be conda-forge, bioconda and defaults in that order - removes any other channels that may point to anaconda.com",
    "crumbs": [
      "Home",
      "Move from Conda to Mamba"
    ]
  },
  {
    "objectID": "docs/conda_to_mamba.html#verify-channel-configuration",
    "href": "docs/conda_to_mamba.html#verify-channel-configuration",
    "title": "Move from Conda to Mamba",
    "section": "8. Verify channel configuration",
    "text": "8. Verify channel configuration\n# High-level info (shows channel list)\nmamba info\n\n# Full config dump\nmamba config list | sed -n '1,200p'\nWhat you want to see: - channels shows only conda-forge, bioconda, and defaults - Nothing should reference repo.anaconda.com or anaconda.org.",
    "crumbs": [
      "Home",
      "Move from Conda to Mamba"
    ]
  },
  {
    "objectID": "docs/conda_to_mamba.html#rebuild-old-env-with-mamba",
    "href": "docs/conda_to_mamba.html#rebuild-old-env-with-mamba",
    "title": "Move from Conda to Mamba",
    "section": "9. Rebuild old env with mamba",
    "text": "9. Rebuild old env with mamba\nRecreate from your cleaned YAMLs:\n# Single env\nmamba create -f xyz.yml --yes --override-channels\n\n# All YAMLs in current directory\nfor y in *.yml *.yaml 2&gt;/dev/null; do\n  [ -f \"$y\" ] || continue\n  echo \"Rebuilding from $y\"\n  mamba create -f \"$y\" --yes --override-channels\ndone\n\n\n\n\n\n\nWarning\n\n\n\nDuring solve, you should NOT see:\nwarning  libmamba 'repo.anaconda.com', a commercial channel hosted by Anaconda.com, is used.\nIf you do, revisit Steps 7–8 to correct your channel configuration.\n\n\nActivate and test:\nmamba activate xyz\npython -V",
    "crumbs": [
      "Home",
      "Move from Conda to Mamba"
    ]
  },
  {
    "objectID": "docs/conda_to_mamba.html#appendix-a-fix_yaml.py",
    "href": "docs/conda_to_mamba.html#appendix-a-fix_yaml.py",
    "title": "Move from Conda to Mamba",
    "section": "Appendix A: fix_yaml.py",
    "text": "Appendix A: fix_yaml.py\n\n\n\n\n\n\nNote\n\n\n\nThis script reads an exported conda env export YAML from stdin (or a file), removes duplicates, deletes prefix:, forces appropriate channels, removes unwanted channels, preserves pip: sections, and writes the cleaned YAML to stdout (or -o FILE).\n\n\n#!/usr/bin/env python3\n\"\"\"\nfix_yaml.py\n- Normalize an exported conda environment YAML for mamba recreation.\n- Drops install-specific fields (prefix), forces conda-forge channels, de-duplicates dependencies.\nUsage:\n  python3 fix_yaml.py input.yml -o output.yml\n  conda env export --name myenv | python3 fix_yaml.py - -o myenv.yml\n\"\"\"\nimport sys\nimport argparse\nimport io\nfrom collections import OrderedDict\n\ntry:\n    import yaml  # PyYAML if available\nexcept Exception:\n    yaml = None\n\ndef load_text(stream):\n    return stream.read()\n\ndef parse_yaml(text):\n    if yaml is None:\n        # Minimal fallback: very light parser for a subset of env YAML\n        # It preserves lines and does line-level dedup under `dependencies:`.\n        # For robust parsing, install PyYAML: mamba install pyyaml\n        data = {\"_raw\": text}\n        return data\n    return yaml.safe_load(text)\n\ndef dump_yaml(data):\n    if yaml is None:\n        # Fallback: return original text with only simple text transforms\n        lines = data[\"_raw\"].splitlines(True)\n\n        out, in_deps, seen, skip_prefix = [], False, set(), False\n        for ln in lines:\n            if ln.startswith(\"name:\"):\n                out.append(ln)\n                continue\n            if ln.strip().startswith(\"prefix:\"):\n                # drop\n                continue\n            if ln.strip().startswith(\"channels:\"):\n                out.append(\"channels:\\n  - conda-forge\\n  - bioconda\\n  - defaults\\n\")\n                skip_prefix = True\n                continue\n            if skip_prefix:\n                # skip original channel lines until a non-channel item appears\n                if ln.startswith(\" \") or ln.startswith(\"\\t\") or ln.strip().startswith(\"-\"):\n                    # still channel block; skip\n                    continue\n                else:\n                    skip_prefix = False\n\n            if ln.strip() == \"dependencies:\":\n                in_deps = True\n                out.append(ln)\n                continue\n\n            if in_deps:\n                    # New rule: handle lines with two '='\n                if ln.count(\"=\") &gt;= 2:\n                    first = ln.find(\"=\")\n                    second = ln.find(\"=\", first + 1)\n                    if second == first + 1:\n                        out.append(ln)\n                        continue\n                    else:\n                        # case like 'pkg=1=py39_0' → keep only before 2nd '='\n                        parts = ln.split(\"=\")\n                        newln = \"=\".join(parts[:2]) + \"\\n\"\n                        out.append(newln)\n                        continue\n                else:\n                    out.append(ln)\n                    if ln[0] != \" \" and ln.strip()[-1] == \":\":\n                        in_deps = False\n                    continue\n\n        return \"\".join(out)\n\n    # PyYAML path (preferred)\n    if data is None:\n        data = {}\n    data.pop(\"prefix\", None)\n\n    # Normalize channels → conda-forge only\n    data[\"channels\"] = [\"conda-forge\"]\n\n    # De-duplicate top-level conda dependencies\n    deps = data.get(\"dependencies\", [])\n    new_deps = []\n    seen = set()\n\n    for item in deps:\n        if isinstance(item, str):\n            if item not in seen:\n                new_deps.append(item)\n                seen.add(item)\n        elif isinstance(item, dict) and \"pip\" in item:\n            # Keep pip sublist as-is (with de-dup inside)\n            pip_list = item.get(\"pip\", [])\n            pip_seen = set()\n            new_pip = []\n            for p in pip_list:\n                if p not in pip_seen:\n                    new_pip.append(p)\n                    pip_seen.add(p)\n            new_deps.append({\"pip\": new_pip})\n        else:\n            # Unknown structured entry; keep\n            new_deps.append(item)\n\n    data[\"dependencies\"] = new_deps\n\n    # Emit YAML\n    class OrderedDumper(yaml.SafeDumper):\n        pass\n    def _dict_representer(dumper, data):\n        return dumper.represent_dict(data.items())\n    OrderedDumper.add_representer(OrderedDict, _dict_representer)\n\n    return yaml.dump(data, Dumper=OrderedDumper, sort_keys=False)\n\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"input\", help=\"Input YAML file or '-' for stdin\")\n    ap.add_argument(\"-o\", \"--output\", help=\"Output YAML file (default: stdout)\")\n    args = ap.parse_args()\n\n    if args.input == \"-\":\n        text = load_text(sys.stdin)\n    else:\n        with open(args.input, \"r\", encoding=\"utf-8\") as f:\n            text = f.read()\n\n    data = parse_yaml(text)\n    out = dump_yaml(data)\n\n    if args.output:\n        with open(args.output, \"w\", encoding=\"utf-8\") as f:\n            f.write(out)\n    else:\n        sys.stdout.write(out)\n\nif __name__ == \"__main__\":\n    main()\n\nThat’s it! You’ve exported your old conda environments, removed Anaconda/Miniconda, installed Miniforge, enforced conda-forge only, and rebuilt your environments with mamba—without touching anaconda.org.",
    "crumbs": [
      "Home",
      "Move from Conda to Mamba"
    ]
  },
  {
    "objectID": "docs/zenodo.html",
    "href": "docs/zenodo.html",
    "title": "Zenodo",
    "section": "",
    "text": "Submit a pipeline to Zenodo in order to create a DOI for publication.",
    "crumbs": [
      "Home",
      "Zenodo"
    ]
  },
  {
    "objectID": "docs/zenodo.html#reference-link",
    "href": "docs/zenodo.html#reference-link",
    "title": "Zenodo",
    "section": "Reference link",
    "text": "Reference link\nUse the link for full information, summarized below: https://www.youtube.com/watch?v=A9FGAU9S9Ow",
    "crumbs": [
      "Home",
      "Zenodo"
    ]
  },
  {
    "objectID": "docs/zenodo.html#prepare-github-repository",
    "href": "docs/zenodo.html#prepare-github-repository",
    "title": "Zenodo",
    "section": "Prepare GitHub Repository",
    "text": "Prepare GitHub Repository\nThe GitHub repository should include the following:\n\nREADME page\nDocumentation page, such as mkdocs, with usage and contact information\nA citation CITATION.cff; Example here\nTagged and versioned, stable repository",
    "crumbs": [
      "Home",
      "Zenodo"
    ]
  },
  {
    "objectID": "docs/zenodo.html#link-github-account-to-zenodo",
    "href": "docs/zenodo.html#link-github-account-to-zenodo",
    "title": "Zenodo",
    "section": "Link GitHub account to Zenodo",
    "text": "Link GitHub account to Zenodo\n\nGo to Zenodo\nSelect username in the top right &gt;&gt; Profile. Select `GitHub``\nClick Sync Now (top right) to update repos. NOTE: You may have to refresh the page\nToggle the On button on the repo you wish to publish. This will move the pipeline to the Enable Repositories list.",
    "crumbs": [
      "Home",
      "Zenodo"
    ]
  },
  {
    "objectID": "docs/zenodo.html#prepare-github-repo",
    "href": "docs/zenodo.html#prepare-github-repo",
    "title": "Zenodo",
    "section": "Prepare GitHub Repo",
    "text": "Prepare GitHub Repo\n\nGo to GitHub and find the repository page.\nSelect Releases &gt;&gt; Draft a new release\nCreate a tag, following naming semantics described here\nDescribe the tag with the following: “Connecting pipeline to Zenodo”",
    "crumbs": [
      "Home",
      "Zenodo"
    ]
  },
  {
    "objectID": "docs/zenodo.html#update-zenodo-information",
    "href": "docs/zenodo.html#update-zenodo-information",
    "title": "Zenodo",
    "section": "Update Zenodo Information",
    "text": "Update Zenodo Information\n\nGo to Zenodo\nSelect My dashboard &gt;&gt; Edit\nUpdate the following information:\n\nResource Type: Software\nTitle: Full Pipeline Name (ShorthandName) (IE Mouse nEoanTigen pRedictOr (METRO)\nCreators: Add creators, including ORCID’s whenever possible\nDescription: A short description of the main features of the pipeline\nAdditional Description: If you use this software, please cite it as below.\nKeywords and subjects: Add several keywords related to the pipeline\nVersion: add the version used in GitHub\nPublisher: Zenodo\nRelated works: “Is original form of” “github website” URL",
    "crumbs": [
      "Home",
      "Zenodo"
    ]
  },
  {
    "objectID": "docs/zenodo.html#add-doi-citation-to-github",
    "href": "docs/zenodo.html#add-doi-citation-to-github",
    "title": "Zenodo",
    "section": "Add DOI, citation to GitHub",
    "text": "Add DOI, citation to GitHub\n\nGo to Zenodo\nSelect username in the top right &gt;&gt; Profile. Select `GitHub``\nClick Sync Now (top right) to update repos. NOTE: You may have to refresh the page\nCopy the DOI for the repository\nReturn to the GitHub repository and edit the README of the GitHub repo, adding the DOI link.\nUpdate the CITATION.cff as needed.\nCreate a new tagged version.",
    "crumbs": [
      "Home",
      "Zenodo"
    ]
  }
]