[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CCBR How-Tos",
    "section": "",
    "text": "This website is designed to share knowledge between analysts and engineers in CCBR.\nYou‚Äôll find how-to-guides, best practices and tutorials under the pages in the sidebar to the left.\nThis page was created through the contributions of several members in CCBR. If you would like to contribute to its development, please reach out to Vishal Koparde to get added to this repo in order to submit a PR."
  },
  {
    "objectID": "docs/lucidcharts.html",
    "href": "docs/lucidcharts.html",
    "title": "LucidChart",
    "section": "",
    "text": "Remember:",
    "crumbs": [
      "Home",
      "LucidChart"
    ]
  },
  {
    "objectID": "docs/lucidcharts.html#existing-document",
    "href": "docs/lucidcharts.html#existing-document",
    "title": "LucidChart",
    "section": "Existing document:",
    "text": "Existing document:\nFor all existing Lucid Charts documents, transfer ownership to nciccbr@mail.nih.gov. This is a 2-step process:\n\nShare the document with edit permissions with nciccbr@mail.nih.gov. Reach out to Vishal Koparde to get the sharing invite accepted.\nOnce, nciccbr@mail.nih.gov has accepted the invite, transfer the ownership over to nciccbr@mail.nih.gov. For doing so please follow the instructions here.",
    "crumbs": [
      "Home",
      "LucidChart"
    ]
  },
  {
    "objectID": "docs/lucidcharts.html#creating-new-document",
    "href": "docs/lucidcharts.html#creating-new-document",
    "title": "LucidChart",
    "section": "Creating new document:",
    "text": "Creating new document:\nYou can create new documents after logging into lucidcharts with your NIH.gov email account. Then, follow the above instructions to transfer ownership to nciccbr@mail.nih.gov.",
    "crumbs": [
      "Home",
      "LucidChart"
    ]
  },
  {
    "objectID": "docs/code-club/2025-08-26_establishing-code-club/slides.html#agenda",
    "href": "docs/code-club/2025-08-26_establishing-code-club/slides.html#agenda",
    "title": "Establishing a Community of Practice",
    "section": "Agenda",
    "text": "Agenda\n\nEstablishing a Community of Practice with Code Clubs\nTutorial on building docker containers",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 08 26 Establishing Code Club",
      "Establishing a Community of Practice"
    ]
  },
  {
    "objectID": "docs/code-club/2025-08-26_establishing-code-club/slides.html#survey",
    "href": "docs/code-club/2025-08-26_establishing-code-club/slides.html#survey",
    "title": "Establishing a Community of Practice",
    "section": "Survey",
    "text": "Survey\nPlease fill out this survey if you haven‚Äôt already!\nhttps://forms.gle/yT5LWko7LhAmGpZM6",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 08 26 Establishing Code Club",
      "Establishing a Community of Practice"
    ]
  },
  {
    "objectID": "docs/code-club/2025-08-26_establishing-code-club/slides.html#recap",
    "href": "docs/code-club/2025-08-26_establishing-code-club/slides.html#recap",
    "title": "Establishing a Community of Practice",
    "section": "Recap",
    "text": "Recap\nJournal club in February 2025",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 08 26 Establishing Code Club",
      "Establishing a Community of Practice"
    ]
  },
  {
    "objectID": "docs/code-club/2025-08-26_establishing-code-club/slides.html#recap-1",
    "href": "docs/code-club/2025-08-26_establishing-code-club/slides.html#recap-1",
    "title": "Establishing a Community of Practice",
    "section": "Recap",
    "text": "Recap\n\n\n\n\nThe improvement process in software dev is like rock climbing",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 08 26 Establishing Code Club",
      "Establishing a Community of Practice"
    ]
  },
  {
    "objectID": "docs/code-club/2025-08-26_establishing-code-club/slides.html#journal-club-for-code",
    "href": "docs/code-club/2025-08-26_establishing-code-club/slides.html#journal-club-for-code",
    "title": "Establishing a Community of Practice",
    "section": "Journal club for code",
    "text": "Journal club for code\nJust like a regular journal club seminar helps scientists keep up with the literature in their field, code club helps computational scientists keep up with best practices in software development.\n\nHagan et al.¬†2020. PLOS Comp Biol.\nhttps://doi.org/10.1371/journal.pcbi.1008119",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 08 26 Establishing Code Club",
      "Establishing a Community of Practice"
    ]
  },
  {
    "objectID": "docs/code-club/2025-08-26_establishing-code-club/slides.html#code-club-as-a-community-of-practice",
    "href": "docs/code-club/2025-08-26_establishing-code-club/slides.html#code-club-as-a-community-of-practice",
    "title": "Establishing a Community of Practice",
    "section": "Code Club as a community of practice",
    "text": "Code Club as a community of practice\n\nCommunities of practice are groups of people who share a concern or a passion for something they do and learn how to do it better as they interact regularly.\n\n\nWenger-Trayner, E. and Wenger-Trayner, B. 2015.\nhttps://www.wenger-trayner.com/introduction-to-communities-of-practice.",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 08 26 Establishing Code Club",
      "Establishing a Community of Practice"
    ]
  },
  {
    "objectID": "docs/code-club/2025-08-26_establishing-code-club/slides.html#goals-of-code-club",
    "href": "docs/code-club/2025-08-26_establishing-code-club/slides.html#goals-of-code-club",
    "title": "Establishing a Community of Practice",
    "section": "Goals of Code Club",
    "text": "Goals of Code Club\n\nAdopt good practices in software development\nImprove the quality of our code\nLearn from each other\nFocus on CCBR-specific practices, ways of working",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 08 26 Establishing Code Club",
      "Establishing a Community of Practice"
    ]
  },
  {
    "objectID": "docs/code-club/2025-08-26_establishing-code-club/slides.html#formats-for-code-club",
    "href": "docs/code-club/2025-08-26_establishing-code-club/slides.html#formats-for-code-club",
    "title": "Establishing a Community of Practice",
    "section": "Formats for code club",
    "text": "Formats for code club\n\nTraditional powerpoint slides\nLive demos\nHands-on activities\nPeer code review\n\nExperimentation is encouraged!",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 08 26 Establishing Code Club",
      "Establishing a Community of Practice"
    ]
  },
  {
    "objectID": "docs/code-club/2025-08-26_establishing-code-club/slides.html#resource-sharing",
    "href": "docs/code-club/2025-08-26_establishing-code-club/slides.html#resource-sharing",
    "title": "Establishing a Community of Practice",
    "section": "Resource Sharing",
    "text": "Resource Sharing\nCode Club resources can be shared in the CCBR HowTos repo.\n\npresentation slides\ntutorial documents\nlinks for further learning\n\n\n\n\n\nhttps://github.com/CCBR/HowTos",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 08 26 Establishing Code Club",
      "Establishing a Community of Practice"
    ]
  },
  {
    "objectID": "docs/code-club/2025-08-26_establishing-code-club/slides.html#discussion",
    "href": "docs/code-club/2025-08-26_establishing-code-club/slides.html#discussion",
    "title": "Establishing a Community of Practice",
    "section": "Discussion",
    "text": "Discussion\nQuestions, comments, suggestions?",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 08 26 Establishing Code Club",
      "Establishing a Community of Practice"
    ]
  },
  {
    "objectID": "docs/code-club/2025-08-26_establishing-code-club/slides.html#next-docker-tutorial",
    "href": "docs/code-club/2025-08-26_establishing-code-club/slides.html#next-docker-tutorial",
    "title": "Establishing a Community of Practice",
    "section": "Next: docker tutorial",
    "text": "Next: docker tutorial",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 08 26 Establishing Code Club",
      "Establishing a Community of Practice"
    ]
  },
  {
    "objectID": "docs/conda-mamba/ccbr-mamba-biowulf.html",
    "href": "docs/conda-mamba/ccbr-mamba-biowulf.html",
    "title": "Using the shared CCBR mamba installation on Biowulf",
    "section": "",
    "text": "You can configure conda/mamba to use our shared CCBR installation of miniforge3 on Biowulf, which will allow you to access CCBR environments while also creating your own environments.",
    "crumbs": [
      "Home",
      "Conda Mamba",
      "Using the shared CCBR mamba installation on Biowulf"
    ]
  },
  {
    "objectID": "docs/conda-mamba/ccbr-mamba-biowulf.html#initialize-your-shell",
    "href": "docs/conda-mamba/ccbr-mamba-biowulf.html#initialize-your-shell",
    "title": "Using the shared CCBR mamba installation on Biowulf",
    "section": "Initialize your shell",
    "text": "Initialize your shell\nInitialize your shell with mamba. This will modify your shell‚Äôs rc file (e.g. ~/.zshrc or ~/.bashrc) to allow you to use mamba.\nSpecify your shell language with --shell SHELL. The below example is for zsh:\n/data/CCBR_Pipeliner/db/PipeDB/miniforge3/bin/mamba shell init \\\n  --shell zsh --root-prefix=/data/$USER/mamba\nor bash:\n/data/CCBR_Pipeliner/db/PipeDB/miniforge3/bin/mamba shell init \\\n  --shell bash --root-prefix=/data/$USER/mamba\nWe recommend setting --root-prefix to your data directory /data/$USER since more space is available than in your home directory ~/.\nSource your shell‚Äôs rc file to make these changes take effect.\nsource ~/.zshrc\nsource ~/.bashrc",
    "crumbs": [
      "Home",
      "Conda Mamba",
      "Using the shared CCBR mamba installation on Biowulf"
    ]
  },
  {
    "objectID": "docs/conda-mamba/ccbr-mamba-biowulf.html#update-your-conda-configuration",
    "href": "docs/conda-mamba/ccbr-mamba-biowulf.html#update-your-conda-configuration",
    "title": "Using the shared CCBR mamba installation on Biowulf",
    "section": "Update your conda configuration",
    "text": "Update your conda configuration\nCopy and paste the following contents into your conda configuration file: ~/.condarc\nchannels:\n  - conda-forge\n  - bioconda\nchannel_priority: strict\nshow_channel_urls: true\nenvs_dirs:\n  - /data/$USER/mamba/envs/\n  - /data/CCBR_Pipeliner/db/PipeDB/miniforge3/envs/\npkgs_dirs:\n  - /data/$USER/mamba/pkgs/\ndenylist_channels:\n  - https://repo.anaconda.com/pkgs/main\n  - https://repo.anaconda.com/pkgs/r\nYou can optionally add other configuration options as you see fit (e.g.¬†auto_activate_base), but it is important to at least have all of the content above.",
    "crumbs": [
      "Home",
      "Conda Mamba",
      "Using the shared CCBR mamba installation on Biowulf"
    ]
  },
  {
    "objectID": "docs/conda-mamba/ccbr-mamba-biowulf.html#verify-mamba-setup",
    "href": "docs/conda-mamba/ccbr-mamba-biowulf.html#verify-mamba-setup",
    "title": "Using the shared CCBR mamba installation on Biowulf",
    "section": "Verify mamba setup",
    "text": "Verify mamba setup\nLet‚Äôs make sure everything is working as expected. You should be able to activate shared CCBR environments, and also create new environments for your own personal use.\nmamba info\n       libmamba version : 2.1.1\n          mamba version : 2.1.1\n           curl version : libcurl/8.14.1 OpenSSL/3.5.1 zlib/1.3.1 zstd/1.5.7 libssh2/1.11.1 nghttp2/1.64.0\n     libarchive version : libarchive 3.7.7 zlib/1.3.1 liblzma/5.8.1 bz2lib/1.0.8 liblz4/1.10.0 libzstd/1.5.7\n       envs directories : /gpfs/gsfs12/users/$USER/mamba/envs\n                          /vf/users/CCBR_Pipeliner/db/PipeDB/miniforge3/envs\n          package cache : /gpfs/gsfs12/users/$USER/mamba/pkgs\n            environment : base (active)\n           env location : /gpfs/gsfs12/users/$USER/mamba\n      user config files : /home/$USER/.mambarc\n populated config files : /home/$USER/.condarc\n       virtual packages : __unix=0=0\n                          __linux=4.18.0=0\n                          __glibc=2.28=0\n                          __archspec=1=x86_64_v4\n               channels : https://conda.anaconda.org/conda-forge/linux-64\n                          https://conda.anaconda.org/conda-forge/noarch\n                          https://conda.anaconda.org/bioconda/linux-64\n                          https://conda.anaconda.org/bioconda/noarch\n       base environment : /gpfs/gsfs12/users/$USER/mamba\n               platform : linux-64\n($USER will be filled in with your actual username on Biowulf.)\nNote that the channel URLs point to https://conda.anaconda.org/ (only community-supported channels), and no channels point to https://repo.anaconda.com (commercial channels).\nNew environments you create will be in /data/$USER/mamba/envs/, but you will also be able to activate any environment listed in /data/CCBR_Pipeliner/db/PipeDB/miniforge3/envs/\n\nActivate a CCBR environment\nmamba activate py3.11-8\nmamba deactivate\n\n\nCreate a new environment for your own use\nThe environment will be created in /data/$USER/mamba/envs/mytestenv.\nmamba create -n mytestenv python=3.13 r-base=4.5\nconda-forge/linux-64                                        Using cache\nconda-forge/noarch                                          Using cache\nbioconda/linux-64                                           Using cache\nbioconda/noarch                                             Using cache\n\n\nTransaction\n\n  Prefix: /gpfs/gsfs12/users/$USER/mamba/envs/mytestenv\n\n  Updating specs:\n\n   - python=3.13\n   - r-base=4.5\n\n\nView environments\nYou should see both your personal environments and the shared CCBR environments listed.\nmamba info --envs",
    "crumbs": [
      "Home",
      "Conda Mamba",
      "Using the shared CCBR mamba installation on Biowulf"
    ]
  },
  {
    "objectID": "docs/conda-mamba/ccbr-mamba-biowulf.html#conclusion-support",
    "href": "docs/conda-mamba/ccbr-mamba-biowulf.html#conclusion-support",
    "title": "Using the shared CCBR mamba installation on Biowulf",
    "section": "Conclusion & Support",
    "text": "Conclusion & Support\nIf all of the above commands worked as expected, you‚Äôre all set up to use mamba on biowulf! üéâ\n\n\n\n\n\n\nTipGetting support\n\n\n\nIf you have any questions about conda or mamba generally, please refer to the mamba documentation\nIf you have any questions or run into problems while using the CCBR mamba installation on biowulf, please contact us at CCBR_Pipeliner@mail.nih.gov",
    "crumbs": [
      "Home",
      "Conda Mamba",
      "Using the shared CCBR mamba installation on Biowulf"
    ]
  },
  {
    "objectID": "docs/conda-mamba/ccbr-mamba-biowulf.html#optional-create-a-new-shared-ccbr-environment",
    "href": "docs/conda-mamba/ccbr-mamba-biowulf.html#optional-create-a-new-shared-ccbr-environment",
    "title": "Using the shared CCBR mamba installation on Biowulf",
    "section": "Optional: create a new shared CCBR environment",
    "text": "Optional: create a new shared CCBR environment\nIf you are a member of the CCBR_Pipeliner group on biowulf, you can create new environments in the CCBR mamba installation for others to use.\n\n\n\n\n\n\nImportant\n\n\n\nYou can run the groups command on biowulf to see which groups you are a member of.\ngroups\nCCBR CCBR_Pipeliner\nIf CCBR_Pipeliner is not listed, you will not be able to create a shared CCBR environment.\n\n\nCreate your new environment using the CCBR miniforge3 directory for the prefix. The environment name is the directory name after /data/CCBR_Pipeliner/db/PipeDB/miniforge3/envs/. For example, the following will create an environment called test-$USER ($USER will be replaced by your actual username).\nmamba create --prefix=/data/CCBR_Pipeliner/db/PipeDB/miniforge3/envs/test-$USER \\\n  python=3.12\nChange the permissions to ensure everyone can use the environment:\nchmod -R a+rX /data/CCBR_Pipeliner/db/PipeDB/miniforge3/envs/test-$USER\nThe environment files should now be readable and executable by all:\nls -alh /vf/users/CCBR_Pipeliner/db/PipeDB/miniforge3/envs/test-$USER\ntotal 0\ndrwxr-sr-x 2 $USER CCBR_Pipeliner 4.0K Oct 24 12:21 .\ndrwxr-sr-x 2 $USER CCBR_Pipeliner 4.0K Oct 24 12:21 ..\ndrwxr-sr-x 2 $USER CCBR_Pipeliner 4.0K Oct 24 12:22 bin\ndrwxr-sr-x 2 $USER CCBR_Pipeliner 4.0K Oct 24 12:21 compiler_compat\ndrwxr-sr-x 2 $USER CCBR_Pipeliner 4.0K Oct 24 12:22 conda-meta\ndrwxr-sr-x 2 $USER CCBR_Pipeliner 4.0K Oct 24 12:21 include\ndrwxr-sr-x 2 $USER CCBR_Pipeliner 4.0K Oct 24 12:21 lib\ndrwxr-sr-x 2 $USER CCBR_Pipeliner 4.0K Oct 24 12:21 man\ndrwxr-sr-x 2 $USER CCBR_Pipeliner 4.0K Oct 24 12:21 share\ndrwxr-sr-x 2 $USER CCBR_Pipeliner 4.0K Oct 24 12:21 ssl\ndrwxr-sr-x 2 $USER CCBR_Pipeliner 4.0K Oct 24 12:21 x86_64-conda-linux-gnu\nActivate the new environment and make sure it works as expected:\nmamba activate test-$USER\nwhich python\npython --version\nmamba deactivate\n/vf/users/CCBR_Pipeliner/db/PipeDB/miniforge3/envs/test-$USER/bin/python\nPython 3.12.12\nThat‚Äôs it! Now anyone on biowulf can activate your environment.\n\n\n\n\n\n\nNoteClean up\n\n\n\nIf you created an environment only for testing purposes, please delete it when you are finished so that the CCBR environments directory doesn‚Äôt get cluttered.\nmamba env remove -n test-$USER",
    "crumbs": [
      "Home",
      "Conda Mamba",
      "Using the shared CCBR mamba installation on Biowulf"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/2_howto_functions.html",
    "href": "docs/GitHub/guide/2_howto_functions.html",
    "title": "GitHub HowTo: Basic Functions",
    "section": "",
    "text": "GitHub HowTo: Basic Functions\nThe following outlines basic GitHub function to push and pull from your repository. It also includes information on creating a new branch and deleting a branch. These commands should be used in line with guidance on GitHub Repo Management.\n\nPushing local changes to remote\nCheck which files have been changed.\ngit status\nStage files that need to be pushed\ngit add &lt;thisfile&gt;\ngit add &lt;thatfile&gt;\nPush changes to branch named new_feature\ngit push origin new_feature\n\n\nPulling remote changes to local\nPull changes from branch new_feature into your branch old_feature\ngit checkout old_feature\ngit pull new_feature\nIf you have non-compatible changes in the old_feature branch, there are two options: 1) ignore local changes and pull remote anyways. This will delete the changes you‚Äôve made to your remote respository.\ngit reset --hard\ngit pull\n\ntemporarily stash changes away, pull and reapply changes after.\n\ngit stash\ngit pull\ngit stash pop\n\n\nCreating a new branch\nThis is a two step process.\n\nCreate the branch locally\n\ngit checkout -b &lt;newbranch&gt;\n\nPush the branch to remote\n\ngit push -u origin &lt;newbranch&gt;\nOR\ngit push -u origin HEAD\nThis is a shortcut to push the current branch to a branch of the same name on origin and track it so that you don‚Äôt need to specify origin HEAD in the future.\n\n\nDeleting branches\n\nLocally\ngit branch -d &lt;BranchName&gt;\n\n\non GitHub\ngit push origin --delete &lt;BranchName&gt;",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub HowTo: Basic Functions"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/3_basic_repo.html",
    "href": "docs/GitHub/guide/3_basic_repo.html",
    "title": "GitHub Basics: Repository",
    "section": "",
    "text": "All CCBR developed pipelines, and techdev efforts should be created under CCBR‚Äôs GitHub Org Account and all CCBR team members should have a minimal of read-only permission to the repository.\n\n\n\n\nAll CCBR developed pipelines should be created from the appropriate templates:\n\nNextflow Pipelines: https://github.com/CCBR/CCBR_NextflowTemplate\nSnakemake Pipelines: https://github.com/CCBR/CCBR_SnakemakeTemplate\nTechDev Projects: https://github.com/CCBR/CCBR_TechDevTemplate\n\n\n\nNote: The above templates are themselves under active development! As we continue building a multitude of analysis pipelines, we keep expanding on the list of ‚Äúcommonalities‚Äù between these analysis pipelines which need to be added to the template itself. Hence, templates are updated from time-to-time.\n\n\n\n\nTo create a new repository on Github using gh cli, you can run the following command on Biowulf after you update the new repository name (&lt;ADD NEW REPO NAME&gt;) and the repository description (&lt;ADD REPO DESCRIPTION&gt;) commands below.\nNaming Nomenclature: - All Repositories: Do not remove the CCBR/ leading the repository name, as this will correctly place the repository under the CCBR organization account. - CCBR Projects: These should be named with the CCBR#. IE CCBR/CCBR1155 - CCBR Pipelines: These should be descriptive of the pipelines main process; acronyms are encouraged. IE CCBR/CARLISLE - TechDev projects: These should be descriptive of the techDev project to be performed and should begin with techdev_. IE CCBR/techdev_peakcall_benchmarking\n\n\ngh repo create CCBR/&lt;ADD NEW REPO NAME&gt; \\\n--template CCBR/CCBR_NextflowTemplate \\\n--description \"&lt;ADD REPO DESCRIPTION&gt;\" \\\n--public \\\n--confirm\ngh repo create CCBR/&lt;ADD NEW REPO NAME&gt; \\\n--template CCBR/CCBR_SnakemakeTemplate \\\n--description \"&lt;ADD REPO DESCRIPTION&gt;\" \\\n--public \\\n--confirm\n\n\n\ngh repo create CCBR/techdev_&lt;ADD NEW REPO NAME&gt; \\\n--template CCBR/CCBR_TechDevTemplate \\\n--description \"&lt;ADD REPO DESCRIPTION&gt;\" \\\n--public \\\n--confirm\nOnce the repo is created, then you can clone a local copy of the new repository:\ngh repo clone CCBR/&lt;reponame&gt;.git\n\n\n\n\nIf you start from one of the above templates, you‚Äôll have these files already. However, if you‚Äôre updating an established repository, you may need to add some of these manually.\n\nCHANGELOG.md\nThe changelog file should be at the top level of your repo. One exception is if your repo is an R package, it should be called NEWS.md instead. You can see an example changelog of a pipeline in active development here.\nVERSION\nThe version file should be at the top level of your repo. If your repo is a Python package, it can be at at the package root instead, e.g.¬†src/pkg/VERSION. If your repo is an R package, the version should be inside the DESCRIPTION file instead. Every time a PR is opened, the PR owner should add a line to the changelog to describe any user-facing changes such as new features added, bugs fixed, documentation updates, or performance improvements.\nYou will also need a CLI command to print the version. The implementation will be different depending on your repo‚Äôs language and structure.\nCITATION.cff\nThe citation file must be at the top level of your repo. See template here.\nYou will also need a CLI command to print the citation. The implementation will be different depending on your repo‚Äôs language and structure.\nPre-commit config files\nPre-commit hooks provide an automated way to style code, draw attention to typos, and validate commit messages. Learn more about pre-commit here. These files can be customized depending on the needs of the repo. For example, you don‚Äôt need the hook for formatting R code if your repo does not and will never contain any R code.\n\n.pre-commit-config.yaml\n.pretterignore\n.prettierrc\n\n.github/PULL_REQUEST_TEMPLATE.md\nThe Pull Request template file helps developers and collaborators remember to write descriptive PR comments, link any relevant issues that the PR resolves, write unit tests, update the docs, and update the changelog. You can customize the Checklist in the template depending on the needs of the repo.\nIssue templates (optional)\nIssue templates help users know how they can best communicate with maintainers to report bugs and request new features. These are helpful but not required.\n\n.github/ISSUE_TEMPLATE/bug_report.yml\n.github/ISSUE_TEMPLATE/config.yml\n.github/ISSUE_TEMPLATE/feature_request.yml\n\nGitHub Actions\nGitHub Actions are automated workflows that run on GitHub‚Äôs servers to execute unit tests, render documentation, build docker containers, etc. Most Actions need to be customized for each repo. Learn more about them here.",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Basics: Repository"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/3_basic_repo.html#repository-location",
    "href": "docs/GitHub/guide/3_basic_repo.html#repository-location",
    "title": "GitHub Basics: Repository",
    "section": "",
    "text": "All CCBR developed pipelines, and techdev efforts should be created under CCBR‚Äôs GitHub Org Account and all CCBR team members should have a minimal of read-only permission to the repository.",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Basics: Repository"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/3_basic_repo.html#use-of-cookiecutter-templates",
    "href": "docs/GitHub/guide/3_basic_repo.html#use-of-cookiecutter-templates",
    "title": "GitHub Basics: Repository",
    "section": "",
    "text": "All CCBR developed pipelines should be created from the appropriate templates:\n\nNextflow Pipelines: https://github.com/CCBR/CCBR_NextflowTemplate\nSnakemake Pipelines: https://github.com/CCBR/CCBR_SnakemakeTemplate\nTechDev Projects: https://github.com/CCBR/CCBR_TechDevTemplate\n\n\n\nNote: The above templates are themselves under active development! As we continue building a multitude of analysis pipelines, we keep expanding on the list of ‚Äúcommonalities‚Äù between these analysis pipelines which need to be added to the template itself. Hence, templates are updated from time-to-time.",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Basics: Repository"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/3_basic_repo.html#creating-a-new-repository",
    "href": "docs/GitHub/guide/3_basic_repo.html#creating-a-new-repository",
    "title": "GitHub Basics: Repository",
    "section": "",
    "text": "To create a new repository on Github using gh cli, you can run the following command on Biowulf after you update the new repository name (&lt;ADD NEW REPO NAME&gt;) and the repository description (&lt;ADD REPO DESCRIPTION&gt;) commands below.\nNaming Nomenclature: - All Repositories: Do not remove the CCBR/ leading the repository name, as this will correctly place the repository under the CCBR organization account. - CCBR Projects: These should be named with the CCBR#. IE CCBR/CCBR1155 - CCBR Pipelines: These should be descriptive of the pipelines main process; acronyms are encouraged. IE CCBR/CARLISLE - TechDev projects: These should be descriptive of the techDev project to be performed and should begin with techdev_. IE CCBR/techdev_peakcall_benchmarking\n\n\ngh repo create CCBR/&lt;ADD NEW REPO NAME&gt; \\\n--template CCBR/CCBR_NextflowTemplate \\\n--description \"&lt;ADD REPO DESCRIPTION&gt;\" \\\n--public \\\n--confirm\ngh repo create CCBR/&lt;ADD NEW REPO NAME&gt; \\\n--template CCBR/CCBR_SnakemakeTemplate \\\n--description \"&lt;ADD REPO DESCRIPTION&gt;\" \\\n--public \\\n--confirm\n\n\n\ngh repo create CCBR/techdev_&lt;ADD NEW REPO NAME&gt; \\\n--template CCBR/CCBR_TechDevTemplate \\\n--description \"&lt;ADD REPO DESCRIPTION&gt;\" \\\n--public \\\n--confirm\nOnce the repo is created, then you can clone a local copy of the new repository:\ngh repo clone CCBR/&lt;reponame&gt;.git",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Basics: Repository"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/3_basic_repo.html#minimal-helper-components",
    "href": "docs/GitHub/guide/3_basic_repo.html#minimal-helper-components",
    "title": "GitHub Basics: Repository",
    "section": "",
    "text": "If you start from one of the above templates, you‚Äôll have these files already. However, if you‚Äôre updating an established repository, you may need to add some of these manually.\n\nCHANGELOG.md\nThe changelog file should be at the top level of your repo. One exception is if your repo is an R package, it should be called NEWS.md instead. You can see an example changelog of a pipeline in active development here.\nVERSION\nThe version file should be at the top level of your repo. If your repo is a Python package, it can be at at the package root instead, e.g.¬†src/pkg/VERSION. If your repo is an R package, the version should be inside the DESCRIPTION file instead. Every time a PR is opened, the PR owner should add a line to the changelog to describe any user-facing changes such as new features added, bugs fixed, documentation updates, or performance improvements.\nYou will also need a CLI command to print the version. The implementation will be different depending on your repo‚Äôs language and structure.\nCITATION.cff\nThe citation file must be at the top level of your repo. See template here.\nYou will also need a CLI command to print the citation. The implementation will be different depending on your repo‚Äôs language and structure.\nPre-commit config files\nPre-commit hooks provide an automated way to style code, draw attention to typos, and validate commit messages. Learn more about pre-commit here. These files can be customized depending on the needs of the repo. For example, you don‚Äôt need the hook for formatting R code if your repo does not and will never contain any R code.\n\n.pre-commit-config.yaml\n.pretterignore\n.prettierrc\n\n.github/PULL_REQUEST_TEMPLATE.md\nThe Pull Request template file helps developers and collaborators remember to write descriptive PR comments, link any relevant issues that the PR resolves, write unit tests, update the docs, and update the changelog. You can customize the Checklist in the template depending on the needs of the repo.\nIssue templates (optional)\nIssue templates help users know how they can best communicate with maintainers to report bugs and request new features. These are helpful but not required.\n\n.github/ISSUE_TEMPLATE/bug_report.yml\n.github/ISSUE_TEMPLATE/config.yml\n.github/ISSUE_TEMPLATE/feature_request.yml\n\nGitHub Actions\nGitHub Actions are automated workflows that run on GitHub‚Äôs servers to execute unit tests, render documentation, build docker containers, etc. Most Actions need to be customized for each repo. Learn more about them here.",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Basics: Repository"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/8_sop_techdev.html",
    "href": "docs/GitHub/guide/8_sop_techdev.html",
    "title": "GitHub Best Practices: TechDev projects",
    "section": "",
    "text": "Users should follow these links to learn more about setting up the repository, before reviewing the best practices below:\n\nPreparing your environment\nBasic Commands\nCreating your GitHub repo\nCreating your Documentation\nGitHub Actions",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Best Practices: TechDev projects"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/8_sop_techdev.html#github-best-practices",
    "href": "docs/GitHub/guide/8_sop_techdev.html#github-best-practices",
    "title": "GitHub Best Practices: TechDev projects",
    "section": "",
    "text": "Users should follow these links to learn more about setting up the repository, before reviewing the best practices below:\n\nPreparing your environment\nBasic Commands\nCreating your GitHub repo\nCreating your Documentation\nGitHub Actions",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Best Practices: TechDev projects"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/8_sop_techdev.html#techdev-documentation",
    "href": "docs/GitHub/guide/8_sop_techdev.html#techdev-documentation",
    "title": "GitHub Best Practices: TechDev projects",
    "section": "TechDev Documentation",
    "text": "TechDev Documentation\n\nAll pipelines should provide users with:\n\ndocumentation for usage\ntest data\nexpected outputs and reports\n\ntroubleshooting information\n\nMarkdown pages can be hosted directly within the repo using GH Pages. Mkdocs is the recommended tool to perform this action, however, other tools may be utilized. The templates (TechDev) template‚Äôs (written for mkdocs) provided have basic yaml markdown files provided for this use, and should be edited according to the pipelines function and user needs. Also, track blockers/hurdles using GitHub Issues.\n\nOverview\n\nInformation on the goal of the TechDev project.\n\nAnalysis\n2.1. Background - Provide any relevant background to the project to be completed. This might include information on: - problem that was encountered leading to this techdev - new feature or tool developed to be benchmarked - relevant biological or statistical information needed to perform this analysis\n2.2. Resources\n\nThis page should include any relevant tools that were used for testing. If these tools were loaded via Biowulf, include all version numbers. If they were installed locally, provide information on installation, source location, and any reference documents used.\n\n2.3. Test Data\n\nThis will include information on the test data included within the project. Species information, source information, and references should be included. Any manipulation performed on the source samples should also be included. Manifest information should also be outlined. Provide location to toy dataset or real dataset or both. It is best to park these datasets at a commonly accessible location on Biowulf and share that location here. Also, make the location read-only to prevent accidental edits by other users.\n\n2.4. Analysis Plan\n\nProvide a summary of the plan of action.\nIf benchmarking tools, include the dataset used, where the source material was obtained, and all relevant metadata.\nInclude the location of any relevant config / data files used in this analysis\nProvide information on the limits of the analysis - IE this was only tested on Biowulf, this was only performed in human samples, etc.\n\n2.5. Results\n\nWhat were the findings of this TechDev exercise? Include links to where intermediate files may be located. Include tables, plots, etc. This will serve as a permanent record of the TechDev efforts to be shared within the team and beyond.\n\n2.6 Conclusions\n\nProvide brief, concise conclusion drawn from the analysis, including any alterations being made to pipelines or analysis.\n\nContributions\n\nProvide any names that contributed to this work.",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Best Practices: TechDev projects"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/8_sop_techdev.html#techdev-repository-management",
    "href": "docs/GitHub/guide/8_sop_techdev.html#techdev-repository-management",
    "title": "GitHub Best Practices: TechDev projects",
    "section": "TechDev Repository Management",
    "text": "TechDev Repository Management\n\nSecurity settings\n\nTwo members of CCBR (creator and one manager) should be granted full administrative privileges to the repository to ensure the source code can be accessed by other members, as needed\nBoth the develop and master branch must be protected (IE have to have a PR to be changed)\n\n\n\nCCBR Branch Strategy\n\nBranch Naming\n\nAll repositories should follow the strategy outlined in the Creating your GitHub repo\n\n\n\nBranch Overview\n\nAll repositories should include a minimum of two branches at any time:\n\nmain (master)\ndev\n\nUtilization of these branches should follow the documentation below.\n\n\n\nBranch Strategy\n\nWe encourage the use of the Git Flow tools for some actions, available on Biowulf. Our current branching strategy is based off of the Git Flow strategy shown below :\n\n\n\nImage title\n\n\n\n\nMaster (named main or master)\n\nbranch that contains the current release / tagged version of the pipeline\nmerges from Dev branch or hotfix branch allowed\nmerges require actions_master_branch pass from GitHub actions. See GitHub actions #4 for more information testing requirements for merge\n\nDevelop (named dev or activeDev)\n\nbranch that contains current dev\nmerges from feature branch allowed\nmerges require actions_dev_branch pass from GitHub actions. See GitHub actions #3 for more information testing requirements for merge",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Best Practices: TechDev projects"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/8_sop_techdev.html#techdev-test-data",
    "href": "docs/GitHub/guide/8_sop_techdev.html#techdev-test-data",
    "title": "GitHub Best Practices: TechDev projects",
    "section": "TechDev Test Data",
    "text": "TechDev Test Data\n\nRequirements\n\nLocation of data\n\nTest data sets should be stored within a .test directory, as found in all templates.\n\nDocumentation\n\nReview information on the documentation page, which will provide basic information on test data used within the project/pipeline.\nA README file should be created under the .test directory, to include the following information:\n\nDate of implementation\nInformation on species (IE Homo Sapiens) and data type (IE RNA-Seq)\nInformation on the references to be used (IE hg38)\nMetadata manifests required by the pipeline\nThe source of the files\nLink to scripts used in created the partial test data\n\n\nChoosing a test data set\n\nTest data should come from a CCBR project or a a publicly available source. Care should be taken when choosing test data sets, to ensure that the data matches the goals of the techdev effort.",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Best Practices: TechDev projects"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/1_howto_setup.html",
    "href": "docs/GitHub/guide/1_howto_setup.html",
    "title": "GitHub Setup: Preparing the Environment",
    "section": "",
    "text": "The gh is installed on Biowulf at /data/CCBR_Pipeliner/db/PipeDB/bin/gh_1.7.0_linux_amd64/bin/gh. You can run the following lines to edit your ~/.bashrc file to add gh to your $PATH:\necho \"export PATH=$PATH:/data/CCBR_Pipeliner/db/PipeDB/bin/gh_1.7.0_linux_amd64/bin\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\nAlternatively, you can use the git commands provided through a Biowulf module\nmodule load git\n\n\n\nPersonal Access Token (PAT) is required to access GitHub (GH) without having to authenticate by other means (like password) every single time. You will need gh cli installed on your laptop or use /data/CCBR_Pipeliner/db/PipeDB/bin/gh_1.7.0_linux_amd64/bin/gh on Biowulf, as described above. You can create a PAT by going here. Then you can copy the PAT and save it into a file on Biowulf (say ~/gh_token). Next, you can run the following command to set everything up correctly on Biowulf (or your laptop)\ngh auth login --with-token &lt; ~/git_token\n\n\n\nIf you hate to re-enter (username and) password every time you push/pull to/from github (or mkdocs gh-deploy), then it is totally worthwhile to spend a couple minutes to set up SSH keys for auto-authentication. The instructions to do this are available here.",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Setup: Preparing the Environment"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/1_howto_setup.html#using-github-cli",
    "href": "docs/GitHub/guide/1_howto_setup.html#using-github-cli",
    "title": "GitHub Setup: Preparing the Environment",
    "section": "",
    "text": "The gh is installed on Biowulf at /data/CCBR_Pipeliner/db/PipeDB/bin/gh_1.7.0_linux_amd64/bin/gh. You can run the following lines to edit your ~/.bashrc file to add gh to your $PATH:\necho \"export PATH=$PATH:/data/CCBR_Pipeliner/db/PipeDB/bin/gh_1.7.0_linux_amd64/bin\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\nAlternatively, you can use the git commands provided through a Biowulf module\nmodule load git",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Setup: Preparing the Environment"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/1_howto_setup.html#creating-pat-for-gh",
    "href": "docs/GitHub/guide/1_howto_setup.html#creating-pat-for-gh",
    "title": "GitHub Setup: Preparing the Environment",
    "section": "",
    "text": "Personal Access Token (PAT) is required to access GitHub (GH) without having to authenticate by other means (like password) every single time. You will need gh cli installed on your laptop or use /data/CCBR_Pipeliner/db/PipeDB/bin/gh_1.7.0_linux_amd64/bin/gh on Biowulf, as described above. You can create a PAT by going here. Then you can copy the PAT and save it into a file on Biowulf (say ~/gh_token). Next, you can run the following command to set everything up correctly on Biowulf (or your laptop)\ngh auth login --with-token &lt; ~/git_token",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Setup: Preparing the Environment"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/1_howto_setup.html#password-less-login",
    "href": "docs/GitHub/guide/1_howto_setup.html#password-less-login",
    "title": "GitHub Setup: Preparing the Environment",
    "section": "",
    "text": "If you hate to re-enter (username and) password every time you push/pull to/from github (or mkdocs gh-deploy), then it is totally worthwhile to spend a couple minutes to set up SSH keys for auto-authentication. The instructions to do this are available here.",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Setup: Preparing the Environment"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/7_sop_projpipes.html",
    "href": "docs/GitHub/guide/7_sop_projpipes.html",
    "title": "GitHub Best Practices: Projects and Pipelines",
    "section": "",
    "text": "Users should follow these links to learn more about setting up the repository, before reviewing the best practices below:\n\nPreparing your environment\nBasic Commands\nCreating your GitHub repo\nCreating your Documentation\nGitHub Actions\n\n\n\n\n\nAll pipelines should provide users with documentation for usage, test data, expected outputs, and troubleshooting information. Mkdocs is the recommended tool to perform this action, however, other tools may be utilized. The template‚Äôs (NextFlow, Snakemake) were written for mkdocs, and provide basic yaml markdown files provided for this use. They should be edited according to the pipelines function and user needs. Examples of the requirements for each page are provided in the templates.\n\nBackground\n\nInformation on who the pipeline was developed for, and a statement if it‚Äôs only been tested on Biowulf.\nAlso include a workflow image to summarize the pipeline.\n\nGetting Started\n\nThis should set the stage for all of the pipeline requirements. This should include the following pages:\n\nIntroduction\nSetup Dependencies\nLogin to the cluster\nLoad an interactive session\n\n\nPreparing Files\n\nThis should include the following pages:\n\nConfigs\n\nCluster Config\nTools Config\nConfig YAML\n\nUser Parameters\nReferences\n\n\nPreparing Manifests\n\nSamples Manifest\n\n\n\nRunning the Pipeline\n\nThis should include all information about the various run commands provided within the pipeline. This should include the following pages:\n\nPipeline Overview\nCommands explained\nTypical Workflow\n\n\nExpected Output\n\nThis should include all pertinent information about output files, including extensions that differentiate files.\n\nRunning Test Data\n\nThis should walk the user through the steps of running the pipeline using test data. This should include the following pages:\n\nGetting Started\nAbout the test data\nSubmit the test data\nReview outputs\n\n\n\n\n\n\n\n\n\n\nAdminTeam should have a minimum of maintain role to the repo\nBoth the develop and master branch must be protected (IE have to have a PR to be changed)\nRepo visibility should be set to Private or Internal and made Public only when required.\n\n\n\n\n\n\n\nAll repositories should follow the strategy outlined in the Creating your GitHub repo\n\n\n\n\n\nAll repositories should include a minimum of two branches at any time:\n\nmain ( or master )\ndev\n\nAdditional branches should be created as needed. These would include feature branches, developed using individual, feature specific addition and hotfix branches, developed using individual, bug specific fixes.\nUtilization of these branches should follow the documentation below.\n\n\n\n\n\nWe encourage the use of the Git Flow tools for some actions, available on Biowulf (module load gitflow). Our current branching strategy is based off of the Git Flow strategy shown below :\n\n ref:https://nvie.com/posts/a-successful-git-branching-model/\n\nMaster (named main or master)\n\nbranch that contains the current release / tagged version of the pipeline\nmerges from Dev branch or hotfix branch allowed\nmerges require actions_master_branch pass from GitHub actions. See GitHub actions #4 for more information testing requirements for merge\n\nDevelop (named dev or activeDev)\n\nbranch that contains current dev\nmerges from feature branch allowed\nmerges require actions_dev_branch pass from GitHub actions. See GitHub actions #3 for more information testing requirements for merge\n\nFeature (named feature/unique_feature_name)\n\nbranch to develop new features that branches off the develop branch\nrecommended usages of git flow feature start unique_feature_name followed by git flow feature publish unique_feature_name\nno merges into this branch are expected\n\nHotfix (named unique_hotfix_name)\n\nbranches arise from a bug that has been discovered and must be resolved; it enables developers to keep working on their own changes on the develop branch while the bug is being fixed\nrecommended usage of git flow hotfix start unique_hotfix_name\nno merges into this branch are expected\n\n\n\nüí° Note\nWhile the git flow feature start command is recommended for feature branch creation, the git flow feature finish command is not. Using the finish command will automatically merge the feature branch into the dev branch without any testing and regardless of any divergence that may have occurred during feature development.\n\n\n\n\n\nAssuming that you have already cloned the repo and initiated git flow with git flow init\nCreate a new feature and publish it git flow feature start unique_feature_name and then git flow feature publish unique_feature_name. The unique_feature_name will be created from the develop branch.\nCode normally, make frequent commits, push frequently to remote. These commits are added to the unique_feature_name branch.\nWhen you are ready to add your feature enhancements back to the develop branch, you may have to make sure that the develop branch has not marched forward while you were working on the unique_feature_name feature (This is possible as other may have added their PRs in the interim). If it has, then you may have to pull in changes made to develop branch into your unique_feature_name feature branch. This can be achieved using the GitHub web interface‚Ä¶ merge develop &lt;‚Äì unique_feature_name. Resolve conflicts if any.\nRetest your unique_feature_name branch.\nNow send in a pull request using the GitHub web interface to pull your unique_feature_name into develop branch.\nOccasionally, we may create a new release branch from the develop branch and perform thorough E2E testing with fixes. Once, everything is working as expected, we pull the release branch back into develop and also push it to main with a version tag.\n\n\n\n\n\nThe following format of versioning should be followed:\nvX.Y.Z\nThe following rules should be applies when determining the version release:\n\nX is major; non-backward compatible (dependent on the amount of changes; from dev)\nY is minor; backwards compatible (dependent on the amount of changes; from dev)\nZ is patches; backwards compatible (bugs; hot fixes)\n\nOther notes:\n\nX,Y,Z must be numeric only\nAll updates to the main (master) branch must be tagged and versioned using the parameters above\nUpdates to the dev branch can be tagged, but should not be versioned\nIf the pipeline is available locally (IE on Biowulf), version changes should be added for use\n\n\n\n\n\n\nThe following information is meant to outline test_data requirements for all pipelines, however, should be altered to fit the needs of the specific pipeline or project developed.\n\n\n\nLocation of data\n\nTest data sets should be stored within a .test directory, as found in all templates.\n\nDocumentation\n\nReview information on the documentation page, which will provide basic information on test data used within the project/pipeline.\nA README file should be created under the .test directory, to include the following information:\n\nDate of implementation\nInformation on species (IE Homo Sapiens) and data type (IE RNA-Seq)\nInformation on the references to be used (IE hg38)\nMetadata manifests required by the pipeline\nThe source of the files\nLink to scripts used in created the partial test data\n\n\nChoosing a test data set\n\nAt a minimum three test sets are recommended to be available:\n\nShould include sub-sampled inputs, to test the pipelines functionality, and to be used as the tutorial test set.\nShould include full-sample inputs, of high quality, to test the robustness of the pipelines resources\nShould include full-sample inputs, of expected project-level quality, to test the robustness of the pipelines error handling\n\nTest data should come from a CCBR project or a publicly available source. Care should be taken when choosing test data sets, to ensure that the robustness of the pipeline will be tested, as well as the ability of the pipeline to handle both high and low quality data. Multiple test sets may need to be created to meet these goals. On BIOWULF, these test data files can be stored under /data/CCBR_Pipeliner/testdata for easy access by all users of the pipeline(s).",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Best Practices: Projects and Pipelines"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/7_sop_projpipes.html#pipeline-documentation",
    "href": "docs/GitHub/guide/7_sop_projpipes.html#pipeline-documentation",
    "title": "GitHub Best Practices: Projects and Pipelines",
    "section": "",
    "text": "All pipelines should provide users with documentation for usage, test data, expected outputs, and troubleshooting information. Mkdocs is the recommended tool to perform this action, however, other tools may be utilized. The template‚Äôs (NextFlow, Snakemake) were written for mkdocs, and provide basic yaml markdown files provided for this use. They should be edited according to the pipelines function and user needs. Examples of the requirements for each page are provided in the templates.\n\nBackground\n\nInformation on who the pipeline was developed for, and a statement if it‚Äôs only been tested on Biowulf.\nAlso include a workflow image to summarize the pipeline.\n\nGetting Started\n\nThis should set the stage for all of the pipeline requirements. This should include the following pages:\n\nIntroduction\nSetup Dependencies\nLogin to the cluster\nLoad an interactive session\n\n\nPreparing Files\n\nThis should include the following pages:\n\nConfigs\n\nCluster Config\nTools Config\nConfig YAML\n\nUser Parameters\nReferences\n\n\nPreparing Manifests\n\nSamples Manifest\n\n\n\nRunning the Pipeline\n\nThis should include all information about the various run commands provided within the pipeline. This should include the following pages:\n\nPipeline Overview\nCommands explained\nTypical Workflow\n\n\nExpected Output\n\nThis should include all pertinent information about output files, including extensions that differentiate files.\n\nRunning Test Data\n\nThis should walk the user through the steps of running the pipeline using test data. This should include the following pages:\n\nGetting Started\nAbout the test data\nSubmit the test data\nReview outputs",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Best Practices: Projects and Pipelines"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/7_sop_projpipes.html#repository-management",
    "href": "docs/GitHub/guide/7_sop_projpipes.html#repository-management",
    "title": "GitHub Best Practices: Projects and Pipelines",
    "section": "",
    "text": "AdminTeam should have a minimum of maintain role to the repo\nBoth the develop and master branch must be protected (IE have to have a PR to be changed)\nRepo visibility should be set to Private or Internal and made Public only when required.\n\n\n\n\n\n\n\nAll repositories should follow the strategy outlined in the Creating your GitHub repo\n\n\n\n\n\nAll repositories should include a minimum of two branches at any time:\n\nmain ( or master )\ndev\n\nAdditional branches should be created as needed. These would include feature branches, developed using individual, feature specific addition and hotfix branches, developed using individual, bug specific fixes.\nUtilization of these branches should follow the documentation below.\n\n\n\n\n\nWe encourage the use of the Git Flow tools for some actions, available on Biowulf (module load gitflow). Our current branching strategy is based off of the Git Flow strategy shown below :\n\n ref:https://nvie.com/posts/a-successful-git-branching-model/\n\nMaster (named main or master)\n\nbranch that contains the current release / tagged version of the pipeline\nmerges from Dev branch or hotfix branch allowed\nmerges require actions_master_branch pass from GitHub actions. See GitHub actions #4 for more information testing requirements for merge\n\nDevelop (named dev or activeDev)\n\nbranch that contains current dev\nmerges from feature branch allowed\nmerges require actions_dev_branch pass from GitHub actions. See GitHub actions #3 for more information testing requirements for merge\n\nFeature (named feature/unique_feature_name)\n\nbranch to develop new features that branches off the develop branch\nrecommended usages of git flow feature start unique_feature_name followed by git flow feature publish unique_feature_name\nno merges into this branch are expected\n\nHotfix (named unique_hotfix_name)\n\nbranches arise from a bug that has been discovered and must be resolved; it enables developers to keep working on their own changes on the develop branch while the bug is being fixed\nrecommended usage of git flow hotfix start unique_hotfix_name\nno merges into this branch are expected\n\n\n\nüí° Note\nWhile the git flow feature start command is recommended for feature branch creation, the git flow feature finish command is not. Using the finish command will automatically merge the feature branch into the dev branch without any testing and regardless of any divergence that may have occurred during feature development.\n\n\n\n\n\nAssuming that you have already cloned the repo and initiated git flow with git flow init\nCreate a new feature and publish it git flow feature start unique_feature_name and then git flow feature publish unique_feature_name. The unique_feature_name will be created from the develop branch.\nCode normally, make frequent commits, push frequently to remote. These commits are added to the unique_feature_name branch.\nWhen you are ready to add your feature enhancements back to the develop branch, you may have to make sure that the develop branch has not marched forward while you were working on the unique_feature_name feature (This is possible as other may have added their PRs in the interim). If it has, then you may have to pull in changes made to develop branch into your unique_feature_name feature branch. This can be achieved using the GitHub web interface‚Ä¶ merge develop &lt;‚Äì unique_feature_name. Resolve conflicts if any.\nRetest your unique_feature_name branch.\nNow send in a pull request using the GitHub web interface to pull your unique_feature_name into develop branch.\nOccasionally, we may create a new release branch from the develop branch and perform thorough E2E testing with fixes. Once, everything is working as expected, we pull the release branch back into develop and also push it to main with a version tag.\n\n\n\n\n\nThe following format of versioning should be followed:\nvX.Y.Z\nThe following rules should be applies when determining the version release:\n\nX is major; non-backward compatible (dependent on the amount of changes; from dev)\nY is minor; backwards compatible (dependent on the amount of changes; from dev)\nZ is patches; backwards compatible (bugs; hot fixes)\n\nOther notes:\n\nX,Y,Z must be numeric only\nAll updates to the main (master) branch must be tagged and versioned using the parameters above\nUpdates to the dev branch can be tagged, but should not be versioned\nIf the pipeline is available locally (IE on Biowulf), version changes should be added for use\n\n\n\n\n\n\nThe following information is meant to outline test_data requirements for all pipelines, however, should be altered to fit the needs of the specific pipeline or project developed.\n\n\n\nLocation of data\n\nTest data sets should be stored within a .test directory, as found in all templates.\n\nDocumentation\n\nReview information on the documentation page, which will provide basic information on test data used within the project/pipeline.\nA README file should be created under the .test directory, to include the following information:\n\nDate of implementation\nInformation on species (IE Homo Sapiens) and data type (IE RNA-Seq)\nInformation on the references to be used (IE hg38)\nMetadata manifests required by the pipeline\nThe source of the files\nLink to scripts used in created the partial test data\n\n\nChoosing a test data set\n\nAt a minimum three test sets are recommended to be available:\n\nShould include sub-sampled inputs, to test the pipelines functionality, and to be used as the tutorial test set.\nShould include full-sample inputs, of high quality, to test the robustness of the pipelines resources\nShould include full-sample inputs, of expected project-level quality, to test the robustness of the pipelines error handling\n\nTest data should come from a CCBR project or a publicly available source. Care should be taken when choosing test data sets, to ensure that the robustness of the pipeline will be tested, as well as the ability of the pipeline to handle both high and low quality data. Multiple test sets may need to be created to meet these goals. On BIOWULF, these test data files can be stored under /data/CCBR_Pipeliner/testdata for easy access by all users of the pipeline(s).",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Best Practices: Projects and Pipelines"
    ]
  },
  {
    "objectID": "docs/containers/build-docker-google-cloud.html",
    "href": "docs/containers/build-docker-google-cloud.html",
    "title": "Build docker containers interactively with Google Cloud Shell",
    "section": "",
    "text": "TipRecording\n\n\n\nCCBR members can view the recording of this tutorial:\n\nCCBR Weekly Presentations 2025-08-26\nAs of this writing, Docker Desktop is not approved for use on NCI laptops. If you want to build and push docker containers using the docker CLI, you can use the Google Cloud Shell: https://console.cloud.google.com/\nYou can create an account with your @nih.gov email. Certain aspects of Google Cloud cost money, but the free tier offers more than enough usage for building containers with the Google Cloud Shell.",
    "crumbs": [
      "Home",
      "Containers",
      "Build docker containers interactively with Google Cloud Shell"
    ]
  },
  {
    "objectID": "docs/containers/build-docker-google-cloud.html#create-a-dockerfile",
    "href": "docs/containers/build-docker-google-cloud.html#create-a-dockerfile",
    "title": "Build docker containers interactively with Google Cloud Shell",
    "section": "Create a Dockerfile",
    "text": "Create a Dockerfile\nFor this tutorial, we‚Äôll create a minimal Dockerfile with these contents:\nFROM python:3.11\n\nRUN mkdir -p /data2/\nRUN echo 'hello world' &gt; /data2/file.txt\n\nUpload\nIf you already have a Dockerfile written, you can upload it to Cloud Shell.\n\n\n\nIn the file explorer tab, right-click and select ‚ÄúUpload‚Äù.\n\n\n\n\nWrite\nCreate a new text file:\n\n\n\nClick File &gt; New Text File and write the contents of your Dockerfile.\n\n\nSave it with the name ‚ÄúDockerfile‚Äù.",
    "crumbs": [
      "Home",
      "Containers",
      "Build docker containers interactively with Google Cloud Shell"
    ]
  },
  {
    "objectID": "docs/containers/build-docker-google-cloud.html#build-the-container",
    "href": "docs/containers/build-docker-google-cloud.html#build-the-container",
    "title": "Build docker containers interactively with Google Cloud Shell",
    "section": "Build the container",
    "text": "Build the container\ndocker build -t minimal:v1 -f ./Dockerfile .\n\n-t, --tag - Name and optional tag (format: ‚Äúname:tag‚Äù)\n-f, --file - Name of the Dockerfile (default: ‚ÄúPATH/Dockerfile‚Äù)\n\nRun docker build --help for more options.\n\n\n\nThe build completed successfully.",
    "crumbs": [
      "Home",
      "Containers",
      "Build docker containers interactively with Google Cloud Shell"
    ]
  },
  {
    "objectID": "docs/containers/build-docker-google-cloud.html#test-the-container-image",
    "href": "docs/containers/build-docker-google-cloud.html#test-the-container-image",
    "title": "Build docker containers interactively with Google Cloud Shell",
    "section": "Test the container image",
    "text": "Test the container image\nRun the container interactively with the bash shell:\ndocker run -it minimal:v1 bash\nRun any commands you wish inside the container to test that everything you need was installed correctly.\ncat /data2/file.txt\nwhich python\npython --version\n\n\n\nThis container should have python installed and should have the ‚Äúhello world‚Äù file.\n\n\nClose the container with the keyboard shortcut Control-DControl-D or type exit and press enter.\nIf the container doesn‚Äôt have the software you expected, something went wrong during the build process. Try editing the Dockerfile and re-building the container.",
    "crumbs": [
      "Home",
      "Containers",
      "Build docker containers interactively with Google Cloud Shell"
    ]
  },
  {
    "objectID": "docs/containers/build-docker-google-cloud.html#push-the-container-to-dockerhub",
    "href": "docs/containers/build-docker-google-cloud.html#push-the-container-to-dockerhub",
    "title": "Build docker containers interactively with Google Cloud Shell",
    "section": "Push the container to dockerhub",
    "text": "Push the container to dockerhub\nIf your container was built successfully, you can push it to a container registry such as dockerhub so you can pull it for use on other platforms.\n\nLog in to dockerhub\ndocker login\n\n\n\nOpen your web browser and type in the confirmation code. This step goes very smoothly if you‚Äôre already logged in to dockerhub in your web browser.\n\n\nIf it worked, you‚Äôll see ‚ÄúLogin Succeeded‚Äù in the Terminal.\n\n\nTag & push the container image\nAdd a tag to the image with your username or the namespace on dockerhub (e.g. nciccbr) that you want to push the container to.\nChange NAMESPACE in the following command to your username or namespace.\ndocker image tag minimal:v1 NAMESPACE/minimal:v1\nPush the container image to dockerhub:\ndocker push NAMESPACE/minimal:v1\n\n\n\nIn these examples, the namespace is kellysovacool (my username)\n\n\n\n\n\nIf the push succeeded, you‚Äôll see it in your list of repositories on dockerhub\n\n\n\n\n\nOn the repository page, you can see the list of tags that have been pushed and optionally add a description, category, overview, etc.",
    "crumbs": [
      "Home",
      "Containers",
      "Build docker containers interactively with Google Cloud Shell"
    ]
  },
  {
    "objectID": "docs/containers/build-docker-google-cloud.html#pull-any-public-container",
    "href": "docs/containers/build-docker-google-cloud.html#pull-any-public-container",
    "title": "Build docker containers interactively with Google Cloud Shell",
    "section": "Pull any public container",
    "text": "Pull any public container\nYou can pull any public container from dockerhub (including the one you just pushed).\nHere is the base image that we use for building most other CCBR containers. You can see details about it on dockerhub:\n\n\n\n&lt;https://hub.docker.com/repository/docker/nciccbr/ccbr_ubuntu_22.04/general&gt;\n\n\n\n\n\nCCBR base image overview with list of installed tools\n\n\nHere‚Äôs an example of pulling this container from the CCBR dockerhub account:\ndocker pull nciccbr/ccbr_ubuntu_22.04:v4\nThe namespace is nciccbr, the container name is ccbr_ubuntu_22.04, and the version tag is v4.\n\nRun the container interactively:\ndocker run -it nciccbr/ccbr_ubuntu_22.04:v4 bash",
    "crumbs": [
      "Home",
      "Containers",
      "Build docker containers interactively with Google Cloud Shell"
    ]
  },
  {
    "objectID": "docs/containers/build-docker-google-cloud.html#docker-docs",
    "href": "docs/containers/build-docker-google-cloud.html#docker-docs",
    "title": "Build docker containers interactively with Google Cloud Shell",
    "section": "Docker Docs",
    "text": "Docker Docs\nView the Docker getting started tutorial for more detailed information: https://docs.docker.com/get-started/introduction/build-and-push-first-image/",
    "crumbs": [
      "Home",
      "Containers",
      "Build docker containers interactively with Google Cloud Shell"
    ]
  },
  {
    "objectID": "docs/R-package/create-from-conda/common_issues.html",
    "href": "docs/R-package/create-from-conda/common_issues.html",
    "title": "common issues",
    "section": "",
    "text": "An important consideration for Conda builds is the list of dependencies, specified versions, and compatibility with each of the other dependencies.\nIf the meta.yaml and DESCRIPTION file specify specific package versions, Conda‚Äôs ability to resolve the Conda environment also becomes more limited.\nFor example, if the Conda package we are building has the following requirements:\nDependency A version == 1.0\nDependency B version &gt;= 2.5\nAnd the Dependencies located in our Conda channel have the following dependencies:\nDependency A version 1.0\n  - Dependency C version == 0.5\n\nDependency A version 1.2\n- Dependency C version &gt;= 0.7\n\nDependency B version 2.7\n  - Dependency C version &gt;= 0.7\nAs you can see, the Conda build will not be able to resolve the environment because Dependency A version 1.0 needs an old version of Dependency C, while Dependency B version 2.7 needs a newer version.\nIn this case, if we changed our package‚Äôs DESCRIPTION and meta.yaml file to be:\nDependency A version &gt;= 1.0\nDependency B version &gt;= 2.5\nThe conda build will be able to resolve. This is a simplified version of a what are more often complex dependency structures, but it is an important concept in conda package building that will inevitably arise as a package‚Äôs dependencies become more specific.\nTo check on the versions of packages that are available in a Conda channel, use the command:\nconda search $dependency\nReplace $dependency with the name of package you would like to investigate. There are more optional commands for this function which can be found here\nTo check the dependencies of packages that exist in your Conda cache, go to the folder specified for your conda cache (that we specified earlier). In case you need to find that path you can use:\nconda info\nHere there will be a folder for each of the packages that has been used in a conda build (including the dependencies). In each folder is another folder called ‚Äúinfo‚Äù and a file called ‚Äúindex.json‚Äù that lists information, such as depends for the package.\nHere is an example:\ncat /rstudio-files/ccbr-data/users/Ned/conda-cache/r-ggplot2-3.3.6-r41hc72bb7e_1/info/index.json\n\n{\n \"arch\": null,\n \"build\": \"r41hc72bb7e_1\",\n \"build_number\": 1,\n \"depends\": [\n   \"r-base &gt;=4.1,&lt;4.2.0a0\",\n   \"r-digest\",\n   \"r-glue\",\n   \"r-gtable &gt;=0.1.1\",\n   \"r-isoband\",\n   \"r-mass\",\n   \"r-mgcv\",\n   \"r-rlang &gt;=0.3.0\",\n   \"r-scales &gt;=0.5.0\",\n   \"r-tibble\",\n   \"r-withr &gt;=2.0.0\"\n ],\n \"license\": \"GPL-2.0-only\",\n \"license_family\": \"GPL2\",\n \"name\": \"r-ggplot2\",\n \"noarch\": \"generic\",\n \"platform\": null,\n \"subdir\": \"noarch\",\n \"timestamp\": 1665515494942,\n \"version\": \"3.3.6\"\n}\nIf you would like to specify an exact package to use in a conda channel for your conda build, specify the build string in your ‚Äúmeta.yaml‚Äù file. In the above example for ggplot version 3.3.6, the build string is listed in the folder name for package as well as in the index.json file for ‚Äúbuild:‚Äùr41hc72bb7e_1‚Äù.",
    "crumbs": [
      "Home",
      "R Package",
      "Create from Conda",
      "common issues"
    ]
  },
  {
    "objectID": "docs/R-package/create-from-conda/common_issues.html#package-dependency-issues",
    "href": "docs/R-package/create-from-conda/common_issues.html#package-dependency-issues",
    "title": "common issues",
    "section": "",
    "text": "An important consideration for Conda builds is the list of dependencies, specified versions, and compatibility with each of the other dependencies.\nIf the meta.yaml and DESCRIPTION file specify specific package versions, Conda‚Äôs ability to resolve the Conda environment also becomes more limited.\nFor example, if the Conda package we are building has the following requirements:\nDependency A version == 1.0\nDependency B version &gt;= 2.5\nAnd the Dependencies located in our Conda channel have the following dependencies:\nDependency A version 1.0\n  - Dependency C version == 0.5\n\nDependency A version 1.2\n- Dependency C version &gt;= 0.7\n\nDependency B version 2.7\n  - Dependency C version &gt;= 0.7\nAs you can see, the Conda build will not be able to resolve the environment because Dependency A version 1.0 needs an old version of Dependency C, while Dependency B version 2.7 needs a newer version.\nIn this case, if we changed our package‚Äôs DESCRIPTION and meta.yaml file to be:\nDependency A version &gt;= 1.0\nDependency B version &gt;= 2.5\nThe conda build will be able to resolve. This is a simplified version of a what are more often complex dependency structures, but it is an important concept in conda package building that will inevitably arise as a package‚Äôs dependencies become more specific.\nTo check on the versions of packages that are available in a Conda channel, use the command:\nconda search $dependency\nReplace $dependency with the name of package you would like to investigate. There are more optional commands for this function which can be found here\nTo check the dependencies of packages that exist in your Conda cache, go to the folder specified for your conda cache (that we specified earlier). In case you need to find that path you can use:\nconda info\nHere there will be a folder for each of the packages that has been used in a conda build (including the dependencies). In each folder is another folder called ‚Äúinfo‚Äù and a file called ‚Äúindex.json‚Äù that lists information, such as depends for the package.\nHere is an example:\ncat /rstudio-files/ccbr-data/users/Ned/conda-cache/r-ggplot2-3.3.6-r41hc72bb7e_1/info/index.json\n\n{\n \"arch\": null,\n \"build\": \"r41hc72bb7e_1\",\n \"build_number\": 1,\n \"depends\": [\n   \"r-base &gt;=4.1,&lt;4.2.0a0\",\n   \"r-digest\",\n   \"r-glue\",\n   \"r-gtable &gt;=0.1.1\",\n   \"r-isoband\",\n   \"r-mass\",\n   \"r-mgcv\",\n   \"r-rlang &gt;=0.3.0\",\n   \"r-scales &gt;=0.5.0\",\n   \"r-tibble\",\n   \"r-withr &gt;=2.0.0\"\n ],\n \"license\": \"GPL-2.0-only\",\n \"license_family\": \"GPL2\",\n \"name\": \"r-ggplot2\",\n \"noarch\": \"generic\",\n \"platform\": null,\n \"subdir\": \"noarch\",\n \"timestamp\": 1665515494942,\n \"version\": \"3.3.6\"\n}\nIf you would like to specify an exact package to use in a conda channel for your conda build, specify the build string in your ‚Äúmeta.yaml‚Äù file. In the above example for ggplot version 3.3.6, the build string is listed in the folder name for package as well as in the index.json file for ‚Äúbuild:‚Äùr41hc72bb7e_1‚Äù.",
    "crumbs": [
      "Home",
      "R Package",
      "Create from Conda",
      "common issues"
    ]
  },
  {
    "objectID": "docs/R-package/create-from-conda/1_build_pkg.html",
    "href": "docs/R-package/create-from-conda/1_build_pkg.html",
    "title": "Creating the package",
    "section": "",
    "text": "To create a package with Conda, you first need to make a new release and tag in the github repo of the R package you would like to create into a Conda Package.\nFor more information on best practices in R package creation, review this documentation.\nBefore creating the release on github, please check for proper dependencies listed in the files NAMESPACE and DESCRIPTION.\nThese files should have the same list of dependencies, and the version numbers for dependencies can be specified in the DESCRIPTION file. The DESCRIPTION file must be edited manually, while the NAMESPACE file should not be edited manually, but rather created automatically using the document() function.\nThe DESCRIPTION file must also be correctly formatted. For more information, see the following website.",
    "crumbs": [
      "Home",
      "R Package",
      "Create from Conda",
      "Creating the package"
    ]
  },
  {
    "objectID": "docs/R-package/create-from-conda/1_build_pkg.html#overview",
    "href": "docs/R-package/create-from-conda/1_build_pkg.html#overview",
    "title": "Creating the package",
    "section": "",
    "text": "To create a package with Conda, you first need to make a new release and tag in the github repo of the R package you would like to create into a Conda Package.\nFor more information on best practices in R package creation, review this documentation.\nBefore creating the release on github, please check for proper dependencies listed in the files NAMESPACE and DESCRIPTION.\nThese files should have the same list of dependencies, and the version numbers for dependencies can be specified in the DESCRIPTION file. The DESCRIPTION file must be edited manually, while the NAMESPACE file should not be edited manually, but rather created automatically using the document() function.\nThe DESCRIPTION file must also be correctly formatted. For more information, see the following website.",
    "crumbs": [
      "Home",
      "R Package",
      "Create from Conda",
      "Creating the package"
    ]
  },
  {
    "objectID": "docs/R-package/create-from-conda/1_build_pkg.html#download-the-r-package-release-from-github",
    "href": "docs/R-package/create-from-conda/1_build_pkg.html#download-the-r-package-release-from-github",
    "title": "Creating the package",
    "section": "Download the R package release from Github",
    "text": "Download the R package release from Github\nTo download the most recent release from the most recent tag on Github, activate Conda then use Conda skeleton to pull the correct URL. In the example below, replace $githubURL with the URL to your R package‚Äôs github repo.\nconda activate\n\nconda skeleton cran $githubURL\nA folder is then created for the downloaded release. Ror example running the following:\nconda skeleton cran https://github.com/NIDAP-Community/DSPWorkflow\nCreates the folder\nr-dspworkflow\nWithin this newly created folder is a file named meta.yaml. You will need to edit this file to include the channels and edit any information on the the package version number or dependency version numbers.\nHere is an example of the top of the meta.yaml file with the channels section added:\n{% set version = '0.9.5.2' %}\n\n{% set posix = 'm2-' if win else '' %}\n{% set native = 'm2w64-' if win else '' %}\n\npackage:\n  name: r-dspworkflow\n  version: {{ version|replace(\"-\", \"_\") }}\n\nchannels:\n  - conda-forge\n  - bioconda\n  - default\n  - file://rstudio-files/RH/ccbr-projects/Conda_package_tutorial/local_channel/channel\n\nsource:\n\n  git_url: https://github.com/NIDAP-Community/DSPWorkflow\n  git_tag: 0.9.5\n\nbuild:\n  merge_build_host: True  # [win]\n  # If this is a new build for the same version, increment the build number.\n  number: 0\n  # no skip\n\n  # This is required to make R link correctly on Linux.\n  rpaths:\n    - lib/R/lib/\n    - lib/\n\n    # Suggests: testthat (== 3.1.4)\nrequirements:\n  build:\n    - {{ posix }}filesystem        # [win]\n    - {{ posix }}git\n    - {{ posix }}zip               # [win]\nHere is an example of the sections for specifying dependency versions from the meta.yaml file:\n  host:\n    - r-base =4.1.3=h2f963a2_5\n    - bioconductor-biobase =2.54.0=r41hc0cfd56_2\n    - bioconductor-biocgenerics =0.40.0=r41hdfd78af_0\n    - bioconductor-geomxtools =3.1.1=r41hdfd78af_0\n    - bioconductor-nanostringnctools =1.2.0\n    - bioconductor-spatialdecon =1.4.3\n    - bioconductor-complexheatmap =2.10.0=r41hdfd78af_0\n    - r-cowplot =1.1.1=r41hc72bb7e_1\n    - r-dplyr =1.0.9=r41h7525677_0\n    - r-ggforce =0.3.4=r41h7525677_0\n    - r-ggplot2 =3.3.6=r41hc72bb7e_1\n    - r-gridextra =2.3=r41hc72bb7e_1004\n    - r-gtable =0.3.0=r41hc72bb7e_3\n    - r-knitr =1.40=r41hc72bb7e_1\n    - r-patchwork =1.1.2=r41hc72bb7e_1\n    - r-reshape2 =1.4.4=r41h7525677_2\n    - r-scales =1.2.1=r41hc72bb7e_1\n    - r-tibble =3.1.8=r41h06615bd_1\n    - r-tidyr =1.2.1=r41h7525677_1\n    - r-umap =0.2.9.0=r41h7525677_1\n    - r-rtsne =0.16=r41h37cf8d7_1\n    - r-magrittr =2.0.3=r41h06615bd_1\n    - r-rlang =1.1.0=r41h38f115c_0\n\n  run:\n    - r-base =4.1.3=h2f963a2_5\n    - bioconductor-biobase =2.54.0=r41hc0cfd56_2\n    - bioconductor-biocgenerics =0.40.0=r41hdfd78af_0\n    - bioconductor-geomxtools =3.1.1=r41hdfd78af_0\n    - bioconductor-nanostringnctools =1.2.0\n    - bioconductor-spatialdecon =1.4.3\n    - bioconductor-complexheatmap =2.10.0=r41hdfd78af_0\n    - r-cowplot =1.1.1=r41hc72bb7e_1\n    - r-dplyr =1.0.9=r41h7525677_0\n    - r-ggforce =0.3.4=r41h7525677_0\n    - r-ggplot2 =3.3.6=r41hc72bb7e_1\n    - r-gridextra =2.3=r41hc72bb7e_1004\n    - r-gtable =0.3.0=r41hc72bb7e_3\n    - r-knitr =1.40=r41hc72bb7e_1\n    - r-patchwork =1.1.2=r41hc72bb7e_1\n    - r-reshape2 =1.4.4=r41h7525677_2\n    - r-scales =1.2.1=r41hc72bb7e_1\n    - r-tibble =3.1.8=r41h06615bd_1\n    - r-tidyr =1.2.1=r41h7525677_1\n    - r-umap =0.2.9.0=r41h7525677_1\n    - r-rtsne =0.16=r41h37cf8d7_1\n    - r-magrittr =2.0.3=r41h06615bd_1\n    - r-rlang =1.1.0=r41h38f115c_0\nIn the above example, each of the dependencies has been assigned a conda build string, so that when conda builds a conda package, it will only use that specific build of the dependency from the listed conda channels. The above example is very restrictive, the dependencies can also be listed in the ‚Äúmeta.yaml‚Äù file to be more open‚Äìit will choose a conda build string that fits in with the other resolved dependency build strings based on what is available in the channels.\nAlso note that the ‚Äúhost‚Äù section matches the ‚Äúrun‚Äù section.\nHere is some examples of a more open setup for these dependencies:\n  host:\n    - r-base &gt;=4.1.3\n    - bioconductor-biobase &gt;=2.54.0\n    - bioconductor-biocgenerics &gt;=0.40.0\n    - bioconductor-geomxtools &gt;=3.1.1\n    - bioconductor-nanostringnctools &gt;=1.2.0\n    - bioconductor-spatialdecon =1.4.3\n    - bioconductor-complexheatmap &gt;=2.10.0\n    - r-cowplot &gt;=1.1.1\n    - r-dplyr &gt;=1.0.9\n    - r-ggforce &gt;=0.3.4\n    - r-ggplot2 &gt;=3.3.6\n    - r-gridextra &gt;=2.3\n    - r-gtable &gt;=0.3.0\n    - r-knitr &gt;=1.40\n    - r-patchwork &gt;=1.1.2\n    - r-reshape2 &gt;=1.4.4\n    - r-scales &gt;=1.2.1\n    - r-tibble &gt;=3.1.8\n    - r-tidyr &gt;=1.2.1\n    - r-umap &gt;=0.2.9.0\n    - r-rtsne &gt;=0.16\n    - r-magrittr &gt;=2.0.3\n    - r-rlang &gt;=1.1.0",
    "crumbs": [
      "Home",
      "R Package",
      "Create from Conda",
      "Creating the package"
    ]
  },
  {
    "objectID": "docs/R-package/create-from-conda/1_build_pkg.html#build-the-conda-package",
    "href": "docs/R-package/create-from-conda/1_build_pkg.html#build-the-conda-package",
    "title": "Creating the package",
    "section": "Build the Conda package",
    "text": "Build the Conda package\nWhen the meta.yaml has been prepared, you can now build the Conda package. To do so, run the command, replacing\n\n$r-package with the name of the R package folder that was created after running conda skeleton (the folder where the meta.yaml is located).\n$build_log_name.log with the name for the log file, such as the date, time, and initials.\n\nconda-build $r-package 2&gt;&1|tee $build_log_name.log\nExample\nconda-build r-dspworkflow 2&gt;&1|tee 05_12_23_330_nc.log\n\nThe log file will list how conda has built the package, including what dependencies version numbers and corresponding build strings were used to resolve the conda environment. These dependencies are what we specified in the ‚Äúmeta.yaml‚Äù file. The log file will be useful troubleshooting a failed build.\nBe aware, the build can take anywhere from several minutes to an hour to complete, depending on the size of the package and the number of dependencies.\nThe conda package will be built as a tar.bz2 file.",
    "crumbs": [
      "Home",
      "R Package",
      "Create from Conda",
      "Creating the package"
    ]
  },
  {
    "objectID": "docs/UCSC/3_creating_tracks.html",
    "href": "docs/UCSC/3_creating_tracks.html",
    "title": "Generating New Tracks",
    "section": "",
    "text": "Generating New Tracks\nBiowulf/Helix hosts its own instance of the UCSC Genome Browser which is behind the NIH firewall.\n\nLogin to VPN\nLogin to the UCSC Browser website\nSelect ‚ÄúMy Data &gt; Custom Tracks‚Äù\nSelect ‚ÄúAdd Custom Tracks‚Äù\nPaste the track data generated in Creating Track Info into the text box\nSelect ‚ÄúSubmit‚Äù\nReview the track information. The column ‚ÄúError‚Äù should be empty. If there is an error then a hyperlink will display ‚ÄúShow‚Äù although this often does not contain helpful error information.\nAfter troubleshooting any errors select ‚ÄúGo‚Äù\nUse the features at the bottom of the page to alter views and/or add additional track information\nSelect ‚ÄúMy Data &gt; My Sessions‚Äù\nUnder ‚ÄúSave current settings as named session:‚Äù enter a descriptive name of the session\nSelect ‚ÄúSubmit‚Äù\nThis will move the descriptive name entered into the ‚Äúsession name‚Äù list\nSelect the descriptve name, view your track information as saved\nCopy the hyperlink for this session and share as needed\n\n\n\nEditing a Previous Tracks\n\nLogin to VPN\nLogin to the UCSC Browser website\nSelect ‚ÄúMy Data &gt; My Sessions‚Äù\nSelect the descriptve name of the session you‚Äôd like to edit\nEdit the tracks as needed:\n\nIf wanting to remove or add tracks, then select ‚ÄúMy Data &gt; Custom Tracks‚Äù; follow steps 5-8 above.\nIf wanting to edit the view of the tracks, follow step 9 above.\n\nSelect ‚ÄúMy Data &gt; My Sessions‚Äù\nUnder ‚ÄúSave current settings as named session:‚Äù enter a new descriptive name OR the previous name of the session if you‚Äôd like to overwrite it\nSelect ‚ÄúSubmit‚Äù\nThis will move the descriptive name entered into the ‚Äúsession name‚Äù list\nSelect the descriptve name, view your track information as saved\nCopy the hyperlink for this session and share as needed\n\n\nTIP: Unindexed file formats like bed and gtf take significantly longer to load in the genome Browser and it is recommended to convert them to indexed formats like bigBed and bigWig prior to adding them to your session.",
    "crumbs": [
      "Home",
      "UCSC",
      "Generating New Tracks"
    ]
  },
  {
    "objectID": "docs/UCSC/0_overview.html",
    "href": "docs/UCSC/0_overview.html",
    "title": "Overview",
    "section": "",
    "text": "Overview\nThe UCSC Genome Browser allows for visualization of genomic data in an interactive and shareable format. User‚Äôs must create accounts with their NIH credentials, and have an active Biowulf account to create the tracks. In addition users have to be connect to VPN in order to view and create the tracks. Once bigwig files are generated and stored in a shared data location, genomic tracks can be edited, and permanent links created, accessible for collaborators to view.",
    "crumbs": [
      "Home",
      "UCSC",
      "Overview"
    ]
  },
  {
    "objectID": "docs/HPCDME/setup.html",
    "href": "docs/HPCDME/setup.html",
    "title": "HPCDME Setup",
    "section": "",
    "text": "Background\nHPC_DME_APIs provides command line utilities or CLUs to interface with HPCDME. This document describes some of the initial setup steps to get the CLUs working on Biowulf.\n\n\nSetup steps:\n\nClone repo:\nThe repo can be cloned at a location accessible to you:\ncd /data/$USER/\ngit clone https://github.com/CBIIT/HPC_DME_APIs.git\n\n\nCreate dirs, log files needed for HPCMDE\nmkdir -p /data/$USER/HPCDMELOG/tmp\ntouch /data/$USER/HPCDMELOG/tmp/hpc-cli.log\n\n\nCopy properties template\nhpcdme.properties is the file that all CLUs look into for various parameters like authentication password, file size limits, number of CPUs, etc. Make a copy of the template provided and prepare it for customization.\ncd /data/$USER/HPC_DME_APIs/utils\ncp hpcdme.properties-sample hpcdme.properties\n\n\nCustomize properties file\nSome of the parameters in this file have become obsolete over the course of time and are commmented out. Change paths and default values, as needed.\nNOTES\n\nReplace $USER with your actual username in the properties file. Bash variables will not be interpolated.\nLeave hpc.ssl.keystore.password=changeit as-is. changeit is not a variable but the actual password for our team.\nBe sure to set the proxy server URL as below (hpc.server.proxy.url=10.1.200.75) when running on biowulf/helix.\n\n#HPC DME Server URL\n#Production server settings\nhpc.server.url=https://hpcdmeapi.nci.nih.gov:8080\nhpc.ssl.keystore.path=hpc-client/keystore/keystore-prod.jks\n#hpc.ssl.keystore.password=hpcdmncif\nhpc.ssl.keystore.password=changeit\n\n#UAT server settings\n#hpc.server.url=https://fr-s-hpcdm-uat-p.ncifcrf.gov:7738/hpc-server\n#hpc.ssl.keystore.path=hpc-client/keystore/keystore-uat.jks\n#hpc.ssl.keystore.password=hpc-server-store-pwd\n\n#Proxy Settings\nhpc.server.proxy.url=10.1.200.75\nhpc.server.proxy.port=3128\n\nhpc.user=$USER\n\n#Globus settings\n#default globus endpoint to be used in registration and download\nhpc.globus.user=$USER\nhpc.default.globus.endpoint=ea6c8fd6-4810-11e8-8ee3-0a6d4e044368\n\n#Log files directory\nhpc.error-log.dir=/data/$USER/HPCDMELOG/tmp\n\n###HPC CLI Logging START####\n#ERROR, WARN, INFO, DEBUG\nhpc.log.level=ERROR\nhpc.log.file=/data/$USER/HPCDMELOG/tmp/hpc-cli.log\n###HPC CLI Logging END####\n\n#############################################################################\n# Please use caution changing following properties. They don't change usually\n#############################################################################\n#hpc.collection.service=collection\n#hpc.dataobject.service=dataObject\n#Log files directory\n#hpc.error-log.dir=.\n\n#Number of thread to run data file import from a CSV file\nhpc.job.thread.count=1\n\nupload.buffer.size=10000000\n\n#Retry count and backoff period for registerFromFilePath (Fixed backoff)\nhpc.retry.max.attempts=3\n#hpc.retry.backoff.period=5000\n\n#Multi-part upload thread pool, threshold and part size configuration\n#hpc.multipart.threadpoolsize=10\n#hpc.multipart.threshold=1074790400\n#hpc.multipart.chunksize=1073741824\n\n#globus.nexus.url=nexus.api.globusonline.org\n#globus.url=www.globusonline.org\n\n#HPC DME Login token file location\nhpc.login.token=tokens/hpcdme-auth.txt\n\n#Globus Login token file location\n#hpc.globus.login.token=tokens/globus-auth.txt\n#validate.md5.checksum=false\n\n# JAR version\n#hpc.jar.version=hpc-cli-1.4.0.jar\n\nNOTE: The current java version used is:\njava -version\nopenjdk version \"1.8.0_181\"\nOpenJDK Runtime Environment (build 1.8.0_181-b13)\nOpenJDK 64-Bit Server VM (build 25.181-b13, mixed mode)\n\n\n\nEdit ~/.bashrc\nAdd the CLUs to PATH by adding the following to ~/.bashrc file\n# export environment variable HPC_DM_UTILS pointing to directory where\n# HPC DME client utilities are, then source functions script in there\nexport HPC_DM_UTILS=/data/$USER/HPC_DME_APIs/utils\nsource $HPC_DM_UTILS/functions\nNext, source it\nsource ~/.bashrc\n\n\nGenerate token\nNow, you are all set to generate a token. This prevents from re-entering your password everytime.\ndm_generate_token\nIf the token generation takes longer than 45 seconds, check the connection:\nping hpcdmeapi.nci.nih.gov\nIf the connection responds, try to export the following proxy, and then re-run the dm_generate_tokens command:\nexport https_proxy=http://dtn01-e0:3128\nDone! You are now all set to use CLUs.\n\n\n\nReferences and Links\n\nHPC_DME_APIs repo\nUser guides\nWiki pages\nYuri Dinh",
    "crumbs": [
      "Home",
      "HPCDME",
      "HPCDME Setup"
    ]
  },
  {
    "objectID": "docs/HPCDME/transferFromHelix.html",
    "href": "docs/HPCDME/transferFromHelix.html",
    "title": "File Transfer from HELIX",
    "section": "",
    "text": "Rawdata or Project folders from Helix can be parked at a secure location after the analysis has reached an endpoint. Traditionally, CCBR analysts have been using GridFTP Globus Archive for doing this. But, this Globus Archive has been running relatively full lately and it is hard to estimate how much space is left there as the volume is shared among multiple groups.",
    "crumbs": [
      "Home",
      "HPCDME",
      "File Transfer from HELIX"
    ]
  },
  {
    "objectID": "docs/HPCDME/transferFromHelix.html#background",
    "href": "docs/HPCDME/transferFromHelix.html#background",
    "title": "File Transfer from HELIX",
    "section": "",
    "text": "Rawdata or Project folders from Helix can be parked at a secure location after the analysis has reached an endpoint. Traditionally, CCBR analysts have been using GridFTP Globus Archive for doing this. But, this Globus Archive has been running relatively full lately and it is hard to estimate how much space is left there as the volume is shared among multiple groups.",
    "crumbs": [
      "Home",
      "HPCDME",
      "File Transfer from HELIX"
    ]
  },
  {
    "objectID": "docs/HPCDME/transferFromHelix.html#parkit",
    "href": "docs/HPCDME/transferFromHelix.html#parkit",
    "title": "File Transfer from HELIX",
    "section": "parkit",
    "text": "parkit\nparkit is designed to assist analysts in archiving project data from the NIH‚Äôs Helix/Helix systems to the HPC-DME storage platform. It provides functionalities to package and store data such as raw FastQ files or processed data from bioinformatics pipelines. Users can automatically: - create tarballs of their data (including .filelist and .md5sum files), - generate metadata, - create collections on HPC-DME, and - deposit tar files into the system for long-term storage. parkit also features comprehensive workflows that support both folder-based and tarball-based archiving. This integration ensures that bioinformatics project data is securely archived and well-organized, allowing for seamless long-term storage.\n\n‚ùó NOTE: HPC DME API CLUs should already be setup as per these instructions in order to use parkit\n\n\n‚ùó NOTE: HPC_DM_UTILS environment variable should be set to point to the utils folder under the HPC_DME_APIs repo setup. Please see these instructions.\n\n\n‚ùó NOTE: If it has been a few months since you last used HPC_DME_APIs or parkit or projark, then please run the following commands before you start using parkit or projark:\ncd $HPC_DM_UTILS\ngit pull\nsource $HPC_DM_UTILS/functions\ndm_generate_token\n\nprojark is the preferred parkit command to completely archive an entire folder as a tarball on HPCDME. SLURM is not available on Helix and it could take a few hours to upload large files. Hence, it is recommended to use ‚Äútmux‚Äù or ‚Äúscreen‚Äù command for projark to continue running even when you log out of Helix.\n\n‚ùó NOTE: Run the following commands inside a screen or tmux session.\n\n\nprojark usage\n\nload conda env\n# source conda\n. \"/data/CCBR_Pipeliner/db/PipeDB/Conda/etc/profile.d/conda.sh\"\n# activate parkit or parkit_dev environment\nconda activate parkit\n# check version of parkit\nparkit --version\nprojark --version\n\n\nExpected sample output\n\nv2.1.2\nprojark is using the following parkit version:\nv2.1.2\n\n\n\nprojark help\nprojark --help\n\n\nExpected sample output\n\nusage: projark [-h] --folder FOLDER --projectnumber PROJECTNUMBER\n               [--executor EXECUTOR] [--rawdata] [--cleanup]\n\nWrapper for folder2hpcdme for quick CCBR project archiving!\n\noptions:\n  -h, --help            show this help message and exit\n  --folder FOLDER       Input folder path to archive\n  --projectnumber PROJECTNUMBER\n                        CCBR project number.. destination will be\n                        /CCBR_Archive/GRIDFTP/Project_CCBR-&lt;projectnumber&gt;\n  --executor EXECUTOR   slurm or local\n  --rawdata             If tarball is rawdata and needs to go under folder\n                        Rawdata\n  --cleanup             post transfer step to delete local files\n\n\n\n\nprojark testing\n\nget dummy data\n# make a tmp folder\nmkdir -p /data/$USER/parkit_tmp\n# copy dummy project folder into the tmp folder\ncp -r /data/CCBR/projects/CCBR-12345 /data/$USER/parkit_tmp/CCBR-12345-$USER\n# check if HPC_DM_UTILS has been set\necho $HPC_DM_UTILS\n\n\nrun projark\nprojark --folder /data/$USER/parkit_tmp/CCBR-12345-$USER --projectnumber 12345-$USER --executor local\n\n‚ùó NOTE: --executor slurm will not work on Helix\n\n\n\nExpected sample output\n\nSOURCE_CONDA_CMD is set to: . \"/data/CCBR_Pipeliner/db/PipeDB/Conda/etc/profile.d/conda.sh\"\nHPC_DM_UTILS is set to: /data/kopardevn/GitRepos/HPC_DME_APIs/utils\nparkit_folder2hpcdme --folder \"/data/$USER/parkit_tmp/CCBR-12345-$USER\" --dest \"/CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\" --projecttitle \"CCBR-12345-kopardevn\" --projectdesc \"CCBR-12345-kopardevn\" --executor \"local\" --hpcdmutilspath /data/kopardevn/GitRepos/HPC_DME_APIs/utils --makereadme\n################ Running createtar #############################\nparkit createtar --folder \"/data/$USER/parkit_tmp/CCBR-12345-$USER\"\ntar cvf /data/$USER/parkit_tmp/CCBR-12345-$USER.tar /data/$USER/parkit_tmp/CCBR-12345-$USER &gt; /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist\ncreatemetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar file was created!\ncreatemetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist file was created!\ncreatemetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.md5 file was created!\ncreatemetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist.md5 file was created!\n################################################################\n############ Running createemptycollection ######################\nparkit createemptycollection --dest \"/CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\" --projectdesc \"CCBR-12345-kopardevn\" --projecttitle \"CCBR-12345-kopardevn\"\nmodule load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_collection /dev/shm/a213dedc-9363-44ec-8a7a-d29f2345a0b5.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\ncat /dev/shm/a213dedc-9363-44ec-8a7a-d29f2345a0b5.json && rm -f /dev/shm/a213dedc-9363-44ec-8a7a-d29f2345a0b5.json\nmodule load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_collection /dev/shm/cabf7826-81b5-4b6a-addd-09fbcf279591.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn/Analysis\nmodule load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_collection /dev/shm/cabf7826-81b5-4b6a-addd-09fbcf279591.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn/Rawdata\ncat /dev/shm/cabf7826-81b5-4b6a-addd-09fbcf279591.json && rm -f /dev/shm/cabf7826-81b5-4b6a-addd-09fbcf279591.json\n################################################################\n########### Running createmetadata ##############################\nparkit createmetadata --tarball \"/data/$USER/parkit_tmp/CCBR-12345-$USER.tar\" --dest \"/CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\"\ncreatemetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.metadata.json file was created!\ncreatemetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist.metadata.json file was created!\n################################################################\n############# Running deposittar ###############################\nparkit deposittar --tarball \"/data/$USER/parkit_tmp/CCBR-12345-$USER.tar\" --dest \"/CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\"\nmodule load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_dataobject /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist.metadata.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn/Analysis/CCBR-12345.tar.filelist /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist\nmodule load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_dataobject_multipart /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.metadata.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn/Analysis/CCBR-12345.tar /data/$USER/parkit_tmp/CCBR-12345-$USER.tar\n################################################################\n\n\n‚ùó NOTE: add --rawdata when folder contains raw fastqs\n\n\n\nverify transfer\nTransfer can be verified by logging into HPC DME web interface.\n\n\n\nalt text\n\n\n\n\ncleanup\nDelete unwanted collection from HPC DME.\n# load java\nmodule load java\n# load dm_ commands\nsource $HPC_DM_UTILS/functions\n# delete collection recursively\ndm_delete_collection -r /CCBR_Archive/GRIDFTP/Project_CCBR-12345-$USER\n\n\nExpected sample output\n\nReading properties from /data/kopardevn/GitRepos/HPC_DME_APIs/utils/hpcdme.properties\nWARNING: You have requested recursive delete of the collection. This will delete all files and sub-collections within it recursively. Are you sure you want to proceed? (Y/N):\nY\nWould you like to see the list of files to delete ?\nN\nThe collection /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn and all files and sub-collections within it will be recursively deleted. Proceed with deletion ? (Y/N):\nY\nExecuting: https://hpcdmeapi.nci.nih.gov:8080/collection\nWrote results into /data/kopardevn/HPCDMELOG/tmp/getCollections_Records20241010.txt\nCmd process Completed\nOct 10, 2024 4:43:09 PM org.springframework.shell.core.AbstractShell handleExecutionResult\nINFO: CLI_SUCCESS\n\n\n‚ö†Ô∏è Reach out to Vishal Koparde in case you run into issues.",
    "crumbs": [
      "Home",
      "HPCDME",
      "File Transfer from HELIX"
    ]
  },
  {
    "objectID": "docs/generative-AI/gh-pilot-vs-code/index.html",
    "href": "docs/generative-AI/gh-pilot-vs-code/index.html",
    "title": "GitHub Copilot in VS Code",
    "section": "",
    "text": "TipRecording\n\n\n\nCCBR members can view the recording of this demo at the link below:\nCCBR TechDev meeting 2025-12-19",
    "crumbs": [
      "Home",
      "Generative AI",
      "GitHub Copilot in VS Code"
    ]
  },
  {
    "objectID": "docs/generative-AI/gh-pilot-vs-code/index.html#why-github-copilot",
    "href": "docs/generative-AI/gh-pilot-vs-code/index.html#why-github-copilot",
    "title": "GitHub Copilot in VS Code",
    "section": "Why GitHub Copilot?",
    "text": "Why GitHub Copilot?\nGitHub Copilot is built to work seamlessly in your IDE. It acts a bridge between you, your codebase, and LLMs like ChatGPT, Claude, etc. Copilot makes suggestions based on the context of your entire project, rather than only a single prompt.\nWith the web interfaces for ChatGPT, Claude, etc, you would have to upload your code in order for it to have access to the context of your project. Further, you would have to copy & paste suggestions from the browser into your IDE. If you frequently switch between working on multiple projects, or have large projects with hundreds or thousands of lines of code, this can quickly become cumbersome.\nGitHub Copilot in your IDE removes friction and makes using LLMs for development tasks a breeze!",
    "crumbs": [
      "Home",
      "Generative AI",
      "GitHub Copilot in VS Code"
    ]
  },
  {
    "objectID": "docs/generative-AI/gh-pilot-vs-code/index.html#setup",
    "href": "docs/generative-AI/gh-pilot-vs-code/index.html#setup",
    "title": "GitHub Copilot in VS Code",
    "section": "Setup",
    "text": "Setup\n\nSign up for Copilot\n\n\n\n\n\n\nNote\n\n\n\nAt this time, we cannot add more users to the NCI Enterprise premium Copilot license. However, you can still do a lot with the the free version of Copilot!\n\n\nEnable Copilot:\nhttps://github.com/settings/copilot\n\n\nSee more: GitHub Docs\n\n\n\nSet up Copilot in VS Code\nVS Code Docs\n\nHover over the Copilot icon in the Status Bar and select ‚ÄòUse AI Features‚Äô.\n\n\n\nSign in to GitHub\n\n\n\nStart using Copilot in VS Code!\n\nThe GitHub Copilot Chat extension should now be installed.\n\nVS Code settings\nYou can customize Copilot settings in VS Code by going to Settings &gt; Extensions &gt; GitHub Copilot or by using the keyboard shortcut CmdCmd,, on Mac.\n\n\n\n\n\n\n\nTip\n\n\n\nGitHub Copilot also works well in Positron, an IDE created by Posit based on VS Code.",
    "crumbs": [
      "Home",
      "Generative AI",
      "GitHub Copilot in VS Code"
    ]
  },
  {
    "objectID": "docs/generative-AI/gh-pilot-vs-code/index.html#demo-a-simple-python-script",
    "href": "docs/generative-AI/gh-pilot-vs-code/index.html#demo-a-simple-python-script",
    "title": "GitHub Copilot in VS Code",
    "section": "Demo: a simple python script",
    "text": "Demo: a simple python script\nExample project: given a GTF file, extract the gene transcription start sites and create a BED file of promoter regions.\nFiles:\n\nextract_promoters.py - Example Python script for demonstrating Copilot\nsetup.sh - Download data files for testing the Python script\n\nCreate a new directory called extract-promoters, download these files, and place them in that directory.\n\nOpen project in new window\nOpen the directory for your project with VS Code.\n\n\n\n\n\n\n\nTipUse git\n\n\n\nWe recommend initializing git and making a commit with the existing code before you start using Copilot.\npwd\n# /path/to/extract-promoters/\ngit init\ngit add extract_promoters.py setup.sh\ngit commit -m 'Initial commit'\n\n\n\n\nAutomatic inline suggestions\nCopilot will make suggestions automatically as you type. You can adjust this behavior in settings.\n\n\n\nHow to turn automatic inline suggestions on and off\n\n\n&lt;vscode://settings/editor.inlineSuggest.enabled&gt;\n\nExample: automatic docstring suggestion\nPlace your cursor where a function docstring should be, wait a little bit, and see if Copilot suggests one. If needed, start typing \"\"\" and see if that triggers Copilot to suggest a docstring.\n{fig-alt=VS Code editor window showing GitHub Copilot automatically suggesting code as light gray text inline with the user‚Äôs typing.}\nYou can press TabTab to accept the suggestion, or EscEsc to reject it.\n\n\n\nInline chat\nPress Cmd-ICmd-I to open the inline chat and ask for modifications right where your cursor is.\n\n\nYou can click the checkmark to accept the suggested edit if it meets your expectations. You can always modify it after accepting it if there are minor corrections to make.\n\n\nChat in the sidebar\nSometimes you may have more long-form questions, or perhaps you‚Äôd like to have a record of the prompts you‚Äôve given to Copilot. The sidebar Chat is great for this!\nYou can toggle the sidebar on and off with the sidebar button in the upper right corner of the VS Code window:\n\n\n\nOpen the sidebar\n\n\nYou can select which Large Language Model to use for your prompts:\n\n\n\nModel selection\n\n\n\nExample: ask Copilot to explain how a line of code works.\nHighlight a complicated line of code and ask Copilot to explain how it works.\n\n\n\nExample: ask Copilot to break out a complicated line of code into a separate function.\n\n\nIf you are not in Agent mode, you will need to make the modifications yourself. But Copilot‚Äôs instructions make it very clear how to do so!\n\n\nExample: ask Copilot to explain the benefits of yield vs return in Python functions\n\n\n\n\nExample: Writing unit tests\nAsk Copilot to suggest unit tests for the Python script.\n\nCopilot suggested a comprehensive suite of unit tests for the script.\n\nIf you‚Äôre not sure where to place the tests, Copilot has a detailed suggestion:\n \nBe sure to read through the unit tests, understand what they‚Äôre testing and why, and consider if there are any gaps in test coverage that you will need to correct.\n\n\n\nSource Control: git GUI in VS Code\nCopilot can even suggest commit messages for you! Using the Source Control extension, add your script to the git staging area and ask Copilot to suggest a commit message.\n\nClick the sparkles ‚ú® icon on the right side of the commit message box to generate a commit message using Copilot.\n\nRead the commit message and make sure it accurately describes the changes you‚Äôve made before you accept it.",
    "crumbs": [
      "Home",
      "Generative AI",
      "GitHub Copilot in VS Code"
    ]
  },
  {
    "objectID": "docs/generative-AI/gh-pilot-vs-code/index.html#tips",
    "href": "docs/generative-AI/gh-pilot-vs-code/index.html#tips",
    "title": "GitHub Copilot in VS Code",
    "section": "Tips",
    "text": "Tips\n\nAlways read and understand the AI suggestion before accepting it\nUse git and commit your changes often\nWrite tests to make sure your code works the way you think it does\nPeriodically step back and reflect on what you‚Äôre trying to accomplish",
    "crumbs": [
      "Home",
      "Generative AI",
      "GitHub Copilot in VS Code"
    ]
  },
  {
    "objectID": "contributors.html",
    "href": "contributors.html",
    "title": "Contributors",
    "section": "",
    "text": "Samantha\n\n\n\n\n\n\n\nVishal Koparde, PhD\n\n\n\n\n\n\n\nKelly Sovacool, PhD\n\n\n\n\n\n\n\nNed Cauley\n\n\n\n\n\nView the contributors graph on GitHub for more details."
  },
  {
    "objectID": "CHANGELOG.html",
    "href": "CHANGELOG.html",
    "title": "CCBR How-Tos",
    "section": "",
    "text": "The website is now rendered with Quarto (#24, @kelly-sovacool)\n\n\n\nLucidChart (#16, #22, @kopardev)\nConda/mamba\n\nConda to mamba migration guide (#37, #38, #39, @kopardev)\nHow to use CCBR mamba installation & environments on Biowulf (#41, @kelly-sovacool)\n\nDocker & Singularity\n\nHow to build containers interactively with Google Cloud Shell (@kelly-sovacool)\nHow to use containers with Singularity on Biowulf (@kelly-sovacool)\n\nGitHub Actions demo (@kelly-sovacool)\nResources from Code Club presentations (@kelly-sovacool)\nGenerative AI:\n\nGitHub Copilot CLI (#45, @kopardev)\nGitHub Copilot in VS Code (#47, @kelly-sovacool)\n\n\n\n\n\n\nStandardized documentation structure and navigation\nMinor content and formatting updates across guides"
  },
  {
    "objectID": "CHANGELOG.html#howtos-development-version",
    "href": "CHANGELOG.html#howtos-development-version",
    "title": "CCBR How-Tos",
    "section": "",
    "text": "The website is now rendered with Quarto (#24, @kelly-sovacool)\n\n\n\nLucidChart (#16, #22, @kopardev)\nConda/mamba\n\nConda to mamba migration guide (#37, #38, #39, @kopardev)\nHow to use CCBR mamba installation & environments on Biowulf (#41, @kelly-sovacool)\n\nDocker & Singularity\n\nHow to build containers interactively with Google Cloud Shell (@kelly-sovacool)\nHow to use containers with Singularity on Biowulf (@kelly-sovacool)\n\nGitHub Actions demo (@kelly-sovacool)\nResources from Code Club presentations (@kelly-sovacool)\nGenerative AI:\n\nGitHub Copilot CLI (#45, @kopardev)\nGitHub Copilot in VS Code (#47, @kelly-sovacool)\n\n\n\n\n\n\nStandardized documentation structure and navigation\nMinor content and formatting updates across guides"
  },
  {
    "objectID": "CHANGELOG.html#howtos-0.1.0",
    "href": "CHANGELOG.html#howtos-0.1.0",
    "title": "CCBR How-Tos",
    "section": "HowTos 0.1.0",
    "text": "HowTos 0.1.0\n\nFirst draft of datashare on Helix doc by @escauley in https://github.com/CCBR/HowTos/pull/1\nFirst draft for Conda R package doc by @escauley in https://github.com/CCBR/HowTos/pull/2\nupdates by @kopardev in https://github.com/CCBR/HowTos/pull/3\nAdd checklist of required github helper files by @kelly-sovacool in https://github.com/CCBR/HowTos/pull/7\nfeat: parkit how tos with HPCDME by @kopardev in https://github.com/CCBR/HowTos/pull/9\ndocs: clarify $USER in HPCDME properties by @kelly-sovacool in https://github.com/CCBR/HowTos/pull/8\ndocs: Added info for setting Conda channels by @escauley in https://github.com/CCBR/HowTos/pull/13\nimprove HPCDME instructions by @kelly-sovacool in https://github.com/CCBR/HowTos/pull/12\nchore(ci): address codeql alerts by @kelly-sovacool in https://github.com/CCBR/HowTos/pull/15"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "MIT License",
    "section": "",
    "text": "MIT License\nCopyright (c) 2025 HowTos authors\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ‚ÄúSoftware‚Äù), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED ‚ÄúAS IS‚Äù, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "docs/generative-AI/copilot-cli.html",
    "href": "docs/generative-AI/copilot-cli.html",
    "title": "GitHub Copilot CLI",
    "section": "",
    "text": "Installing GitHub Copilot CLI (Laptop Setup Only)\n  \n  üß© Prerequisites\n  üß± Step 1. Create and Activate a Unified Environment\n  \n  Verify Installation\n  \n  üîë Step 2. Authenticate GitHub CLI\n  ‚öôÔ∏è Step 3. Install Copilot CLI as a GitHub Extension\n  üß† Step 4. Install Standalone Copilot CLI (via npm)\n  üîë Step 5. Authenticate Standalone Copilot CLI\n  üß∞ Step 6. Enable Shell Aliases (??, gh?, git?)\n  üß™ Step 7. Verify Dual-Mode Copilot Operation\n  \n  Example 1 ‚Äî Explain a Command\n  Example 2 ‚Äî Generate a Bash Script\n  Example 3 ‚Äî Git Workflow Help\n  Example 4 ‚Äî Commit information\n  Example 5 - GitHub Issues in terminal\n  \n  üß∞ Step 8. Maintenance and Upgrades\n  üßæ Summary Table",
    "crumbs": [
      "Home",
      "Generative AI",
      "GitHub Copilot CLI"
    ]
  },
  {
    "objectID": "docs/generative-AI/copilot-cli.html#prerequisites",
    "href": "docs/generative-AI/copilot-cli.html#prerequisites",
    "title": "GitHub Copilot CLI",
    "section": "üß© Prerequisites",
    "text": "üß© Prerequisites\nBefore proceeding, ensure you have the following:\n\n‚úÖ A GitHub account with Copilot CLI access (available via your GitHub Enterprise account)\n‚úÖ Mamba or Conda installed locally\n‚úÖ A terminal configured for bash or zsh\n‚úÖ Internet connectivity (Copilot CLI requires an active connection to GitHub servers)",
    "crumbs": [
      "Home",
      "Generative AI",
      "GitHub Copilot CLI"
    ]
  },
  {
    "objectID": "docs/generative-AI/copilot-cli.html#step-1.-create-and-activate-a-unified-environment",
    "href": "docs/generative-AI/copilot-cli.html#step-1.-create-and-activate-a-unified-environment",
    "title": "GitHub Copilot CLI",
    "section": "üß± Step 1. Create and Activate a Unified Environment",
    "text": "üß± Step 1. Create and Activate a Unified Environment\nWe‚Äôll create a single environment to house both the GitHub CLI (gh) and the Node.js-based Copilot CLI. Using one environment avoids path conflicts and keeps both tools version-synced. You may also use an existing mamba environment on your laptop ‚Ä¶ remember to activate it first.\nmamba create -n ghcopilot -c conda-forge gh nodejs -y\nmamba activate ghcopilot\n\nVerify Installation\ngh --version\nnode --version\nnpm --version\nExpected output (approximate):\ngh version 2.63.0 (2025-03-15)\nnode v20.11.0\nnpm 10.2.5\n\nüß† Tip: Using Node.js ‚â• 18 ensures ES2022 compatibility and smoother npm package builds.",
    "crumbs": [
      "Home",
      "Generative AI",
      "GitHub Copilot CLI"
    ]
  },
  {
    "objectID": "docs/generative-AI/copilot-cli.html#step-2.-authenticate-github-cli",
    "href": "docs/generative-AI/copilot-cli.html#step-2.-authenticate-github-cli",
    "title": "GitHub Copilot CLI",
    "section": "üîë Step 2. Authenticate GitHub CLI",
    "text": "üîë Step 2. Authenticate GitHub CLI\nAuthenticate GitHub CLI to your personal or enterprise account:\ngh auth login\nWhen prompted:\n\nChoose GitHub.com\nSelect HTTPS as the protocol\nOpt to Authenticate Git with your GitHub credentials\nChoose Login with a web browser and follow the browser flow ‚Ä¶ you may be given a code in the terminal which you should be copy-pasting in the browser\n\nConfirm authentication:\ngh auth status\nExpected output example:\n‚úì Logged in to github.com as &lt;your_GH_handle&gt; (GitHub Enterprise)\n‚úì Git operations authenticated for github.com\n...",
    "crumbs": [
      "Home",
      "Generative AI",
      "GitHub Copilot CLI"
    ]
  },
  {
    "objectID": "docs/generative-AI/copilot-cli.html#step-3.-install-copilot-cli-as-a-github-extension",
    "href": "docs/generative-AI/copilot-cli.html#step-3.-install-copilot-cli-as-a-github-extension",
    "title": "GitHub Copilot CLI",
    "section": "‚öôÔ∏è Step 3. Install Copilot CLI as a GitHub Extension",
    "text": "‚öôÔ∏è Step 3. Install Copilot CLI as a GitHub Extension\nThe GitHub CLI supports extensions that integrate seamlessly. Copilot CLI is one such extension.\ngh extension install github/gh-copilot\nCheck the installation:\ngh extension list\nYou should see:\ngithub/gh-copilot  latest  Copilot for gh CLI\nNow, test the basic commands:\ngh copilot --help\nYour AI command line Copilot.\n\nUsage:\n  copilot [command]\n\nExamples:\n\n$ gh copilot suggest \"Install git\"\n$ gh copilot explain \"traceroute github.com\"\n\n\nAvailable Commands:\n  alias       Generate shell-specific aliases for convenience\n  config      Configure options\n  explain     Explain a command\n  suggest     Suggest a command\n\nFlags:\n  -h, --help              help for copilot\n      --hostname string   The GitHub host to use for authentication\n  -v, --version           version for copilot\n\nUse \"copilot [command] --help\" for more information about a command.\nTake a test drive:\ngh copilot suggest \"a bash script to compress all .fastq.gz files\"\n\nWelcome to GitHub Copilot in the CLI!\nversion 1.1.1 (2025-06-17)\n\nI'm powered by AI, so surprises and mistakes are possible. Make sure to verify any generated code or suggestions, and share feedback so that we can learn and improve. For more information, see https://gh.io/gh-copilot-transparency\n\n? What kind of command can I help you with?\n&gt; generic shell command\n\nSuggestion:\n\n  for file in *.fastq.gz; do\n      gzip \"$file\"\n  done\n\n? Select an option  [Use arrows to move, type to filter]\n&gt; Copy command to clipboard\n  Explain command\n  Execute command\n  Revise command\n  Rate response\n  Exit\ngh copilot suggest \"a command to list all files changed in the last 10 hours\"\n\nWelcome to GitHub Copilot in the CLI!\nversion 1.1.1 (2025-06-17)\n\nI'm powered by AI, so surprises and mistakes are possible. Make sure to verify any generated code or suggestions, and share feedback so that we can learn and improve. For more information, see https://gh.io/gh-copilot-transparency\n\n? What kind of command can I help you with?\n&gt; generic shell command\n\nSuggestion:\n\n  find . -type f -mmin -600\n\n? Select an option  [Use arrows to move, type to filter]\n&gt; Copy command to clipboard\n  Explain command\n  Execute command\n  Revise command\n  Rate response\n  Exit",
    "crumbs": [
      "Home",
      "Generative AI",
      "GitHub Copilot CLI"
    ]
  },
  {
    "objectID": "docs/generative-AI/copilot-cli.html#step-4.-install-standalone-copilot-cli-via-npm",
    "href": "docs/generative-AI/copilot-cli.html#step-4.-install-standalone-copilot-cli-via-npm",
    "title": "GitHub Copilot CLI",
    "section": "üß† Step 4. Install Standalone Copilot CLI (via npm)",
    "text": "üß† Step 4. Install Standalone Copilot CLI (via npm)\nWhile the gh extension is GitHub-integrated, the standalone Copilot CLI offers a lightweight, shell-native interface. It provides convenient commands such as ??, gh?, and git?, which allow you to quickly generate shell commands, GitHub CLI commands, or git commands based on natural language prompts. This makes it easier to automate tasks, discover command-line workflows, and boost productivity directly from your terminal without leaving your development environment.\nInstall it inside the same environment:\nnpm install -g @githubnext/github-copilot-cli\nCheck installation path and version:\nwhich github-copilot-cli\ngithub-copilot-cli --version\nExpected output should be something like this:\n~/mambaforge/envs/ghcopilot/bin/github-copilot-cli\nGitHub Copilot CLI version 1.0.8",
    "crumbs": [
      "Home",
      "Generative AI",
      "GitHub Copilot CLI"
    ]
  },
  {
    "objectID": "docs/generative-AI/copilot-cli.html#step-5.-authenticate-standalone-copilot-cli",
    "href": "docs/generative-AI/copilot-cli.html#step-5.-authenticate-standalone-copilot-cli",
    "title": "GitHub Copilot CLI",
    "section": "üîë Step 5. Authenticate Standalone Copilot CLI",
    "text": "üîë Step 5. Authenticate Standalone Copilot CLI\nLogin to associate your CLI instance with your GitHub account:\ngithub-copilot-cli auth login\nThis launches a browser window to complete authentication. Again, copy-paste the code in browser.\nConfirm login status:\ngithub-copilot-cli auth status\nExpected output:\n‚úì Authenticated as &lt;your_github_handle&gt;\n...",
    "crumbs": [
      "Home",
      "Generative AI",
      "GitHub Copilot CLI"
    ]
  },
  {
    "objectID": "docs/generative-AI/copilot-cli.html#step-6.-enable-shell-aliases-gh-git",
    "href": "docs/generative-AI/copilot-cli.html#step-6.-enable-shell-aliases-gh-git",
    "title": "GitHub Copilot CLI",
    "section": "üß∞ Step 6. Enable Shell Aliases (??, gh?, git?)",
    "text": "üß∞ Step 6. Enable Shell Aliases (??, gh?, git?)\nTo enable shortcut-style commands, append the following to your shell configuration file (~/.bashrc or ~/.zshrc):\neval \"$(github-copilot-cli alias -- \"$0\")\"\nReload your shell:\nsource ~/.zshrc\nVerify alias activation:\nalias | grep copilot\nYou should see entries such as:\nalias ??='github-copilot-cli what-the-shell'\nalias gh?='github-copilot-cli git-assist'\nalias git?='github-copilot-cli explain'",
    "crumbs": [
      "Home",
      "Generative AI",
      "GitHub Copilot CLI"
    ]
  },
  {
    "objectID": "docs/generative-AI/copilot-cli.html#step-7.-verify-dual-mode-copilot-operation",
    "href": "docs/generative-AI/copilot-cli.html#step-7.-verify-dual-mode-copilot-operation",
    "title": "GitHub Copilot CLI",
    "section": "üß™ Step 7. Verify Dual-Mode Copilot Operation",
    "text": "üß™ Step 7. Verify Dual-Mode Copilot Operation\nNow, test both integrations:\n\nExample 1 ‚Äî Explain a Command\n?? \"explain this: find . -type f -name '*.fq.gz' | xargs du -sh\"\n\n ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Command ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\nThis command finds all files in the current directory and its subdirectories with the extension '.fq.gz' and then calculates and\ndisplays the disk usage (size) of each of those files in a human-readable format.\n\n ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Explanation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n‚óã find . -name \"*.fq.gz\" is used to list files.\n  ‚óÜ . specifies that we start the search in the current directory.\n  ‚óÜ -name \"*.fq.gz\" stipulates that we search for files ending in .fq.gz.\n‚óã | xargs du -h means we pass that list of files to xargs which constructs and executes a command using those files as arguments.\n  ‚óÜ du is used to calculate disk usage (size) of files.\n  ‚óÜ -h specifies that we want the sizes in a human-readable format (e.g., KB, MB, GB).\n\n‚ùØ ‚úÖ Run this command\n  üìù Revise query\n  ‚ùå Cancel\n\n\nExample 2 ‚Äî Generate a Bash Script\n?? \"bash script to count unique reads in all fastq files\"\n\n ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Command ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\nfor file in *.fastq; do\n  awk 'NR % 4 == 2' $file | sort | uniq | wc -l;\ndone\n\n ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Explanation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n‚óã The for loop iterates over a list of items and executes\n\n‚ùØ ‚úÖ Run this command\n  üìù Revise query\n  ‚ùå Cancel\n\n\nExample 3 ‚Äî Git Workflow Help\ngit? \"how to undo the last commit but keep changes\"\n\n ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Command ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\ngit reset --soft HEAD~1\n\n ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Explanation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n‚óã git reset resets the current branch to a previous commit.\n  ‚óÜ --soft means that we keep any changes made to the files in the working directory.\n  ‚óÜ HEAD~1 specifies that we reset to the commit one before the current one.\n\n‚ùØ ‚úÖ Run this command\n  üìù Revise query\n  ‚ùå Cancel\n\n\nExample 4 ‚Äî Commit information\ngit? when was the last commit made\n\n ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Command ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\ngit log -1 --pretty=format:\"%ad\" --date=iso\n\n ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Explanation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n‚óã git log is used to show the commit history.\n  ‚óÜ -1 specifies that we only want to see the latest commit.\n  ‚óÜ --pretty=format:\"%ad\" specifies that we want to format the output to only show the author date of the commit.\n  ‚óÜ --date=iso specifies that we want the date to be in ISO format.\n\n‚ùØ ‚úÖ Run this command\n  üìù Revise query\n  ‚ùå Cancel\n\n\nExample 5 - GitHub Issues in terminal\ngh? list all the issues\n\n ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Command ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\ngh issue list -L 10000\n\n ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Explanation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n‚óã gh issue list lists all issues in the current GitHub repository.\n  ‚óÜ -L 10000 specifies that we want to list up to 10,000 issues.\n\n‚ùØ ‚úÖ Run this command\n  üìù Revise query\n  ‚ùå Cancel",
    "crumbs": [
      "Home",
      "Generative AI",
      "GitHub Copilot CLI"
    ]
  },
  {
    "objectID": "docs/generative-AI/copilot-cli.html#step-8.-maintenance-and-upgrades",
    "href": "docs/generative-AI/copilot-cli.html#step-8.-maintenance-and-upgrades",
    "title": "GitHub Copilot CLI",
    "section": "üß∞ Step 8. Maintenance and Upgrades",
    "text": "üß∞ Step 8. Maintenance and Upgrades\nKeep both components up to date:\n\n\n\n\n\n\n\nAction\nCommand\n\n\n\n\nUpdate gh-copilot\ngh extension upgrade github/gh-copilot\n\n\nRemove gh-copilot\ngh extension remove github/gh-copilot\n\n\nUpdate npm CLI\nnpm update -g @githubnext/github-copilot-cli\n\n\nRemove npm CLI\nnpm uninstall -g @githubnext/github-copilot-cli",
    "crumbs": [
      "Home",
      "Generative AI",
      "GitHub Copilot CLI"
    ]
  },
  {
    "objectID": "docs/generative-AI/copilot-cli.html#summary-table",
    "href": "docs/generative-AI/copilot-cli.html#summary-table",
    "title": "GitHub Copilot CLI",
    "section": "üßæ Summary Table",
    "text": "üßæ Summary Table\n\n\n\n\n\n\n\nTask\nCommand\n\n\n\n\nCreate Environment\nmamba create -n ghcopilot -c conda-forge gh nodejs -y\n\n\nActivate Environment\nmamba activate ghcopilot\n\n\nInstall gh Extension\ngh extension install github/gh-copilot\n\n\nInstall npm CLI\nnpm install -g @githubnext/github-copilot-cli\n\n\nAuthenticate\ngh auth login / github-copilot-cli auth login\n\n\nEnable Aliases\neval \"$(github-copilot-cli alias -- \"$0\")\"\n\n\nTest Commands\ngh copilot suggest / ?? \"explain command\"",
    "crumbs": [
      "Home",
      "Generative AI",
      "GitHub Copilot CLI"
    ]
  },
  {
    "objectID": "docs/snakemake.html",
    "href": "docs/snakemake.html",
    "title": "Snakemake",
    "section": "",
    "text": "Step-by-step guide for setting up and learning to use Snakemake, with examples and use cases\nhttps://CCBR.github.io/snakemake_tutorial/",
    "crumbs": [
      "Home",
      "Snakemake"
    ]
  },
  {
    "objectID": "docs/HPCDME/transfer.html",
    "href": "docs/HPCDME/transfer.html",
    "title": "File Transfers from Biowulf",
    "section": "",
    "text": "üõë STOP: dm_register_dataobject_multipart does not work via SLURM. Hence, file transfers are NOT working when initiated from BIOWULF. We recommend transferring files from Helix.",
    "crumbs": [
      "Home",
      "HPCDME",
      "File Transfers from Biowulf"
    ]
  },
  {
    "objectID": "docs/HPCDME/transfer.html#background",
    "href": "docs/HPCDME/transfer.html#background",
    "title": "File Transfers from Biowulf",
    "section": "Background",
    "text": "Background\nRawdata or Project folders from Biowulf can be parked at a secure location after the analysis has reached an endpoint. Traditionally, CCBR analysts have been using GridFTP Globus Archive for doing this. But, this Globus Archive has been running relatively full lately and it is hard to estimate how much space is left there as the volume is shared among multiple groups.",
    "crumbs": [
      "Home",
      "HPCDME",
      "File Transfers from Biowulf"
    ]
  },
  {
    "objectID": "docs/HPCDME/transfer.html#parkit",
    "href": "docs/HPCDME/transfer.html#parkit",
    "title": "File Transfers from Biowulf",
    "section": "parkit",
    "text": "parkit\nparkit is designed to assist analysts in archiving project data from the NIH‚Äôs Biowulf/Helix systems to the HPC-DME storage platform. It provides functionalities to package and store data such as raw FastQ files or processed data from bioinformatics pipelines. Users can automatically:\n\ncreate tarballs of their data (including .filelist and .md5sum files),\ngenerate metadata,\ncreate collections on HPC-DME, and\ndeposit tar files into the system for long-term storage.\n\nparkit also features comprehensive workflows that support both folder-based and tarball-based archiving. These workflows are integrated with the SLURM job scheduler, enabling efficient execution of archival tasks on the Biowulf HPC cluster. This integration ensures that bioinformatics project data is securely archived and well-organized, allowing for seamless long-term storage.\n\n‚ùó NOTE: HPC DME API CLUs should already be setup as per these instructions in order to use parkit\n\n\n‚ùó NOTE: HPC_DM_UTILS environment variable should be set to point to the utils folder under the HPC_DME_APIs repo setup. Please see these instructions.\n\nprojark is the preferred parkit command to completely archive an entire folder as a tarball on HPCDME using SLURM.\n\nprojark usage\n\nload conda env\n# source conda\n. \"/data/CCBR_Pipeliner/db/PipeDB/Conda/etc/profile.d/conda.sh\"\n# activate parkit or parkit_dev environment\nconda activate parkit\n# check version of parkit\nparkit --version\nprojark --version\n\n\nExpected sample output\n\nv2.0.2-dev\nprojark is using the following parkit version:\nv2.0.2-dev\n\n\n\nprojark help\nprojark --help\n\n\nExpected sample output\n\nusage: projark [-h] --folder FOLDER --projectnumber PROJECTNUMBER\n               [--executor EXECUTOR] [--rawdata] [--cleanup]\n\nWrapper for folder2hpcdme for quick CCBR project archiving!\n\noptions:\n  -h, --help            show this help message and exit\n  --folder FOLDER       Input folder path to archive\n  --projectnumber PROJECTNUMBER\n                        CCBR project number.. destination will be\n                        /CCBR_Archive/GRIDFTP/Project_CCBR-&lt;projectnumber&gt;\n  --executor EXECUTOR   slurm or local\n  --rawdata             If tarball is rawdata and needs to go under folder\n                        Rawdata\n  --cleanup             post transfer step to delete local files\n\n\n\n\nprojark testing\n\nget dummy data\n# make a tmp folder\nmkdir -p /data/$USER/parkit_tmp\n# copy dummy project folder into the tmp folder\ncp -r /data/CCBR/projects/CCBR-12345 /data/$USER/parkit_tmp/CCBR-12345-$USER\n# check if HPC_DM_UTILS has been set\necho $HPC_DM_UTILS\n\n\nrun projark\nprojark --folder /data/$USER/parkit_tmp/CCBR-12345-$USER --projectnumber 12345-$USER --executor local\n\n\nExpected sample output\n\nSOURCE_CONDA_CMD is set to: . \"/data/CCBR_Pipeliner/db/PipeDB/Conda/etc/profile.d/conda.sh\"\nHPC_DM_UTILS is set to: /data/kopardevn/GitRepos/HPC_DME_APIs/utils\nparkit_folder2hpcdme --folder \"/data/$USER/parkit_tmp/CCBR-12345-$USER\" --dest \"/CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\" --projecttitle \"CCBR-12345-kopardevn\" --projectdesc \"CCBR-12345-kopardevn\" --executor \"local\" --hpcdmutilspath /data/kopardevn/GitRepos/HPC_DME_APIs/utils --makereadme\n################ Running createtar #############################\nparkit createtar --folder \"/data/$USER/parkit_tmp/CCBR-12345-$USER\"\ntar cvf /data/$USER/parkit_tmp/CCBR-12345-$USER.tar /data/$USER/parkit_tmp/CCBR-12345-$USER &gt; /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist\ncreatemetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar file was created!\ncreatemetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist file was created!\ncreatemetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.md5 file was created!\ncreatemetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist.md5 file was created!\n################################################################\n############ Running createemptycollection ######################\nparkit createemptycollection --dest \"/CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\" --projectdesc \"CCBR-12345-kopardevn\" --projecttitle \"CCBR-12345-kopardevn\"\nmodule load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_collection /dev/shm/a213dedc-9363-44ec-8a7a-d29f2345a0b5.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\ncat /dev/shm/a213dedc-9363-44ec-8a7a-d29f2345a0b5.json && rm -f /dev/shm/a213dedc-9363-44ec-8a7a-d29f2345a0b5.json\nmodule load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_collection /dev/shm/cabf7826-81b5-4b6a-addd-09fbcf279591.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn/Analysis\nmodule load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_collection /dev/shm/cabf7826-81b5-4b6a-addd-09fbcf279591.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn/Rawdata\ncat /dev/shm/cabf7826-81b5-4b6a-addd-09fbcf279591.json && rm -f /dev/shm/cabf7826-81b5-4b6a-addd-09fbcf279591.json\n################################################################\n########### Running createmetadata ##############################\nparkit createmetadata --tarball \"/data/$USER/parkit_tmp/CCBR-12345-$USER.tar\" --dest \"/CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\"\ncreatemetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.metadata.json file was created!\ncreatemetadata: /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist.metadata.json file was created!\n################################################################\n############# Running deposittar ###############################\nparkit deposittar --tarball \"/data/$USER/parkit_tmp/CCBR-12345-$USER.tar\" --dest \"/CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn\"\nmodule load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_dataobject /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist.metadata.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn/Analysis/CCBR-12345.tar.filelist /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.filelist\nmodule load java/11.0.21 && source $HPC_DM_UTILS/functions && dm_register_dataobject_multipart /data/$USER/parkit_tmp/CCBR-12345-$USER.tar.metadata.json /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn/Analysis/CCBR-12345.tar /data/$USER/parkit_tmp/CCBR-12345-$USER.tar\n################################################################\n\n\n‚ùó NOTE: remove --executor local from the command when running on real data (not test data) to submit jobs through SLURM\n\n\n‚ùó NOTE: add --rawdata when folder contains raw fastqs\n\n\n\nverify transfer\nTransfer can be verified by logging into HPC DME web interface.\n\n\n\nalt text\n\n\n\n\ncleanup\nDelete unwanted collection from HPC DME.\n# load java\nmodule load java\n# load dm_ commands\nsource $HPC_DM_UTILS/functions\n# delete collection recursively\ndm_delete_collection -r /CCBR_Archive/GRIDFTP/Project_CCBR-12345-$USER\n\n\nExpected sample output\n\nReading properties from /data/kopardevn/GitRepos/HPC_DME_APIs/utils/hpcdme.properties\nWARNING: You have requested recursive delete of the collection. This will delete all files and sub-collections within it recursively. Are you sure you want to proceed? (Y/N):\nY\nWould you like to see the list of files to delete ?\nN\nThe collection /CCBR_Archive/GRIDFTP/Project_CCBR-12345-kopardevn and all files and sub-collections within it will be recursively deleted. Proceed with deletion ? (Y/N):\nY\nExecuting: https://hpcdmeapi.nci.nih.gov:8080/collection\nWrote results into /data/kopardevn/HPCDMELOG/tmp/getCollections_Records20241010.txt\nCmd process Completed\nOct 10, 2024 4:43:09 PM org.springframework.shell.core.AbstractShell handleExecutionResult\nINFO: CLI_SUCCESS\n\n\n‚ö†Ô∏è Reach out to Vishal Koparde in case you run into issues.",
    "crumbs": [
      "Home",
      "HPCDME",
      "File Transfers from Biowulf"
    ]
  },
  {
    "objectID": "docs/datashare.html",
    "href": "docs/datashare.html",
    "title": "Datashare",
    "section": "",
    "text": "Host data on Helix or Biowulf, that is publicly accessible through a URL",
    "crumbs": [
      "Home",
      "Datashare"
    ]
  },
  {
    "objectID": "docs/datashare.html#setup",
    "href": "docs/datashare.html#setup",
    "title": "Datashare",
    "section": "Setup",
    "text": "Setup\n\nAccess Login\n# Helix\nssh -Y username@helix.nih.gov\n\n# Biowulf\nssh -Y username@biowulf.nih.gov\nCreate a new dir (tutorial) in the datashare path:\ncd /data/CCBR/datashare/\nmkdir tutorial",
    "crumbs": [
      "Home",
      "Datashare"
    ]
  },
  {
    "objectID": "docs/datashare.html#processing",
    "href": "docs/datashare.html#processing",
    "title": "Datashare",
    "section": "Processing",
    "text": "Processing\n\nMake a directory in the datashare folder\nNOTE: For all steps below, an example is shown for Helix, but the same process is applicable for Biowulf, after changing the helix.nih.gov to biowulf.nih.gov\nNow you can transfer your data to the new directory. One method is to use scp to copy data from your local machine to Helix.\nHere is an example of using scp to copy the file file.txt from a local directory to Helix.\nscp /data/$USER/file.txt username@helix.nih.gov:/data/CCBR/datashare/tutorial/\nTo copy multiple directories recursively, you can also include the -r command with scp and from the top level directory:\nscp -r /data/$USER/ username@helix.nih.gov:/data/CCBR/datashare/tutorial/\n\n\nCreate public permissions for data\nWhen the data has been successully copied, we need to open the permissions.\nNOTE: This will give open access to anyone with the link. Ensure this is appropriate for the data type\n# cd to the shared dir\ncd /data/CCBR/datashare/\n\n# run CHMOD, twice\nchmod -R 777 tutorial\nchmod -R 777 tutorial/*\n\n# run SETFACL\nsetfacl -m u:webcpu:r-x tutorial/*",
    "crumbs": [
      "Home",
      "Datashare"
    ]
  },
  {
    "objectID": "docs/datashare.html#public-access",
    "href": "docs/datashare.html#public-access",
    "title": "Datashare",
    "section": "Public Access",
    "text": "Public Access\nNOTE: You must be logged into HPC in order to access these files from a web browser.\nFiles will be available for access through a browser, via tools like wget and UCSC genome track browser via the following format:\nhttp://hpc.nih.gov/~CCBR/tutorial/file.txt\nFor more information and a tutorial for creating UCSC tracks, visit the CCBR HowTo Page.",
    "crumbs": [
      "Home",
      "Datashare"
    ]
  },
  {
    "objectID": "docs/UCSC/2_creating_track_info.html",
    "href": "docs/UCSC/2_creating_track_info.html",
    "title": "Generating Track information",
    "section": "",
    "text": "It‚Äôs recommended to create a text file of all sample track information to ease in editing and submission to the UCSC browser website. A single line of code is needed for each sample which will provide the track location, sample name, description of the sample, whether to autoscale the samples, max height of the samples, view limits, and color. An example is provided below.\ntrack type=bigWig bigDataUrl=https://hpc.nih.gov/~CCBR/ccbr1155/${dir_loc}/bigwig/${complete_sample_id}.bigwig name=${sample_id} description=${sample_id} visibility=full autoScale=off maxHeightPixels=128:30:1 viewLimits=1:120 color=65,105,225\nUsers may find it helpful to create a single script which would create this text file for all samples. An example of this is listed below, which assumes that input files were generated using the CARLISLE pipeline. It can be edited to adapt to other output files, as needed.\nGenerally, each ‚Äútrack‚Äù line should have at least the following key value pairs: - name : label for the track - description : defines the center lable displayed - type : BAM, BED, bigBed, bigWig, etc. - bigDataUrl : URL of the data file - for other options see here\n\n\n\n\nsamples_list.txt: a single column text file with sampleID‚Äôs\ntrack_dir: path to the linked files\ntrack_output: path to output file\npeak_list: all peak types to be included\nmethod_list: what method to be included\ndedupe_list: type of duplication to be included\n\n# input arguments\nsample_list_input=/\"path/to/samples.txt\"\ntrack_dir=\"/path/to/shared/dir/\"\ntrack_output=\"/path/to/output/file/tracks.txt\npeak_list=(\"norm.relaxed.bed\" \"narrowPeak\" \"broadGo_peaks.bed\" \"narrowGo_peaks.bed\")\nmethod_list=(\"fragmentsbased\")\ndedup_list=(\"dedup\")\n\n# read sample file\nIFS=$'\\n' read -d '' -r -a sample_list &lt; $sample_list_input\n\nrun_sample_tracks (){\n    sample_id=$1\n    dedup_id=$2\n\n    # sample name\n    # eg siNC_H3K27Ac_1.dedup.bigwig\n    complete_sample_id=\"${sample_id}.${dedup_id}\"\n\n    # set link location\n    link_loc=\"${track_dir}/bigwig/${complete_sample_id}.bigwig\"\n\n    # echo track info\n    echo \"track type=bigWig bigDataUrl=https://hpc.nih.gov/~CCBR/ccbr1155/${dir_loc}/bigwig/${complete_sample_id}.bigwig name=${sample_id} description=${sample_id} visibility=full autoScale=off maxHeightPixels=128:30:1 viewLimits=1:120 color=65,105,225\" &gt;&gt; $track_output\n}\n\n# iterate through samples\n# at the sample level only DEDUP matters\nfor sample_id in ${sample_list[@]}; do\n    for dedup_id in ${dedup_list[@]}; do\n        run_sample_tracks $sample_id $dedup_id\n    done\ndone",
    "crumbs": [
      "Home",
      "UCSC",
      "Generating Track information"
    ]
  },
  {
    "objectID": "docs/UCSC/2_creating_track_info.html#single-samples",
    "href": "docs/UCSC/2_creating_track_info.html#single-samples",
    "title": "Generating Track information",
    "section": "",
    "text": "It‚Äôs recommended to create a text file of all sample track information to ease in editing and submission to the UCSC browser website. A single line of code is needed for each sample which will provide the track location, sample name, description of the sample, whether to autoscale the samples, max height of the samples, view limits, and color. An example is provided below.\ntrack type=bigWig bigDataUrl=https://hpc.nih.gov/~CCBR/ccbr1155/${dir_loc}/bigwig/${complete_sample_id}.bigwig name=${sample_id} description=${sample_id} visibility=full autoScale=off maxHeightPixels=128:30:1 viewLimits=1:120 color=65,105,225\nUsers may find it helpful to create a single script which would create this text file for all samples. An example of this is listed below, which assumes that input files were generated using the CARLISLE pipeline. It can be edited to adapt to other output files, as needed.\nGenerally, each ‚Äútrack‚Äù line should have at least the following key value pairs: - name : label for the track - description : defines the center lable displayed - type : BAM, BED, bigBed, bigWig, etc. - bigDataUrl : URL of the data file - for other options see here",
    "crumbs": [
      "Home",
      "UCSC",
      "Generating Track information"
    ]
  },
  {
    "objectID": "docs/UCSC/2_creating_track_info.html#inputs",
    "href": "docs/UCSC/2_creating_track_info.html#inputs",
    "title": "Generating Track information",
    "section": "",
    "text": "samples_list.txt: a single column text file with sampleID‚Äôs\ntrack_dir: path to the linked files\ntrack_output: path to output file\npeak_list: all peak types to be included\nmethod_list: what method to be included\ndedupe_list: type of duplication to be included\n\n# input arguments\nsample_list_input=/\"path/to/samples.txt\"\ntrack_dir=\"/path/to/shared/dir/\"\ntrack_output=\"/path/to/output/file/tracks.txt\npeak_list=(\"norm.relaxed.bed\" \"narrowPeak\" \"broadGo_peaks.bed\" \"narrowGo_peaks.bed\")\nmethod_list=(\"fragmentsbased\")\ndedup_list=(\"dedup\")\n\n# read sample file\nIFS=$'\\n' read -d '' -r -a sample_list &lt; $sample_list_input\n\nrun_sample_tracks (){\n    sample_id=$1\n    dedup_id=$2\n\n    # sample name\n    # eg siNC_H3K27Ac_1.dedup.bigwig\n    complete_sample_id=\"${sample_id}.${dedup_id}\"\n\n    # set link location\n    link_loc=\"${track_dir}/bigwig/${complete_sample_id}.bigwig\"\n\n    # echo track info\n    echo \"track type=bigWig bigDataUrl=https://hpc.nih.gov/~CCBR/ccbr1155/${dir_loc}/bigwig/${complete_sample_id}.bigwig name=${sample_id} description=${sample_id} visibility=full autoScale=off maxHeightPixels=128:30:1 viewLimits=1:120 color=65,105,225\" &gt;&gt; $track_output\n}\n\n# iterate through samples\n# at the sample level only DEDUP matters\nfor sample_id in ${sample_list[@]}; do\n    for dedup_id in ${dedup_list[@]}; do\n        run_sample_tracks $sample_id $dedup_id\n    done\ndone",
    "crumbs": [
      "Home",
      "UCSC",
      "Generating Track information"
    ]
  },
  {
    "objectID": "docs/UCSC/2_creating_track_info.html#inputs-1",
    "href": "docs/UCSC/2_creating_track_info.html#inputs-1",
    "title": "Generating Track information",
    "section": "Inputs",
    "text": "Inputs\n\nsamples_list.txt: a single column text file with sampleID‚Äôs\ntrack_dir: path to the linked files\ntrack_output: path to output file\npeak_list: all peak types to be included\nmethod_list: what method to be included\ndedupe_list: type of duplication to be included\n\n# input arguments\nsample_list_input=/\"path/to/samples.txt\"\ntrack_dir=\"/path/to/shared/dir/\"\ntrack_output=\"/path/to/output/file/tracks.txt\npeak_list=(\"norm.relaxed.bed\" \"narrowPeak\" \"broadGo_peaks.bed\" \"narrowGo_peaks.bed\")\nmethod_list=(\"fragmentsbased\")\ndedup_list=(\"dedup\")\n\n# read sample file\nIFS=$'\\n' read -d '' -r -a deg_list &lt; $deg_list_input\n\nrun_comparison_tracks (){\n    peak_type=$1\n    method_type=$2\n    dedup_type=$3\n    sample_id=$4\n\n    # sample name\n    # eg siSmyd3_2m_Smyd3_0.25HCHO_500K_vs_siNC_2m_Smyd3_0.25HCHO_500K__no_dedup__norm.relaxed\n    complete_sample_id=\"${sample_id}__${dedup_type}__${peak_type}\"\n\n    # set link location\n    link_loc=\"${track_dir}/bigbed/${complete_sample_id}_${method_type}_diffresults.bigbed\"\n\n    # echo track info\n    echo \"track name=${sample_id}_${peak_type} bigDataUrl=https://hpc.nih.gov/~CCBR/ccbr1155/${dir_loc}/bigbed/${complete_sample_id}_fragmentsbased_diffresults.bigbed type=bigBed itemRgb=On\" &gt;&gt; $track_info\n}\n\n# iterate through samples / peaks / methods / dedup\nfor sample_id in ${deg_list[@]}; do\n    for peak_id in ${peak_list[@]}; do\n        for method_id in ${method_list[@]}; do\n            for dedup_id in ${dedup_list[@]}; do\n                run_comparison_tracks $peak_id $method_id $dedup_id $sample_id\n            done\n        done\n    done\ndone",
    "crumbs": [
      "Home",
      "UCSC",
      "Generating Track information"
    ]
  },
  {
    "objectID": "docs/UCSC/1_creating_inputs.html",
    "href": "docs/UCSC/1_creating_inputs.html",
    "title": "CCBR How-Tos",
    "section": "",
    "text": "In order to use the genomic broswer features, sample files must be created.\n\n\nFor individual samples, where peak density is to be observed, bigwig formatted files must be generated. If using the CCBR pipelines these are automatically generated as outputs of the pipeline (WORKDIR/results/bigwig). In many cases, scaling or normalization of bigwig is required to visualize multiple samples in comparison with each other. See various deeptools options for details/ideas. If not using CCBR pipelines, example code is provided below for the file generation.\nmodue load ucsc\n\nfragments_bed=\"/path/to/sample1.fragments.bed\"\nbw=\"/path/to/sample1.bigwig\"\ngenome_len=\"numeric_genome_length\"\nbg=\"/path/to/sample1.bedgraph\"\nbw=\"/path/to/sample2.bigwig\"\n\n# if using a spike-in scale, the scaling factor should be applied\n# while not required, it is recommended for CUT&RUN experiements\nspikein_scale=\"spike_in_value\"\n\n# create bed file\nbedtools genomecov -bg -scale $spikein_scale -i $fragments_bed -g $genome_len &gt; $bg\n\n# create bigwig file\nbedGraphToBigWig $bg $genome_len $bw\n\n\n\nFor contrasts, where peak differences are to be observed, bigbed formatted files must be generated. If using the CCBR/CARLISLE pipeline these are automatically generated as outputs of the pipeline (WORKDIR/results/peaks/contrasts/contrast_id/). If not using this pipeline, example code is provided below for the file generation.\nmodule load ucsc\n\nbed=\"/path/to/sample1_vs_sample2_fragmentsbased_diffresults.bed\"\nbigbed=\"/path/to/output/sample1_vs_sample2_fragmentsbased_diffresults.bigbed\"\ngenome_len=\"numeric_genome_length\"\n\n# create bigbed file\nbedToBigBed -type=bed9 $bed $genome_len $bigbed",
    "crumbs": [
      "Home",
      "UCSC",
      "Generating Inputs"
    ]
  },
  {
    "objectID": "docs/UCSC/1_creating_inputs.html#generating-inputs",
    "href": "docs/UCSC/1_creating_inputs.html#generating-inputs",
    "title": "CCBR How-Tos",
    "section": "",
    "text": "In order to use the genomic broswer features, sample files must be created.\n\n\nFor individual samples, where peak density is to be observed, bigwig formatted files must be generated. If using the CCBR pipelines these are automatically generated as outputs of the pipeline (WORKDIR/results/bigwig). In many cases, scaling or normalization of bigwig is required to visualize multiple samples in comparison with each other. See various deeptools options for details/ideas. If not using CCBR pipelines, example code is provided below for the file generation.\nmodue load ucsc\n\nfragments_bed=\"/path/to/sample1.fragments.bed\"\nbw=\"/path/to/sample1.bigwig\"\ngenome_len=\"numeric_genome_length\"\nbg=\"/path/to/sample1.bedgraph\"\nbw=\"/path/to/sample2.bigwig\"\n\n# if using a spike-in scale, the scaling factor should be applied\n# while not required, it is recommended for CUT&RUN experiements\nspikein_scale=\"spike_in_value\"\n\n# create bed file\nbedtools genomecov -bg -scale $spikein_scale -i $fragments_bed -g $genome_len &gt; $bg\n\n# create bigwig file\nbedGraphToBigWig $bg $genome_len $bw\n\n\n\nFor contrasts, where peak differences are to be observed, bigbed formatted files must be generated. If using the CCBR/CARLISLE pipeline these are automatically generated as outputs of the pipeline (WORKDIR/results/peaks/contrasts/contrast_id/). If not using this pipeline, example code is provided below for the file generation.\nmodule load ucsc\n\nbed=\"/path/to/sample1_vs_sample2_fragmentsbased_diffresults.bed\"\nbigbed=\"/path/to/output/sample1_vs_sample2_fragmentsbased_diffresults.bigbed\"\ngenome_len=\"numeric_genome_length\"\n\n# create bigbed file\nbedToBigBed -type=bed9 $bed $genome_len $bigbed",
    "crumbs": [
      "Home",
      "UCSC",
      "Generating Inputs"
    ]
  },
  {
    "objectID": "docs/UCSC/1_creating_inputs.html#sharing-data",
    "href": "docs/UCSC/1_creating_inputs.html#sharing-data",
    "title": "CCBR How-Tos",
    "section": "Sharing data",
    "text": "Sharing data\nFor all sample types, data must be stored on a shared directory. It is recommended that symlnks be created from the source location to this shared directory to ensure that minial disc space is being used. Example code for creating symlinks is provided below.\n\nsingle sample\n# single sample\n## set source file location\nsource_loc=\"/WORKDIR/results/bigwig/sample1.bigwig \"\n\n## set destination link location\nlink_loc=\"/SHAREDDIR/bigwig/sample1.bigwig\"\n\n## create hard links\nln $source_loc $link_loc\n\n\ncontrast sample\n# contrast\n## set source file location\nsource_loc=\"WORKDIR/results/peaks/contrasts/sample1_vs_sample2/sample1_vs_sample2_fragmentsbased_diffresults.bigbed \"\n\n## set destination link location\nlink_loc=\"/SHAREDDIR/bigbed/sample1_vs_sample2.bigbed\"\n\n## create hard links\nln $source_loc $link_loc\nOnce the links have been generated, the data folder must be open to read and write access.\n## set destination link location\nlink_loc=\"/SHAREDDIR/bigbed/\"\n\n# open dir\nchmod -R a+rX $link_loc",
    "crumbs": [
      "Home",
      "UCSC",
      "Generating Inputs"
    ]
  },
  {
    "objectID": "docs/R-package/create-from-conda/0_setup.html",
    "href": "docs/R-package/create-from-conda/0_setup.html",
    "title": "setup",
    "section": "",
    "text": "You will first need to set up Conda in order to use the Conda tools for creating your Conda package.\nThe documentation for getting started can be found here, including installation guidelines.\n\n\n\n\nIn a space shared with other users that may use Conda, your personal Conda cache needs to be specified. To edit how your cache is saved perform the following steps:\n\nCreate a new directory where you would like to store the conda cache called ‚Äòconda-cache‚Äô\n\nmkdir conda/conda-cache\n\nIn your home directory, create the file .condarc\n\ntouch ~/.condarc\n\nOpen the new file .condarc and add the following sections:\n\n\npkgs_dirs\nenvs_dirs\nconda-build\nchannels\n\nIn each section you will add the path to the directories you would like to use for each section.\nExample:\nkgs_dirs:\n  - /rstudio-files/ccbr-data/users/$USER/conda-cache\nenvs_dirs:\n  - /rstudio-files/ccbr-data/users/$USER/conda-envs\nconda-build:\n    root-dir: /rstudio-files/ccbr-data/users/$USER/conda-bld\n    build_folder: /rstudio-files/ccbr-data/users/$USER/conda-bld\n    conda-build\n  output_folder: /rstudio-files/ccbr-data/users/$USER/conda-bld/conda-output\nchannels:\n  - file://rstudio-files/RH/ccbr-projects/Conda_package_tutorial/local_channel/channel\n  - conda-forge\n  - bioconda\n  - defaults\n\n\n\nTo check that conda has been setup with the specified paths from .condarc start conda:\nconda activate\nThen check the conda info:\nconda info\n\n\nTo build a Conda package, ‚Äòchannels‚Äô are needed to supply the dependencies that are specified in the DESCRIPTION and meta.yaml files (discussed below). These R packages in these channels are also Conda packages that have been previously built as a specific version of that package and given a ‚Äòbuild string‚Äô, a unique indentifier for the build of that specific conda package.\nFor channels to be available to you when you build your own conda package, you first need to add them. To add a Conda channel run:\nconfig ‚Äìadd channels \nFor , some examples are:\n\nfile://rstudio-files/RH/ccbr-projects/Conda_package_tutorial/local_channel/channel\nconda-forge\nbioconda\ndefaults\n\n\n\n\n\nTo create a package with Conda, you first need to make a new release and tag in the github repo of the R package you would like to create into a Conda Package.\nFor more information on best practices in R package creation, see:\nhttps://r-pkgs.org/whole-game.html\nBefore creating the release on github, please check for proper dependencies listed in the files NAMESPACE and DESCRIPTION.\nThese files should have the same list of dependencies, and the version numbers for dependencies can be specified in the DESCRIPTION file. The DESCRIPTION file must be editted manually, while the NAMESPACE file should not be editted manually, but rather created automatically using the document() function.\nThe DESCRITPION file must also be correctly formatted. For more information, see:\nhttps://r-pkgs.org/description.html\n\n\nTo download the most recent release from the most recent tag on Github, activate Conda then use Conda skeleton like so:\nconda activate\n\nconda skeleton cran $githubURL\nReplace $githubURL with the URL to your R package‚Äôs github repo.\nA folder is then created for the downloaded release, for example running the following:\nconda skeleton cran https://github.com/NIDAP-Community/DSPWorkflow\ncreates the folder\nr-dspworkflow\nWithin this newly created folder is a file named ‚Äúmeta.yaml‚Äù. You will need to edit this file to include the channels and edit any information on the the package version number or dependency version numbers.\nHere is an example of the top of the ‚Äúmeta.yaml‚Äù file with the channels section added:\n{% set version = '0.9.5.2' %}\n\n{% set posix = 'm2-' if win else '' %}\n{% set native = 'm2w64-' if win else '' %}\n\npackage:\n  name: r-dspworkflow\n  version: {{ version|replace(\"-\", \"_\") }}\n\nchannels:\n  - conda-forge\n  - bioconda\n  - default\n  - file://rstudio-files/RH/ccbr-projects/Conda_package_tutorial/local_channel/channel\n\nsource:\n\n  git_url: https://github.com/NIDAP-Community/DSPWorkflow\n  git_tag: 0.9.5\n\nbuild:\n  merge_build_host: True  # [win]\n  # If this is a new build for the same version, increment the build number.\n  number: 0\n  # no skip\n\n  # This is required to make R link correctly on Linux.\n  rpaths:\n    - lib/R/lib/\n    - lib/\n\n    # Suggests: testthat (== 3.1.4)\nrequirements:\n  build:\n    - {{ posix }}filesystem        # [win]\n    - {{ posix }}git\n    - {{ posix }}zip               # [win]\nHere is an example of the sections for specifying dependency versions from the ‚Äúmeta.yaml‚Äù file:\n  host:\n    - r-base =4.1.3=h2f963a2_5\n    - bioconductor-biobase =2.54.0=r41hc0cfd56_2\n    - bioconductor-biocgenerics =0.40.0=r41hdfd78af_0\n    - bioconductor-geomxtools =3.1.1=r41hdfd78af_0\n    - bioconductor-nanostringnctools =1.2.0\n    - bioconductor-spatialdecon =1.4.3\n    - bioconductor-complexheatmap =2.10.0=r41hdfd78af_0\n    - r-cowplot =1.1.1=r41hc72bb7e_1\n    - r-dplyr =1.0.9=r41h7525677_0\n    - r-ggforce =0.3.4=r41h7525677_0\n    - r-ggplot2 =3.3.6=r41hc72bb7e_1\n    - r-gridextra =2.3=r41hc72bb7e_1004\n    - r-gtable =0.3.0=r41hc72bb7e_3\n    - r-knitr =1.40=r41hc72bb7e_1\n    - r-patchwork =1.1.2=r41hc72bb7e_1\n    - r-reshape2 =1.4.4=r41h7525677_2\n    - r-scales =1.2.1=r41hc72bb7e_1\n    - r-tibble =3.1.8=r41h06615bd_1\n    - r-tidyr =1.2.1=r41h7525677_1\n    - r-umap =0.2.9.0=r41h7525677_1\n    - r-rtsne =0.16=r41h37cf8d7_1\n    - r-magrittr =2.0.3=r41h06615bd_1\n    - r-rlang =1.1.0=r41h38f115c_0\n\n  run:\n    - r-base =4.1.3=h2f963a2_5\n    - bioconductor-biobase =2.54.0=r41hc0cfd56_2\n    - bioconductor-biocgenerics =0.40.0=r41hdfd78af_0\n    - bioconductor-geomxtools =3.1.1=r41hdfd78af_0\n    - bioconductor-nanostringnctools =1.2.0\n    - bioconductor-spatialdecon =1.4.3\n    - bioconductor-complexheatmap =2.10.0=r41hdfd78af_0\n    - r-cowplot =1.1.1=r41hc72bb7e_1\n    - r-dplyr =1.0.9=r41h7525677_0\n    - r-ggforce =0.3.4=r41h7525677_0\n    - r-ggplot2 =3.3.6=r41hc72bb7e_1\n    - r-gridextra =2.3=r41hc72bb7e_1004\n    - r-gtable =0.3.0=r41hc72bb7e_3\n    - r-knitr =1.40=r41hc72bb7e_1\n    - r-patchwork =1.1.2=r41hc72bb7e_1\n    - r-reshape2 =1.4.4=r41h7525677_2\n    - r-scales =1.2.1=r41hc72bb7e_1\n    - r-tibble =3.1.8=r41h06615bd_1\n    - r-tidyr =1.2.1=r41h7525677_1\n    - r-umap =0.2.9.0=r41h7525677_1\n    - r-rtsne =0.16=r41h37cf8d7_1\n    - r-magrittr =2.0.3=r41h06615bd_1\n    - r-rlang =1.1.0=r41h38f115c_0\nIn the above example, each of the dependencies has been assigned a conda build string, so that when conda builds a conda package, it will only use that specific build of the dependency from the listed conda channels. The above example is very restrictive, the dependencies can also be listed in the ‚Äúmeta.yaml‚Äù file to be more open‚Äìit will choose a conda build string that fits in with the other resolved dependency build strings based on what is available in the channels.\nAlso note that the ‚Äúhost‚Äù section matches the ‚Äúrun‚Äù section.\nHere is some examples of a more open setup for these dependencies:\n  host:\n    - r-base &gt;=4.1.3\n    - bioconductor-biobase &gt;=2.54.0\n    - bioconductor-biocgenerics &gt;=0.40.0\n    - bioconductor-geomxtools &gt;=3.1.1\n    - bioconductor-nanostringnctools &gt;=1.2.0\n    - bioconductor-spatialdecon =1.4.3\n    - bioconductor-complexheatmap &gt;=2.10.0\n    - r-cowplot &gt;=1.1.1\n    - r-dplyr &gt;=1.0.9\n    - r-ggforce &gt;=0.3.4\n    - r-ggplot2 &gt;=3.3.6\n    - r-gridextra &gt;=2.3\n    - r-gtable &gt;=0.3.0\n    - r-knitr &gt;=1.40\n    - r-patchwork &gt;=1.1.2\n    - r-reshape2 &gt;=1.4.4\n    - r-scales &gt;=1.2.1\n    - r-tibble &gt;=3.1.8\n    - r-tidyr &gt;=1.2.1\n    - r-umap &gt;=0.2.9.0\n    - r-rtsne &gt;=0.16\n    - r-magrittr &gt;=2.0.3\n    - r-rlang &gt;=1.1.0\n\n\n\nWhen the ‚Äúmeta.yaml‚Äù has been prepared, you can now build the Conda package.\nTo do so, run the command:\nconda-build $r-package 2&gt;&1 | tee $build_log_name.log\nReplace $r-package with the name of the R package folder that was created after running conda skeleton (the folder where the meta.yaml is located).\nExample: r-dspworkflow\nReplace $build_log_name.log with the name for the log file, such as the date, time, and initials.\nExample: 05_12_23_330_nc.log\nThe log file will list how conda has built the package, including what dependencies version numbers and corresponding build strings were used to resolve the conda environment. These dependencies are what we specified in the ‚Äúmeta.yaml‚Äù file. The log file will be useful troubleshooting a failed build.\nBe aware, the build can take anywhere from several minutes to an hour to complete, depending on the size of the package and the number of dependencies.\nThe conda package will be built as a tar.bz2 file.\n\n\n\n\n\n\nAn important consideration for Conda builds is the list of dependencies, specified versions, and compatibility with each of the other dependencies.\nIf the meta.yaml and DESCRIPTION file specify specific package versions, Conda‚Äôs ability to resolve the Conda environment also becomes more limited.\nFor example, if the Conda package we are building has the following requirements:\nDependency A version == 1.0\nDependency B version &gt;= 2.5\nAnd the Dependencies located in our Conda channel have the following dependencies:\nDependency A version 1.0\n  - Dependency C version == 0.5\n\nDependency A version 1.2\n- Dependency C version &gt;= 0.7\n\nDependency B version 2.7\n  - Dependency C version &gt;= 0.7\nAs you can see, the Conda build will not be able to resolve the environment because Dependency A verion 1.0 needs an old version of Dependency C, while Dependency B version 2.7 needs a newer version.\nIn this case, if we changed our package‚Äôs DESCRIPTION and meta.yaml file to be:\nDependency A version &gt;= 1.0\nDependency B version &gt;= 2.5\nThe conda build will be able to resolve. This is a simplied version of a what are more often complex dependency structures, but it is an important concept in conda package building that will inevitably arise as a package‚Äôs dependencies become more specific.\nTo check on the versions of packages that are available in a Conda channel, use the command:\nconda search $dependency\nReplace $dependency with the name of package you would like to investigate. There are more optional commands for this function which can be found here:\nhttps://docs.conda.io/projects/conda/en/latest/commands/search.html\nTo check the dependencies of packages that exist in your Conda cache, go to the folder specified for your conda cache (that we specified earlier). In case you need to find that path you can use:\nconda info\nHere there will be a folder for each of the packages that has been used in a conda build (including the dependencies). In each folder is another folder called ‚Äúinfo‚Äù and a file called ‚Äúindex.json‚Äù that lists information, such as depends for the package.\nHere is an example:\ncat /rstudio-files/ccbr-data/users/$USER/conda-cache/r-ggplot2-3.3.6-r41hc72bb7e_1/info/index.json\n\n{\n \"arch\": null,\n \"build\": \"r41hc72bb7e_1\",\n \"build_number\": 1,\n \"depends\": [\n   \"r-base &gt;=4.1,&lt;4.2.0a0\",\n   \"r-digest\",\n   \"r-glue\",\n   \"r-gtable &gt;=0.1.1\",\n   \"r-isoband\",\n   \"r-mass\",\n   \"r-mgcv\",\n   \"r-rlang &gt;=0.3.0\",\n   \"r-scales &gt;=0.5.0\",\n   \"r-tibble\",\n   \"r-withr &gt;=2.0.0\"\n ],\n \"license\": \"GPL-2.0-only\",\n \"license_family\": \"GPL2\",\n \"name\": \"r-ggplot2\",\n \"noarch\": \"generic\",\n \"platform\": null,\n \"subdir\": \"noarch\",\n \"timestamp\": 1665515494942,\n \"version\": \"3.3.6\"\n}\nIf you would like to specify an exact package to use in a conda channel for your conda build, specify the build string in your ‚Äúmeta.yaml‚Äù file. In the above example for ggplot version 3.3.6, the build string is listed in the folder name for package as well as in the index.json file for ‚Äúbuild:‚Äùr41hc72bb7e_1‚Äù.",
    "crumbs": [
      "Home",
      "R Package",
      "Create from Conda",
      "setup"
    ]
  },
  {
    "objectID": "docs/R-package/create-from-conda/0_setup.html#set-up-conda-tools-installation-if-needed",
    "href": "docs/R-package/create-from-conda/0_setup.html#set-up-conda-tools-installation-if-needed",
    "title": "setup",
    "section": "",
    "text": "You will first need to set up Conda in order to use the Conda tools for creating your Conda package.\nThe documentation for getting started can be found here, including installation guidelines.",
    "crumbs": [
      "Home",
      "R Package",
      "Create from Conda",
      "setup"
    ]
  },
  {
    "objectID": "docs/R-package/create-from-conda/0_setup.html#set-up-conda-cache",
    "href": "docs/R-package/create-from-conda/0_setup.html#set-up-conda-cache",
    "title": "setup",
    "section": "",
    "text": "In a space shared with other users that may use Conda, your personal Conda cache needs to be specified. To edit how your cache is saved perform the following steps:\n\nCreate a new directory where you would like to store the conda cache called ‚Äòconda-cache‚Äô\n\nmkdir conda/conda-cache\n\nIn your home directory, create the file .condarc\n\ntouch ~/.condarc\n\nOpen the new file .condarc and add the following sections:\n\n\npkgs_dirs\nenvs_dirs\nconda-build\nchannels\n\nIn each section you will add the path to the directories you would like to use for each section.\nExample:\nkgs_dirs:\n  - /rstudio-files/ccbr-data/users/$USER/conda-cache\nenvs_dirs:\n  - /rstudio-files/ccbr-data/users/$USER/conda-envs\nconda-build:\n    root-dir: /rstudio-files/ccbr-data/users/$USER/conda-bld\n    build_folder: /rstudio-files/ccbr-data/users/$USER/conda-bld\n    conda-build\n  output_folder: /rstudio-files/ccbr-data/users/$USER/conda-bld/conda-output\nchannels:\n  - file://rstudio-files/RH/ccbr-projects/Conda_package_tutorial/local_channel/channel\n  - conda-forge\n  - bioconda\n  - defaults",
    "crumbs": [
      "Home",
      "R Package",
      "Create from Conda",
      "setup"
    ]
  },
  {
    "objectID": "docs/R-package/create-from-conda/0_setup.html#check-conda-setup",
    "href": "docs/R-package/create-from-conda/0_setup.html#check-conda-setup",
    "title": "setup",
    "section": "",
    "text": "To check that conda has been setup with the specified paths from .condarc start conda:\nconda activate\nThen check the conda info:\nconda info\n\n\nTo build a Conda package, ‚Äòchannels‚Äô are needed to supply the dependencies that are specified in the DESCRIPTION and meta.yaml files (discussed below). These R packages in these channels are also Conda packages that have been previously built as a specific version of that package and given a ‚Äòbuild string‚Äô, a unique indentifier for the build of that specific conda package.\nFor channels to be available to you when you build your own conda package, you first need to add them. To add a Conda channel run:\nconfig ‚Äìadd channels \nFor , some examples are:\n\nfile://rstudio-files/RH/ccbr-projects/Conda_package_tutorial/local_channel/channel\nconda-forge\nbioconda\ndefaults",
    "crumbs": [
      "Home",
      "R Package",
      "Create from Conda",
      "setup"
    ]
  },
  {
    "objectID": "docs/R-package/create-from-conda/0_setup.html#conda-r-package-creation",
    "href": "docs/R-package/create-from-conda/0_setup.html#conda-r-package-creation",
    "title": "setup",
    "section": "",
    "text": "To create a package with Conda, you first need to make a new release and tag in the github repo of the R package you would like to create into a Conda Package.\nFor more information on best practices in R package creation, see:\nhttps://r-pkgs.org/whole-game.html\nBefore creating the release on github, please check for proper dependencies listed in the files NAMESPACE and DESCRIPTION.\nThese files should have the same list of dependencies, and the version numbers for dependencies can be specified in the DESCRIPTION file. The DESCRIPTION file must be editted manually, while the NAMESPACE file should not be editted manually, but rather created automatically using the document() function.\nThe DESCRITPION file must also be correctly formatted. For more information, see:\nhttps://r-pkgs.org/description.html\n\n\nTo download the most recent release from the most recent tag on Github, activate Conda then use Conda skeleton like so:\nconda activate\n\nconda skeleton cran $githubURL\nReplace $githubURL with the URL to your R package‚Äôs github repo.\nA folder is then created for the downloaded release, for example running the following:\nconda skeleton cran https://github.com/NIDAP-Community/DSPWorkflow\ncreates the folder\nr-dspworkflow\nWithin this newly created folder is a file named ‚Äúmeta.yaml‚Äù. You will need to edit this file to include the channels and edit any information on the the package version number or dependency version numbers.\nHere is an example of the top of the ‚Äúmeta.yaml‚Äù file with the channels section added:\n{% set version = '0.9.5.2' %}\n\n{% set posix = 'm2-' if win else '' %}\n{% set native = 'm2w64-' if win else '' %}\n\npackage:\n  name: r-dspworkflow\n  version: {{ version|replace(\"-\", \"_\") }}\n\nchannels:\n  - conda-forge\n  - bioconda\n  - default\n  - file://rstudio-files/RH/ccbr-projects/Conda_package_tutorial/local_channel/channel\n\nsource:\n\n  git_url: https://github.com/NIDAP-Community/DSPWorkflow\n  git_tag: 0.9.5\n\nbuild:\n  merge_build_host: True  # [win]\n  # If this is a new build for the same version, increment the build number.\n  number: 0\n  # no skip\n\n  # This is required to make R link correctly on Linux.\n  rpaths:\n    - lib/R/lib/\n    - lib/\n\n    # Suggests: testthat (== 3.1.4)\nrequirements:\n  build:\n    - {{ posix }}filesystem        # [win]\n    - {{ posix }}git\n    - {{ posix }}zip               # [win]\nHere is an example of the sections for specifying dependency versions from the ‚Äúmeta.yaml‚Äù file:\n  host:\n    - r-base =4.1.3=h2f963a2_5\n    - bioconductor-biobase =2.54.0=r41hc0cfd56_2\n    - bioconductor-biocgenerics =0.40.0=r41hdfd78af_0\n    - bioconductor-geomxtools =3.1.1=r41hdfd78af_0\n    - bioconductor-nanostringnctools =1.2.0\n    - bioconductor-spatialdecon =1.4.3\n    - bioconductor-complexheatmap =2.10.0=r41hdfd78af_0\n    - r-cowplot =1.1.1=r41hc72bb7e_1\n    - r-dplyr =1.0.9=r41h7525677_0\n    - r-ggforce =0.3.4=r41h7525677_0\n    - r-ggplot2 =3.3.6=r41hc72bb7e_1\n    - r-gridextra =2.3=r41hc72bb7e_1004\n    - r-gtable =0.3.0=r41hc72bb7e_3\n    - r-knitr =1.40=r41hc72bb7e_1\n    - r-patchwork =1.1.2=r41hc72bb7e_1\n    - r-reshape2 =1.4.4=r41h7525677_2\n    - r-scales =1.2.1=r41hc72bb7e_1\n    - r-tibble =3.1.8=r41h06615bd_1\n    - r-tidyr =1.2.1=r41h7525677_1\n    - r-umap =0.2.9.0=r41h7525677_1\n    - r-rtsne =0.16=r41h37cf8d7_1\n    - r-magrittr =2.0.3=r41h06615bd_1\n    - r-rlang =1.1.0=r41h38f115c_0\n\n  run:\n    - r-base =4.1.3=h2f963a2_5\n    - bioconductor-biobase =2.54.0=r41hc0cfd56_2\n    - bioconductor-biocgenerics =0.40.0=r41hdfd78af_0\n    - bioconductor-geomxtools =3.1.1=r41hdfd78af_0\n    - bioconductor-nanostringnctools =1.2.0\n    - bioconductor-spatialdecon =1.4.3\n    - bioconductor-complexheatmap =2.10.0=r41hdfd78af_0\n    - r-cowplot =1.1.1=r41hc72bb7e_1\n    - r-dplyr =1.0.9=r41h7525677_0\n    - r-ggforce =0.3.4=r41h7525677_0\n    - r-ggplot2 =3.3.6=r41hc72bb7e_1\n    - r-gridextra =2.3=r41hc72bb7e_1004\n    - r-gtable =0.3.0=r41hc72bb7e_3\n    - r-knitr =1.40=r41hc72bb7e_1\n    - r-patchwork =1.1.2=r41hc72bb7e_1\n    - r-reshape2 =1.4.4=r41h7525677_2\n    - r-scales =1.2.1=r41hc72bb7e_1\n    - r-tibble =3.1.8=r41h06615bd_1\n    - r-tidyr =1.2.1=r41h7525677_1\n    - r-umap =0.2.9.0=r41h7525677_1\n    - r-rtsne =0.16=r41h37cf8d7_1\n    - r-magrittr =2.0.3=r41h06615bd_1\n    - r-rlang =1.1.0=r41h38f115c_0\nIn the above example, each of the dependencies has been assigned a conda build string, so that when conda builds a conda package, it will only use that specific build of the dependency from the listed conda channels. The above example is very restrictive, the dependencies can also be listed in the ‚Äúmeta.yaml‚Äù file to be more open‚Äìit will choose a conda build string that fits in with the other resolved dependency build strings based on what is available in the channels.\nAlso note that the ‚Äúhost‚Äù section matches the ‚Äúrun‚Äù section.\nHere is some examples of a more open setup for these dependencies:\n  host:\n    - r-base &gt;=4.1.3\n    - bioconductor-biobase &gt;=2.54.0\n    - bioconductor-biocgenerics &gt;=0.40.0\n    - bioconductor-geomxtools &gt;=3.1.1\n    - bioconductor-nanostringnctools &gt;=1.2.0\n    - bioconductor-spatialdecon =1.4.3\n    - bioconductor-complexheatmap &gt;=2.10.0\n    - r-cowplot &gt;=1.1.1\n    - r-dplyr &gt;=1.0.9\n    - r-ggforce &gt;=0.3.4\n    - r-ggplot2 &gt;=3.3.6\n    - r-gridextra &gt;=2.3\n    - r-gtable &gt;=0.3.0\n    - r-knitr &gt;=1.40\n    - r-patchwork &gt;=1.1.2\n    - r-reshape2 &gt;=1.4.4\n    - r-scales &gt;=1.2.1\n    - r-tibble &gt;=3.1.8\n    - r-tidyr &gt;=1.2.1\n    - r-umap &gt;=0.2.9.0\n    - r-rtsne &gt;=0.16\n    - r-magrittr &gt;=2.0.3\n    - r-rlang &gt;=1.1.0\n\n\n\nWhen the ‚Äúmeta.yaml‚Äù has been prepared, you can now build the Conda package.\nTo do so, run the command:\nconda-build $r-package 2&gt;&1 | tee $build_log_name.log\nReplace $r-package with the name of the R package folder that was created after running conda skeleton (the folder where the meta.yaml is located).\nExample: r-dspworkflow\nReplace $build_log_name.log with the name for the log file, such as the date, time, and initials.\nExample: 05_12_23_330_nc.log\nThe log file will list how conda has built the package, including what dependencies version numbers and corresponding build strings were used to resolve the conda environment. These dependencies are what we specified in the ‚Äúmeta.yaml‚Äù file. The log file will be useful troubleshooting a failed build.\nBe aware, the build can take anywhere from several minutes to an hour to complete, depending on the size of the package and the number of dependencies.\nThe conda package will be built as a tar.bz2 file.",
    "crumbs": [
      "Home",
      "R Package",
      "Create from Conda",
      "setup"
    ]
  },
  {
    "objectID": "docs/R-package/create-from-conda/0_setup.html#common-issues",
    "href": "docs/R-package/create-from-conda/0_setup.html#common-issues",
    "title": "setup",
    "section": "",
    "text": "An important consideration for Conda builds is the list of dependencies, specified versions, and compatibility with each of the other dependencies.\nIf the meta.yaml and DESCRIPTION file specify specific package versions, Conda‚Äôs ability to resolve the Conda environment also becomes more limited.\nFor example, if the Conda package we are building has the following requirements:\nDependency A version == 1.0\nDependency B version &gt;= 2.5\nAnd the Dependencies located in our Conda channel have the following dependencies:\nDependency A version 1.0\n  - Dependency C version == 0.5\n\nDependency A version 1.2\n- Dependency C version &gt;= 0.7\n\nDependency B version 2.7\n  - Dependency C version &gt;= 0.7\nAs you can see, the Conda build will not be able to resolve the environment because Dependency A verion 1.0 needs an old version of Dependency C, while Dependency B version 2.7 needs a newer version.\nIn this case, if we changed our package‚Äôs DESCRIPTION and meta.yaml file to be:\nDependency A version &gt;= 1.0\nDependency B version &gt;= 2.5\nThe conda build will be able to resolve. This is a simplied version of a what are more often complex dependency structures, but it is an important concept in conda package building that will inevitably arise as a package‚Äôs dependencies become more specific.\nTo check on the versions of packages that are available in a Conda channel, use the command:\nconda search $dependency\nReplace $dependency with the name of package you would like to investigate. There are more optional commands for this function which can be found here:\nhttps://docs.conda.io/projects/conda/en/latest/commands/search.html\nTo check the dependencies of packages that exist in your Conda cache, go to the folder specified for your conda cache (that we specified earlier). In case you need to find that path you can use:\nconda info\nHere there will be a folder for each of the packages that has been used in a conda build (including the dependencies). In each folder is another folder called ‚Äúinfo‚Äù and a file called ‚Äúindex.json‚Äù that lists information, such as depends for the package.\nHere is an example:\ncat /rstudio-files/ccbr-data/users/$USER/conda-cache/r-ggplot2-3.3.6-r41hc72bb7e_1/info/index.json\n\n{\n \"arch\": null,\n \"build\": \"r41hc72bb7e_1\",\n \"build_number\": 1,\n \"depends\": [\n   \"r-base &gt;=4.1,&lt;4.2.0a0\",\n   \"r-digest\",\n   \"r-glue\",\n   \"r-gtable &gt;=0.1.1\",\n   \"r-isoband\",\n   \"r-mass\",\n   \"r-mgcv\",\n   \"r-rlang &gt;=0.3.0\",\n   \"r-scales &gt;=0.5.0\",\n   \"r-tibble\",\n   \"r-withr &gt;=2.0.0\"\n ],\n \"license\": \"GPL-2.0-only\",\n \"license_family\": \"GPL2\",\n \"name\": \"r-ggplot2\",\n \"noarch\": \"generic\",\n \"platform\": null,\n \"subdir\": \"noarch\",\n \"timestamp\": 1665515494942,\n \"version\": \"3.3.6\"\n}\nIf you would like to specify an exact package to use in a conda channel for your conda build, specify the build string in your ‚Äúmeta.yaml‚Äù file. In the above example for ggplot version 3.3.6, the build string is listed in the folder name for package as well as in the index.json file for ‚Äúbuild:‚Äùr41hc72bb7e_1‚Äù.",
    "crumbs": [
      "Home",
      "R Package",
      "Create from Conda",
      "setup"
    ]
  },
  {
    "objectID": "docs/containers/singularity-biowulf.html",
    "href": "docs/containers/singularity-biowulf.html",
    "title": "How to use containers with singularity on biowulf",
    "section": "",
    "text": "Log in to biowulf and start an interactive session.\nLoad the singularity module\nPull the container image from dockerhub:\nSingularity will convert the docker image to a SIF file and write it in your current directory.\nRun the container in an interactive bash shell:\nAlternatively, you can pull the container and run it interactively in one command like this:\nRun any commands you wish to while inside the container.\nYou can exit the container by typing exit and pressing enter.",
    "crumbs": [
      "Home",
      "Containers",
      "How to use containers with singularity on biowulf"
    ]
  },
  {
    "objectID": "docs/containers/singularity-biowulf.html#cache-directory",
    "href": "docs/containers/singularity-biowulf.html#cache-directory",
    "title": "How to use containers with singularity on biowulf",
    "section": "Cache directory",
    "text": "Cache directory\nSingularity caches images in a directory set by the SINGULARITY_CACHEDIR environment variable. By default, this is set to $HOME/.singularity. However, there is limited space in user‚Äôs home directories. Instead, we recommend setting it to a location with more disk space, such as /data/$USER/.singularity.\nYou can set this custom cache directory location by pasting the following line in your ~/.bashrc or ~/.zshrc:\nexport SINGULARITY_CACHEDIR=/data/$USER/.singularity\nThis will take effect if you log out and log back in, or source your rc file. You can check the cache directory variable like so:\necho $SINGULARITY_CACHEDIR\n\n/data/$USER/.singularity\n\n(Your own username will appear instead of $USER.)",
    "crumbs": [
      "Home",
      "Containers",
      "How to use containers with singularity on biowulf"
    ]
  },
  {
    "objectID": "docs/containers/singularity-biowulf.html#package-versions",
    "href": "docs/containers/singularity-biowulf.html#package-versions",
    "title": "How to use containers with singularity on biowulf",
    "section": "Package versions",
    "text": "Package versions\n\nR\nYou can find out the versions of R packages installed in a container like so:\nsingularity exec docker://nciccbr/carlisle_r:v2 R -e \"readr::write_tsv(tibble::as_tibble(installed.packages()), 'r-packages.tsv')\"\nThis command runs R code in the container to write the installed packages to a TSV file called r-packages.tsv.\n\n\nPython\nRun this command to find out the versions of Python packages installed in a container:\nsingularity exec docker://nciccbr/ccbr_ubuntu_22.04:v4 pip list &gt; pip_list.txt\nThe packages & versions are written to pip_list.txt.",
    "crumbs": [
      "Home",
      "Containers",
      "How to use containers with singularity on biowulf"
    ]
  },
  {
    "objectID": "docs/GitHub/github-actions-demo.html",
    "href": "docs/GitHub/github-actions-demo.html",
    "title": "GitHub Actions demo",
    "section": "",
    "text": "Demo: http://sovacool.dev/gh-actions-demo/\nExample code and worfklow files used in the demo: https://github.com/kelly-sovacool/gh-actions-sandbox\nPresented as a 1-hour live demo for the CCR Bioinformatics Training and Education Program\n\nrecording link: https://cbiit.webex.com/cbiit/ldr.php?RCID=3c728d9bd6f2efc3ecad6b72a807a7c2",
    "crumbs": [
      "Home",
      "GitHub",
      "GitHub Actions demo"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/0_overview.html",
    "href": "docs/GitHub/guide/0_overview.html",
    "title": "Overview of GitHub Topics",
    "section": "",
    "text": "Preparing your environment:\n\nDescribes how to create a PAT, add GH to your bash profile and use password-less login features\n\nBasic Commands:\n\nDescribes basic functions to be used with this SOP",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "Overview of GitHub Topics"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/0_overview.html#getting-set-up-and-familiar-with-github",
    "href": "docs/GitHub/guide/0_overview.html#getting-set-up-and-familiar-with-github",
    "title": "Overview of GitHub Topics",
    "section": "",
    "text": "Preparing your environment:\n\nDescribes how to create a PAT, add GH to your bash profile and use password-less login features\n\nBasic Commands:\n\nDescribes basic functions to be used with this SOP",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "Overview of GitHub Topics"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/0_overview.html#basics",
    "href": "docs/GitHub/guide/0_overview.html#basics",
    "title": "Overview of GitHub Topics",
    "section": "Basics",
    "text": "Basics\n\nCreating your GitHub repo:\n\nProvides information on how to setup a GitHub repository under CCBR and the use of templates\n\nCreating your Documentation:\n\nProvides information on how to setup documentation under your repository; provided with all template repos\n\nGitHub Actions:\n\nProvides information for using GitHub Actions under your repository; provided with all template repos",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "Overview of GitHub Topics"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/0_overview.html#best-practices",
    "href": "docs/GitHub/guide/0_overview.html#best-practices",
    "title": "Overview of GitHub Topics",
    "section": "Best Practices",
    "text": "Best Practices\n\nUse Pre-commit Hooks\nCCBR Projects, new Pipelines\nTechDev",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "Overview of GitHub Topics"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/4_basic_docs.html",
    "href": "docs/GitHub/guide/4_basic_docs.html",
    "title": "GitHub Basics: Documentation",
    "section": "",
    "text": "GitHub Pages is quick and easy way to build static websites for your GitHub repositories. Essentially, you write pages in Markdown which are then rendered to HTML and hosted on GitHub, free of cost!\nCCBR has used GitHub pages to provide extensive, legible and organized documentation for our pipelines. Examples are included below:\n\nCARLISLE\nPipeliner\nRNA-seek\n\nMkdocs is the with documentation tool preferred, with the Material theme, for most of the CCBR GitHub Pages websites.\n\n\nmkdocs and the Material for mkdocs theme can be installed using the following:\npip install --upgrade pip\npip install mkdocs\npip install mkdocs-material\nAlso install other common dependencies:\npip install mkdocs-pymdownx-material-extras\npip install mkdocs-git-revision-date-localized-plugin\npip install mkdocs-git-revision-date-plugin\npip install mkdocs-minify-plugin\n\n\n\nGenerally, for GitHub repos with GitHub pages:\n\nThe repository needs to be public (not private)\nThe main/master branch has the markdown documents under a docs folder at the root level\nRendered HTMLs are hosted under a gh-pages branch at root level\n\n\n\n\nThe following steps can be followed to build your first website\n\n\n\nmkdocs.yaml needs to be added to the root of the master branch. A template of this file is available in the cookiecutter template.\ngit clone https://github.com/CCBR/xyz.git\ncd xyz\nvi mkdocs.yaml\ngit add mkdocs.yaml\ngit commit -m \"adding mkdocs.yaml\"\ngit push\nHere is a sample mkdocs.yaml:\n# Project Information\nsite_name: CCBR How Tos\nsite_author: Vishal Koparde, Ph.D.\nsite_description: &gt;-\n  The **DEVIL** is in the **DETAILS**. Step-by-step detailed How To Guides for data management and other CCBR-relevant tasks.\n\n# Repository\nrepo_name: CCBR/HowTos\nrepo_url: https://github.com/CCBR/HowTos\nedit_uri: https://github.com/CCBR/HowTos/edit/main/docs/\n\n# Copyright\ncopyright: Copyright &copy; 2023 CCBR\n\n# Configuration\ntheme:\n  name: material\n  features:\n    - navigation.tabs\n    - navigation.top\n    - navigation.indexes\n    - toc.integrate\n  palette:\n    - scheme: default\n      primary: indigo\n      accent: indigo\n      toggle:\n        icon: material/toggle-switch-off-outline\n        name: Switch to dark mode\n    - scheme: slate\n      primary: red\n      accent: red\n      toggle:\n        icon: material/toggle-switch\n        name: Switch to light mode\n  logo: images/doc-book.svg\n  favicon: images/favicon.png\n\n# Plugins\nplugins:\n  - search\n  - git-revision-date\n  - minify:\n      minify_html: true\n\n\n# Customization\nextra:\n  social:\n    - icon: fontawesome/solid/users\n      link: http://bioinformatics.cancer.gov\n    - icon: fontawesome/brands/github\n      link: https://github.com/CCRGeneticsBranch\n    - icon: fontawesome/brands/docker\n      link: https://hub.docker.com/orgs/nciccbr/repositories\n  version:\n    provider: mike\n\n\n# Extensions\nmarkdown_extensions:\n  - markdown.extensions.admonition\n  - markdown.extensions.attr_list\n  - markdown.extensions.def_list\n  - markdown.extensions.footnotes\n  - markdown.extensions.meta\n  - markdown.extensions.toc:\n      permalink: true\n  - pymdownx.arithmatex:\n      generic: true\n  - pymdownx.betterem:\n      smart_enable: all\n  - pymdownx.caret\n  - pymdownx.critic\n  - pymdownx.details\n  - pymdownx.emoji:\n      emoji_index: !!python/name:materialx.emoji.twemoji\n      emoji_generator: !!python/name:materialx.emoji.to_svg\n  - pymdownx.highlight\n  - pymdownx.inlinehilite\n  - pymdownx.keys\n  - pymdownx.magiclink:\n      repo_url_shorthand: true\n      user: squidfunk\n      repo: mkdocs-material\n  - pymdownx.mark\n  - pymdownx.smartsymbols\n  - pymdownx.snippets:\n      check_paths: true\n  - pymdownx.superfences\n  - pymdownx.tabbed\n  - pymdownx.tasklist:\n      custom_checkbox: true\n  - pymdownx.tilde\n\n# Page Tree\nnav:\n  - Intro : index.md\n\n\n\nCreate docs folder, add your index.md there.\nmkdir docs\necho \"### Testing\" &gt; docs/index.md\ngit add docs/index.md\ngit commit -m \"adding landing page\"\ngit push\n\n\n\nmkdocs can now be used to render .md to HTML\nmkdocs build\nINFO     -  Cleaning site directory\nINFO     -  Building documentation to directory: /Users/$USER/Documents/GitRepos/parkit/site\nINFO     -  Documentation built in 0.32 seconds\nThe above command creates a local site folder which is the root of your ‚Äúto-be-hosted‚Äù website. You can now open the HTMLs in the site folder locally to ensure that that HTML is as per you liking. If not, then you can make edits to the .md files and rebuild the site.\nNOTE: You do not want to push the site folder back to GH and hence it needs to be added to .gitignore file:\necho \"**/site/*\" &gt; .gitignore\ngit add .gitignore\ngit commit -m \"adding .gitignore\"\ngit push\n\n\n\nThe following command with auto-create a gh-pages branch (if it does not exist) and copy the contents of the site folder to the root of that branch. It will also provide you the URL to your newly created website.\nmkdocs gh-deploy\nINFO     -  Cleaning site directory\nINFO     -  Building documentation to directory: /Users/kopardevn/Documents/GitRepos/xyz/site\nINFO     -  Documentation built in 0.34 seconds\nWARNING  -  Version check skipped: No version specified in previous deployment.\nINFO     -  Copying '/Users/kopardevn/Documents/GitRepos/xyz/site' to 'gh-pages' branch and pushing to\n            GitHub.\nEnumerating objects: 51, done.\nCounting objects: 100(51/51), done.\nDelta compression using up to 16 threads\nCompressing objects: 100(47/47), done.\nWriting objects: 100(51/51), 441.71 KiB | 4.29 MiB/s, done.\nTotal 51 (delta 4), reused 0 (delta 0), pack-reused 0\nremote: Resolving deltas: 100(4/4), done.\nremote:\nremote: Create a pull request for 'gh-pages' on GitHub by visiting:\nremote:      https://github.com/CCBR/xyz/pull/new/gh-pages\nremote:\nTo https://github.com/CCBR/xyz.git\n * [new branch]      gh-pages -&gt; gh-pages\nINFO     -  Your documentation should shortly be available at: https://CCBR.github.io/xyz/\nNow if you point your web browser to the URL from gh-deploy command (IE https://CCBR.github.io/xyz/) you will see your HTML hosted on GitHub. After creating your docs, the cookiecutter template includes a GitHub action which will automatically perform the above tasks whenever a push is performed to the main branch.\n\n\n\n\nGo to the main GitHub page of your repository\nOn the top right select the gear icon next to About\nUnder Website, select Use your GitHub Pages website.\nSelect Save Changes",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Basics: Documentation"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/4_basic_docs.html#install-mkdocs-themes",
    "href": "docs/GitHub/guide/4_basic_docs.html#install-mkdocs-themes",
    "title": "GitHub Basics: Documentation",
    "section": "",
    "text": "mkdocs and the Material for mkdocs theme can be installed using the following:\npip install --upgrade pip\npip install mkdocs\npip install mkdocs-material\nAlso install other common dependencies:\npip install mkdocs-pymdownx-material-extras\npip install mkdocs-git-revision-date-localized-plugin\npip install mkdocs-git-revision-date-plugin\npip install mkdocs-minify-plugin",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Basics: Documentation"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/4_basic_docs.html#conventions",
    "href": "docs/GitHub/guide/4_basic_docs.html#conventions",
    "title": "GitHub Basics: Documentation",
    "section": "",
    "text": "Generally, for GitHub repos with GitHub pages:\n\nThe repository needs to be public (not private)\nThe main/master branch has the markdown documents under a docs folder at the root level\nRendered HTMLs are hosted under a gh-pages branch at root level",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Basics: Documentation"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/4_basic_docs.html#create-website",
    "href": "docs/GitHub/guide/4_basic_docs.html#create-website",
    "title": "GitHub Basics: Documentation",
    "section": "",
    "text": "The following steps can be followed to build your first website",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Basics: Documentation"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/4_basic_docs.html#add-mkdocs.yaml",
    "href": "docs/GitHub/guide/4_basic_docs.html#add-mkdocs.yaml",
    "title": "GitHub Basics: Documentation",
    "section": "",
    "text": "mkdocs.yaml needs to be added to the root of the master branch. A template of this file is available in the cookiecutter template.\ngit clone https://github.com/CCBR/xyz.git\ncd xyz\nvi mkdocs.yaml\ngit add mkdocs.yaml\ngit commit -m \"adding mkdocs.yaml\"\ngit push\nHere is a sample mkdocs.yaml:\n# Project Information\nsite_name: CCBR How Tos\nsite_author: Vishal Koparde, Ph.D.\nsite_description: &gt;-\n  The **DEVIL** is in the **DETAILS**. Step-by-step detailed How To Guides for data management and other CCBR-relevant tasks.\n\n# Repository\nrepo_name: CCBR/HowTos\nrepo_url: https://github.com/CCBR/HowTos\nedit_uri: https://github.com/CCBR/HowTos/edit/main/docs/\n\n# Copyright\ncopyright: Copyright &copy; 2023 CCBR\n\n# Configuration\ntheme:\n  name: material\n  features:\n    - navigation.tabs\n    - navigation.top\n    - navigation.indexes\n    - toc.integrate\n  palette:\n    - scheme: default\n      primary: indigo\n      accent: indigo\n      toggle:\n        icon: material/toggle-switch-off-outline\n        name: Switch to dark mode\n    - scheme: slate\n      primary: red\n      accent: red\n      toggle:\n        icon: material/toggle-switch\n        name: Switch to light mode\n  logo: images/doc-book.svg\n  favicon: images/favicon.png\n\n# Plugins\nplugins:\n  - search\n  - git-revision-date\n  - minify:\n      minify_html: true\n\n\n# Customization\nextra:\n  social:\n    - icon: fontawesome/solid/users\n      link: http://bioinformatics.cancer.gov\n    - icon: fontawesome/brands/github\n      link: https://github.com/CCRGeneticsBranch\n    - icon: fontawesome/brands/docker\n      link: https://hub.docker.com/orgs/nciccbr/repositories\n  version:\n    provider: mike\n\n\n# Extensions\nmarkdown_extensions:\n  - markdown.extensions.admonition\n  - markdown.extensions.attr_list\n  - markdown.extensions.def_list\n  - markdown.extensions.footnotes\n  - markdown.extensions.meta\n  - markdown.extensions.toc:\n      permalink: true\n  - pymdownx.arithmatex:\n      generic: true\n  - pymdownx.betterem:\n      smart_enable: all\n  - pymdownx.caret\n  - pymdownx.critic\n  - pymdownx.details\n  - pymdownx.emoji:\n      emoji_index: !!python/name:materialx.emoji.twemoji\n      emoji_generator: !!python/name:materialx.emoji.to_svg\n  - pymdownx.highlight\n  - pymdownx.inlinehilite\n  - pymdownx.keys\n  - pymdownx.magiclink:\n      repo_url_shorthand: true\n      user: squidfunk\n      repo: mkdocs-material\n  - pymdownx.mark\n  - pymdownx.smartsymbols\n  - pymdownx.snippets:\n      check_paths: true\n  - pymdownx.superfences\n  - pymdownx.tabbed\n  - pymdownx.tasklist:\n      custom_checkbox: true\n  - pymdownx.tilde\n\n# Page Tree\nnav:\n  - Intro : index.md",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Basics: Documentation"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/4_basic_docs.html#create-index.md",
    "href": "docs/GitHub/guide/4_basic_docs.html#create-index.md",
    "title": "GitHub Basics: Documentation",
    "section": "",
    "text": "Create docs folder, add your index.md there.\nmkdir docs\necho \"### Testing\" &gt; docs/index.md\ngit add docs/index.md\ngit commit -m \"adding landing page\"\ngit push",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Basics: Documentation"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/4_basic_docs.html#build-site",
    "href": "docs/GitHub/guide/4_basic_docs.html#build-site",
    "title": "GitHub Basics: Documentation",
    "section": "",
    "text": "mkdocs can now be used to render .md to HTML\nmkdocs build\nINFO     -  Cleaning site directory\nINFO     -  Building documentation to directory: /Users/$USER/Documents/GitRepos/parkit/site\nINFO     -  Documentation built in 0.32 seconds\nThe above command creates a local site folder which is the root of your ‚Äúto-be-hosted‚Äù website. You can now open the HTMLs in the site folder locally to ensure that that HTML is as per you liking. If not, then you can make edits to the .md files and rebuild the site.\nNOTE: You do not want to push the site folder back to GH and hence it needs to be added to .gitignore file:\necho \"**/site/*\" &gt; .gitignore\ngit add .gitignore\ngit commit -m \"adding .gitignore\"\ngit push",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Basics: Documentation"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/4_basic_docs.html#deploy-site",
    "href": "docs/GitHub/guide/4_basic_docs.html#deploy-site",
    "title": "GitHub Basics: Documentation",
    "section": "",
    "text": "The following command with auto-create a gh-pages branch (if it does not exist) and copy the contents of the site folder to the root of that branch. It will also provide you the URL to your newly created website.\nmkdocs gh-deploy\nINFO     -  Cleaning site directory\nINFO     -  Building documentation to directory: /Users/kopardevn/Documents/GitRepos/xyz/site\nINFO     -  Documentation built in 0.34 seconds\nWARNING  -  Version check skipped: No version specified in previous deployment.\nINFO     -  Copying '/Users/kopardevn/Documents/GitRepos/xyz/site' to 'gh-pages' branch and pushing to\n            GitHub.\nEnumerating objects: 51, done.\nCounting objects: 100(51/51), done.\nDelta compression using up to 16 threads\nCompressing objects: 100(47/47), done.\nWriting objects: 100(51/51), 441.71 KiB | 4.29 MiB/s, done.\nTotal 51 (delta 4), reused 0 (delta 0), pack-reused 0\nremote: Resolving deltas: 100(4/4), done.\nremote:\nremote: Create a pull request for 'gh-pages' on GitHub by visiting:\nremote:      https://github.com/CCBR/xyz/pull/new/gh-pages\nremote:\nTo https://github.com/CCBR/xyz.git\n * [new branch]      gh-pages -&gt; gh-pages\nINFO     -  Your documentation should shortly be available at: https://CCBR.github.io/xyz/\nNow if you point your web browser to the URL from gh-deploy command (IE https://CCBR.github.io/xyz/) you will see your HTML hosted on GitHub. After creating your docs, the cookiecutter template includes a GitHub action which will automatically perform the above tasks whenever a push is performed to the main branch.",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Basics: Documentation"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/4_basic_docs.html#add-to-the-github-page",
    "href": "docs/GitHub/guide/4_basic_docs.html#add-to-the-github-page",
    "title": "GitHub Basics: Documentation",
    "section": "",
    "text": "Go to the main GitHub page of your repository\nOn the top right select the gear icon next to About\nUnder Website, select Use your GitHub Pages website.\nSelect Save Changes",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Basics: Documentation"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/5_basic_actions.html",
    "href": "docs/GitHub/guide/5_basic_actions.html",
    "title": "GitHub Basics: GitHub Actions",
    "section": "",
    "text": "GitHub Basics: GitHub Actions\nThe following describe the minimum GitHub actions that should be deployed with any production pipeline. The actions are automatically provided via the cookiecutter templates: NextFlow and Snakemake.\n\nDocumentation (assumes mkdocs build; required for all repos)\n\nThese rules will automatically update any documentation built with mkdocs for all PR‚Äôs.\nRule Name(s): mkdocs_build ‚Äì&gt; pages-build-and-deployment\n\nLintr (required for CCBR projects and new pipelines)\n\nThis rule will automatically perform a lintr with the data provided in the .test folder of the pipeline. Review the GitHub Best Practices - Test Data page for more information.\n\nDry-run with test sample data for any PR to dev branch (required for CCBR projects and new pipelines)\n\nThis rule will automatically perform a dry-run with the data provided in the .test folder of the pipeline. Review the GitHub Best Practices - Test Data page for more information.\n\nFull-run with full sample data for any PR to main branch (required for CCBR projects and new pipelines)\n\nThis rule will automatically perform a full-run with the data provided in the .test folder of the pipeline. Review the GitHub Best Practices - Test Data page for more information.\n\nAuto pull/push from source (if applicable for CCBR projects and new pipelines)\n\nIf the pipeline is forked from another location and updating this forked pipeline is required, an action will automatically perform a pull from the source location at least once a week.\n\nAdd assigned issues & PRs to user projects.\nWhen an issue or PR is assigned to a CCBR member, this action will automatically add it to their personal GitHub Project, if they have one. This file can be copy and pasted exactly as-is into any CCBR repo from here.",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub Basics: GitHub Actions"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/6_howto_precommit.html",
    "href": "docs/GitHub/guide/6_howto_precommit.html",
    "title": "GitHub HowTo: Pre-Commit",
    "section": "",
    "text": "Pre-commit should be added to all GitHub repositories on Biowulf and any clones created elsewhere to ensure cohesive and informative commit messages. After the creating the repository the following commands should be run in order to initialize the pre-commit hook, and establish the following requirements for all commit messages:\n\n\nPre-commit has been installed as a module on Biowulf. Set up an interactive session, and follow the steps below. A pre-commit configuration file is needed, and can be copied from the CCBR template repo.\n# load module on biowulf\nmodule load precommit\n\n# CD into the GitHub repo\ncd &lt;repo_name&gt;\n\n# install precommit in the repo - this is the only time you need to do this, per local repo location\npre-commit install\n\n# copy and update the precommit config\ntouch .pre-commit.config\n\n\n\nCommits must follow the format listed below, as designated by Angular:\n&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;\n&lt;body&gt;\n&lt;BLANK LINE&gt;\n&lt;footer&gt;\n\n\nThe type must be one of the following options:\n\nfeat: A new feature\nfix: A bug fix\ndocs: Documentation only changes\nperf: A code change that improves performance\nstyle: Style changes that do not affect the meaning of the code (white-space, formatting, etc.)\ntest: Adding tests or correcting existing tests\nci: Changes to our CI configuration files and scripts\nbuild: Changes that affect the build system or external dependencies\nrefactor: A code change that neither fixes a bug nor adds a feature, such as to improve code understandability\nchore: Maintenance tasks that don‚Äôt affect production code behavior (e.g.¬†configuration files)\n\nfeat, fix, docs, and perf changes are user-facing changes because they affect users of the code, and should be included in the changelog. All other types are non user-facing changes and typically do not need to be included in the changelog.\n\n\n\nThe scope must be one of the following options:\n\nanimations\ncommon\ncompiler\ncompiler-cli\ncore\nelements\nforms\nhttp\nlanguage-service\nplatform-browser\nplatform-browser-dynamic\nplatform-server\nplatform-webworker\nplatform-webworker-dynamic\nrouter\nservice-worker\nupgrade\n\n\n\n\nThe subject must be a succinct description of the change. It should follow the following rules:\n\nuse the imperative, present tense: ‚Äúchange‚Äù not ‚Äúchanged‚Äù nor ‚Äúchanges‚Äù\ndon‚Äôt capitalize the first letter\nno dot (.) at the end\n\n\n\n\nThe body should include the motivation for the change and contrast this with previous behavior. It should follow the following rule:\n\nuse the imperative, present tense: ‚Äúchange‚Äù not ‚Äúchanged‚Äù nor ‚Äúchanges‚Äù\n\n\n\n\nThe footer should contain any information about Breaking Changes and is also the place to reference GitHub issues that this commit Closes.\n\nBreaking Changes should start with the word BREAKING CHANGE: with a space or two newlines. The rest of the commit message is then used for this.\nClosed bugs should be listed on a separate line in the footer from Breaking Changes, prefixed with ‚ÄúCloses‚Äù keyword (Closes #234 or Closes #123, #245, #992).\n\n\n\n\nBelow are some examples of properly formatted commit messages\n# example\ndocs(changelog): update changelog to beta.5\n\n# example\nfix(release): need to depend on latest rxjs and zone.js\n\n# example\nfeat($browser): onUrlChange event (popstate/hashchange/polling)\nAdded new event to $browser:\n- forward popstate event if available\n- forward hashchange event if popstate not available\n- do polling when neither popstate nor hashchange available\n\nBreaks $browser.onHashChange, which was removed (use onUrlChange instead)\n\n# example\nfix($compile): couple of unit tests for IE9\nOlder IEs serialize html uppercased, but IE9 does not...\nWould be better to expect case insensitive, unfortunately jasmine does\nnot allow to user regexps for throw expectations.\n\nCloses #392\nBreaks foo.bar api, foo.baz should be used instead",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub HowTo: Pre-Commit"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/6_howto_precommit.html#using-precommit",
    "href": "docs/GitHub/guide/6_howto_precommit.html#using-precommit",
    "title": "GitHub HowTo: Pre-Commit",
    "section": "",
    "text": "Pre-commit has been installed as a module on Biowulf. Set up an interactive session, and follow the steps below. A pre-commit configuration file is needed, and can be copied from the CCBR template repo.\n# load module on biowulf\nmodule load precommit\n\n# CD into the GitHub repo\ncd &lt;repo_name&gt;\n\n# install precommit in the repo - this is the only time you need to do this, per local repo location\npre-commit install\n\n# copy and update the precommit config\ntouch .pre-commit.config",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub HowTo: Pre-Commit"
    ]
  },
  {
    "objectID": "docs/GitHub/guide/6_howto_precommit.html#structure-of-commit-messages",
    "href": "docs/GitHub/guide/6_howto_precommit.html#structure-of-commit-messages",
    "title": "GitHub HowTo: Pre-Commit",
    "section": "",
    "text": "Commits must follow the format listed below, as designated by Angular:\n&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;\n&lt;body&gt;\n&lt;BLANK LINE&gt;\n&lt;footer&gt;\n\n\nThe type must be one of the following options:\n\nfeat: A new feature\nfix: A bug fix\ndocs: Documentation only changes\nperf: A code change that improves performance\nstyle: Style changes that do not affect the meaning of the code (white-space, formatting, etc.)\ntest: Adding tests or correcting existing tests\nci: Changes to our CI configuration files and scripts\nbuild: Changes that affect the build system or external dependencies\nrefactor: A code change that neither fixes a bug nor adds a feature, such as to improve code understandability\nchore: Maintenance tasks that don‚Äôt affect production code behavior (e.g.¬†configuration files)\n\nfeat, fix, docs, and perf changes are user-facing changes because they affect users of the code, and should be included in the changelog. All other types are non user-facing changes and typically do not need to be included in the changelog.\n\n\n\nThe scope must be one of the following options:\n\nanimations\ncommon\ncompiler\ncompiler-cli\ncore\nelements\nforms\nhttp\nlanguage-service\nplatform-browser\nplatform-browser-dynamic\nplatform-server\nplatform-webworker\nplatform-webworker-dynamic\nrouter\nservice-worker\nupgrade\n\n\n\n\nThe subject must be a succinct description of the change. It should follow the following rules:\n\nuse the imperative, present tense: ‚Äúchange‚Äù not ‚Äúchanged‚Äù nor ‚Äúchanges‚Äù\ndon‚Äôt capitalize the first letter\nno dot (.) at the end\n\n\n\n\nThe body should include the motivation for the change and contrast this with previous behavior. It should follow the following rule:\n\nuse the imperative, present tense: ‚Äúchange‚Äù not ‚Äúchanged‚Äù nor ‚Äúchanges‚Äù\n\n\n\n\nThe footer should contain any information about Breaking Changes and is also the place to reference GitHub issues that this commit Closes.\n\nBreaking Changes should start with the word BREAKING CHANGE: with a space or two newlines. The rest of the commit message is then used for this.\nClosed bugs should be listed on a separate line in the footer from Breaking Changes, prefixed with ‚ÄúCloses‚Äù keyword (Closes #234 or Closes #123, #245, #992).\n\n\n\n\nBelow are some examples of properly formatted commit messages\n# example\ndocs(changelog): update changelog to beta.5\n\n# example\nfix(release): need to depend on latest rxjs and zone.js\n\n# example\nfeat($browser): onUrlChange event (popstate/hashchange/polling)\nAdded new event to $browser:\n- forward popstate event if available\n- forward hashchange event if popstate not available\n- do polling when neither popstate nor hashchange available\n\nBreaks $browser.onHashChange, which was removed (use onUrlChange instead)\n\n# example\nfix($compile): couple of unit tests for IE9\nOlder IEs serialize html uppercased, but IE9 does not...\nWould be better to expect case insensitive, unfortunately jasmine does\nnot allow to user regexps for throw expectations.\n\nCloses #392\nBreaks foo.bar api, foo.baz should be used instead",
    "crumbs": [
      "Home",
      "GitHub",
      "Guide",
      "GitHub HowTo: Pre-Commit"
    ]
  },
  {
    "objectID": "docs/conda-mamba/conda-to-mamba.html",
    "href": "docs/conda-mamba/conda-to-mamba.html",
    "title": "Move from Conda to Mamba",
    "section": "",
    "text": "Migrating from Anaconda/Miniconda (conda) to Miniforge (mamba)\n  \n  1. List the conda environments\n  2. Download the Python script to fix exported YAML\n  3. Create mamba compatible exported YAML files\n  4. Uninstall conda (Anaconda/Miniconda)\n  5. Install mamba (Miniforge)\n  6. Clear cache\n  7. Enforce strict channel policy\n  8. Verify channel configuration\n  9. Rebuild old env with mamba\n  Appendix A: fix_yaml.py",
    "crumbs": [
      "Home",
      "Conda Mamba",
      "Move from Conda to Mamba"
    ]
  },
  {
    "objectID": "docs/conda-mamba/conda-to-mamba.html#list-the-conda-environments",
    "href": "docs/conda-mamba/conda-to-mamba.html#list-the-conda-environments",
    "title": "Move from Conda to Mamba",
    "section": "1. List the conda environments",
    "text": "1. List the conda environments\nUse your current conda to list environments before migrating.\nconda env list\n# or (equivalent)\nconda info --envs",
    "crumbs": [
      "Home",
      "Conda Mamba",
      "Move from Conda to Mamba"
    ]
  },
  {
    "objectID": "docs/conda-mamba/conda-to-mamba.html#download-the-python-script-to-fix-exported-yaml",
    "href": "docs/conda-mamba/conda-to-mamba.html#download-the-python-script-to-fix-exported-yaml",
    "title": "Move from Conda to Mamba",
    "section": "2. Download the Python script to fix exported YAML",
    "text": "2. Download the Python script to fix exported YAML\nThis script: - removes duplicate packages, - drops prefix: and other install-specific metadata, - forces conda-forge, bioconda and defaults as the only channels, - prepares the file for mamba environment creation.\n\n\n\n\n\n\nImportant\n\n\n\nCopy the script from Appendix A into fix_yaml.py.",
    "crumbs": [
      "Home",
      "Conda Mamba",
      "Move from Conda to Mamba"
    ]
  },
  {
    "objectID": "docs/conda-mamba/conda-to-mamba.html#create-mamba-compatible-exported-yaml-files",
    "href": "docs/conda-mamba/conda-to-mamba.html#create-mamba-compatible-exported-yaml-files",
    "title": "Move from Conda to Mamba",
    "section": "3. Create mamba compatible exported YAML files",
    "text": "3. Create mamba compatible exported YAML files\nExport a single environment named xyz:\nconda env export --name xyz | python3 fix_yaml.py - -o xyz.yml\n\n\n\n\n\n\nNote\n\n\n\nYou can export all environments at once:\nfor env in $(conda env list | grep -v \"^#\" | awk 'NF&gt;1 {print $1}'); do\n  conda env export --name \"$env\" | python3 fix_yaml.py - -o \"${env}.yml\"\ndone\n\n\n\n\n\n\n\n\nCaution\n\n\n\nIf you have path-only environments (created from arbitrary folders), conda env list may show full paths instead of simple names. Consider exporting those individually to ensure correct name: in the YAML.",
    "crumbs": [
      "Home",
      "Conda Mamba",
      "Move from Conda to Mamba"
    ]
  },
  {
    "objectID": "docs/conda-mamba/conda-to-mamba.html#uninstall-conda-anacondaminiconda",
    "href": "docs/conda-mamba/conda-to-mamba.html#uninstall-conda-anacondaminiconda",
    "title": "Move from Conda to Mamba",
    "section": "4. Uninstall conda (Anaconda/Miniconda)",
    "text": "4. Uninstall conda (Anaconda/Miniconda)\nFollow vendor instructions (macOS/Linux):\nhttps://www.anaconda.com/docs/getting-started/anaconda/uninstall#macos-or-linux\nThen remove common leftovers:\n# If Miniconda\nrm -rf ~/miniconda ~/miniconda3\n\n# If Anaconda\nrm -rf ~/anaconda ~/anaconda3\n\nrm -rf ~/.conda\nrm -rf ~/.condarc\nrm -rf ~/.continuum\nrm -rf ~/.conda_envs_dir\nrm -rf ~/.conda_envs_dir_test\nEdit your shell init and delete the conda init block:\n# &gt;&gt;&gt; conda initialize &gt;&gt;&gt;\n...\n# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;\nFiles to check (depending on your shell): ~/.zshrc, ~/.bashrc, ~/.bash_profile.\nRestart your terminal.\n\n\n\n\n\n\nWarning\n\n\n\nMake sure conda is no longer on your PATH:\nwhich conda || echo \"conda not found (expected)\"",
    "crumbs": [
      "Home",
      "Conda Mamba",
      "Move from Conda to Mamba"
    ]
  },
  {
    "objectID": "docs/conda-mamba/conda-to-mamba.html#install-mamba-miniforge",
    "href": "docs/conda-mamba/conda-to-mamba.html#install-mamba-miniforge",
    "title": "Move from Conda to Mamba",
    "section": "5. Install mamba (Miniforge)",
    "text": "5. Install mamba (Miniforge)\nDownload mamba for macOS:\n# Apple Silicon (M1/M2/M3):\ncurl -L -O https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge-MacOSX-arm64.sh\n\n# Intel Macs:\ncurl -L -O https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge-MacOSX-x86_64.sh\n\n# Install (interactive)\nbash Miniforge-MacOSX-*.sh\nVerify:\nmamba --version\nmamba info | sed -n '1,60p'\n\n\n\n\n\n\nNote\n\n\n\nOnce mamba installation is completed then delete the install script.\nrm -f Miniforge-MacOSX-*.sh",
    "crumbs": [
      "Home",
      "Conda Mamba",
      "Move from Conda to Mamba"
    ]
  },
  {
    "objectID": "docs/conda-mamba/conda-to-mamba.html#clear-cache",
    "href": "docs/conda-mamba/conda-to-mamba.html#clear-cache",
    "title": "Move from Conda to Mamba",
    "section": "6. Clear cache",
    "text": "6. Clear cache\n# Safe to run; ignores errors if conda is gone\nconda clean --all -y || true\nmamba clean --all -y || true",
    "crumbs": [
      "Home",
      "Conda Mamba",
      "Move from Conda to Mamba"
    ]
  },
  {
    "objectID": "docs/conda-mamba/conda-to-mamba.html#enforce-strict-channel-policy",
    "href": "docs/conda-mamba/conda-to-mamba.html#enforce-strict-channel-policy",
    "title": "Move from Conda to Mamba",
    "section": "7. Enforce strict channel policy",
    "text": "7. Enforce strict channel policy\n# Start fresh (ignore error if key doesn't exist)\nmamba config remove-key channels 2&gt;/dev/null || true\n\n# Add exactly the three you want\nmamba config prepend channels conda-forge\nmamba config append  channels bioconda\nmamba config append  channels defaults\n\n# Recommended\nmamba config set channel_priority strict\nThis: - forces to be conda-forge, bioconda and defaults in that order - removes any other channels that may point to repo.anaconda.com",
    "crumbs": [
      "Home",
      "Conda Mamba",
      "Move from Conda to Mamba"
    ]
  },
  {
    "objectID": "docs/conda-mamba/conda-to-mamba.html#verify-channel-configuration",
    "href": "docs/conda-mamba/conda-to-mamba.html#verify-channel-configuration",
    "title": "Move from Conda to Mamba",
    "section": "8. Verify channel configuration",
    "text": "8. Verify channel configuration\n# High-level info (shows channel list)\nmamba info\n\n# Full config dump\nmamba config list | sed -n '1,200p'\nWhat you want to see: - channels shows only conda-forge, bioconda, and defaults - Nothing should reference repo.anaconda.com. - conda.anaconda.org/conda-forge and conda.anaconda.org/bioconda are community-supported channels and are thus okay to use.",
    "crumbs": [
      "Home",
      "Conda Mamba",
      "Move from Conda to Mamba"
    ]
  },
  {
    "objectID": "docs/conda-mamba/conda-to-mamba.html#rebuild-old-env-with-mamba",
    "href": "docs/conda-mamba/conda-to-mamba.html#rebuild-old-env-with-mamba",
    "title": "Move from Conda to Mamba",
    "section": "9. Rebuild old env with mamba",
    "text": "9. Rebuild old env with mamba\nRecreate from your cleaned YAMLs:\n# Single env\nmamba create -f xyz.yml --yes --override-channels\n\n# All YAMLs in current directory\nfor y in *.yml *.yaml 2&gt;/dev/null; do\n  [ -f \"$y\" ] || continue\n  echo \"Rebuilding from $y\"\n  mamba create -f \"$y\" --yes --override-channels\ndone\n\n\n\n\n\n\nWarning\n\n\n\nDuring solve, you should NOT see:\nwarning  libmamba 'repo.anaconda.com', a commercial channel hosted by Anaconda.com, is used.\nIf you do, revisit Steps 7‚Äì8 to correct your channel configuration.\n\n\nActivate and test:\nmamba activate xyz\npython -V",
    "crumbs": [
      "Home",
      "Conda Mamba",
      "Move from Conda to Mamba"
    ]
  },
  {
    "objectID": "docs/conda-mamba/conda-to-mamba.html#appendix-a-fix_yaml.py",
    "href": "docs/conda-mamba/conda-to-mamba.html#appendix-a-fix_yaml.py",
    "title": "Move from Conda to Mamba",
    "section": "Appendix A: fix_yaml.py",
    "text": "Appendix A: fix_yaml.py\n\n\n\n\n\n\nNote\n\n\n\nThis script reads an exported conda env export YAML from stdin (or a file), removes duplicates, deletes prefix:, forces appropriate channels, removes unwanted channels, preserves pip: sections, and writes the cleaned YAML to stdout (or -o FILE).\n\n\n#!/usr/bin/env python3\n\"\"\"\nfix_yaml.py\n- Normalize an exported conda environment YAML for mamba recreation.\n- Drops install-specific fields (prefix), forces conda-forge channels, de-duplicates dependencies.\nUsage:\n  python3 fix_yaml.py input.yml -o output.yml\n  conda env export --name myenv | python3 fix_yaml.py - -o myenv.yml\n\"\"\"\nimport sys\nimport argparse\nimport io\nfrom collections import OrderedDict\n\ntry:\n    import yaml  # PyYAML if available\nexcept Exception:\n    yaml = None\n\ndef load_text(stream):\n    return stream.read()\n\ndef parse_yaml(text):\n    if yaml is None:\n        # Minimal fallback: very light parser for a subset of env YAML\n        # It preserves lines and does line-level dedup under `dependencies:`.\n        # For robust parsing, install PyYAML: mamba install pyyaml\n        data = {\"_raw\": text}\n        return data\n    return yaml.safe_load(text)\n\ndef dump_yaml(data):\n    if yaml is None:\n        # Fallback: return original text with only simple text transforms\n        lines = data[\"_raw\"].splitlines(True)\n\n        out, in_deps, seen, skip_prefix = [], False, set(), False\n        for ln in lines:\n            if ln.startswith(\"name:\"):\n                out.append(ln)\n                continue\n            if ln.strip().startswith(\"prefix:\"):\n                # drop\n                continue\n            if ln.strip().startswith(\"channels:\"):\n                out.append(\"channels:\\n  - conda-forge\\n  - bioconda\\n  - defaults\\n\")\n                skip_prefix = True\n                continue\n            if skip_prefix:\n                # skip original channel lines until a non-channel item appears\n                if ln.startswith(\" \") or ln.startswith(\"\\t\") or ln.strip().startswith(\"-\"):\n                    # still channel block; skip\n                    continue\n                else:\n                    skip_prefix = False\n\n            if ln.strip() == \"dependencies:\":\n                in_deps = True\n                out.append(ln)\n                continue\n\n            if in_deps:\n                    # New rule: handle lines with two '='\n                if ln.count(\"=\") &gt;= 2:\n                    first = ln.find(\"=\")\n                    second = ln.find(\"=\", first + 1)\n                    if second == first + 1:\n                        out.append(ln)\n                        continue\n                    else:\n                        # case like 'pkg=1=py39_0' ‚Üí keep only before 2nd '='\n                        parts = ln.split(\"=\")\n                        newln = \"=\".join(parts[:2]) + \"\\n\"\n                        out.append(newln)\n                        continue\n                else:\n                    out.append(ln)\n                    if ln[0] != \" \" and ln.strip()[-1] == \":\":\n                        in_deps = False\n                    continue\n\n        return \"\".join(out)\n\n    # PyYAML path (preferred)\n    if data is None:\n        data = {}\n    data.pop(\"prefix\", None)\n\n    # Normalize channels ‚Üí conda-forge only\n    data[\"channels\"] = [\"conda-forge\"]\n\n    # De-duplicate top-level conda dependencies\n    deps = data.get(\"dependencies\", [])\n    new_deps = []\n    seen = set()\n\n    for item in deps:\n        if isinstance(item, str):\n            if item not in seen:\n                new_deps.append(item)\n                seen.add(item)\n        elif isinstance(item, dict) and \"pip\" in item:\n            # Keep pip sublist as-is (with de-dup inside)\n            pip_list = item.get(\"pip\", [])\n            pip_seen = set()\n            new_pip = []\n            for p in pip_list:\n                if p not in pip_seen:\n                    new_pip.append(p)\n                    pip_seen.add(p)\n            new_deps.append({\"pip\": new_pip})\n        else:\n            # Unknown structured entry; keep\n            new_deps.append(item)\n\n    data[\"dependencies\"] = new_deps\n\n    # Emit YAML\n    class OrderedDumper(yaml.SafeDumper):\n        pass\n    def _dict_representer(dumper, data):\n        return dumper.represent_dict(data.items())\n    OrderedDumper.add_representer(OrderedDict, _dict_representer)\n\n    return yaml.dump(data, Dumper=OrderedDumper, sort_keys=False)\n\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"input\", help=\"Input YAML file or '-' for stdin\")\n    ap.add_argument(\"-o\", \"--output\", help=\"Output YAML file (default: stdout)\")\n    args = ap.parse_args()\n\n    if args.input == \"-\":\n        text = load_text(sys.stdin)\n    else:\n        with open(args.input, \"r\", encoding=\"utf-8\") as f:\n            text = f.read()\n\n    data = parse_yaml(text)\n    out = dump_yaml(data)\n\n    if args.output:\n        with open(args.output, \"w\", encoding=\"utf-8\") as f:\n            f.write(out)\n    else:\n        sys.stdout.write(out)\n\nif __name__ == \"__main__\":\n    main()\n\nThat‚Äôs it! You‚Äôve exported your old conda environments, removed Anaconda/Miniconda, installed Miniforge, enforced conda-forge only, and rebuilt your environments with mamba‚Äîwithout touching repo.anaconda.com.",
    "crumbs": [
      "Home",
      "Conda Mamba",
      "Move from Conda to Mamba"
    ]
  },
  {
    "objectID": "docs/code-club/index.html",
    "href": "docs/code-club/index.html",
    "title": "Code Club",
    "section": "",
    "text": "Journal Club where the idea to have a CCBR Code Club was proposed\nFirst Code Club: Establishing a community of practice",
    "crumbs": [
      "Home",
      "Code Club"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#the-problem",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#the-problem",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "The problem",
    "text": "The problem\nBioinformatics is critical for biological research, but bioinformatics software often does not follow good software engineering practices.\n\n\n\n\nhttps://imgs.xkcd.com/comics/dependency.png",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#implications",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#implications",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "Implications",
    "text": "Implications\n\nerror-prone code can lead to invalid scientific findings\ntechnical debt - makes future changes more difficult",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#causes-of-poor-software-quality-in-bioinformatics",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#causes-of-poor-software-quality-in-bioinformatics",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "Causes of poor software quality in bioinformatics",
    "text": "Causes of poor software quality in bioinformatics\n\nmany bioinformaticians lack training in software development\nacademia credits individual researchers to aid their careers, which deincentivizes teams",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#writing-good-code-is-hard",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#writing-good-code-is-hard",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "Writing good code is hard!",
    "text": "Writing good code is hard!\n\n\n\n\nhttps://imgs.xkcd.com/comics/good_code.png",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#the-trap-of-technical-debt",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#the-trap-of-technical-debt",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "The trap of technical debt",
    "text": "The trap of technical debt\n\n\n\n\nhttps://imgs.xkcd.com/comics/fixing_problems.png",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#the-trap-of-perfect-code",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#the-trap-of-perfect-code",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "The trap of ‚Äúperfect‚Äù code",
    "text": "The trap of ‚Äúperfect‚Äù code\n\n\n\n\nI find that when someone‚Äôs taking time to do something right in the present, they‚Äôre a perfectionist with no ability to prioritize, whereas when someone took time to do something right in the past, they‚Äôre a master artisan of great foresight.\n\n\nhttps://imgs.xkcd.com/comics/the_general_problem.png",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#the-authors-proposed-solution",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#the-authors-proposed-solution",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "The authors‚Äô proposed solution",
    "text": "The authors‚Äô proposed solution\nOrganize bioinformaticians into collaborative teams to facilitate:\n\nsoftware quality seminars\ncode reviews\nresource sharing\n\n(i.e.¬†a learning community)\n\nGoal\nPersuade academic researchers to organize collaborative teams within their institutions to improve software quality",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#lessons-from-the-tech-industry",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#lessons-from-the-tech-industry",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "Lessons from the tech industry",
    "text": "Lessons from the tech industry\n\nthe team is a basic unit: there are no lone geniuses\nteams incentivize collective ownership\nthe team context requires all members to adopt good software practices\n\n\nyou can‚Äôt collaborate effectively without version control software. your teammates won‚Äôt want to work with hard-to-read code.",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#lessons-from-rock-climbing",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#lessons-from-rock-climbing",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "Lessons from rock climbing",
    "text": "Lessons from rock climbing\nClimbers view the route together and discuss the best way to climb it.\n\n\n\n\n\nhttps://www.youtube.com/live/K2zoh6_4Pvw?feature=shared&t=4344\n\n\n\nstart at 1:12:24\nstop by 1:13:56",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#lessons-from-rock-climbing-1",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#lessons-from-rock-climbing-1",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "Lessons from rock climbing",
    "text": "Lessons from rock climbing\n\n\n\n\nFig 1. Kerenc et al. 2024. 10.1093/bioinformatics/btae632\n\n\ndiscuss most optimal way to reach goal before you start the project",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#improving-software-dev-as-a-team",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#improving-software-dev-as-a-team",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "Improving software dev as a team",
    "text": "Improving software dev as a team\na learning community with:\n\nsoftware quality seminars\ncode reviews\nresource sharing",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#software-quality-seminars",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#software-quality-seminars",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "1. Software quality seminars",
    "text": "1. Software quality seminars\n\npresentations & demos covering new techniques, tools, methods, and theory\ntopics are applicable across multiple projects\nresult: members acquire new knowledge to apply to their projects, community gains a shared vocabulary to discuss their work",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#code-review-sessions",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#code-review-sessions",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "2. Code review sessions",
    "text": "2. Code review sessions\n\nreview a section of code line-by-line to offer feedback\nstandard practice in industry, but seldom implemented in academia\nresult: enforce consistent coding standards, detect bugs, positive peer pressure to write good",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#resource-sharing",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#resource-sharing",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "3. Resource sharing",
    "text": "3. Resource sharing\n\nexternal open access resources + internal resources\nrepositories, packages, libraries, seminar recordings\nresult: encourages re-use and further enables knowledge sharing\n\n\nhttps://ferenckata.github.io/ImprovingSoftwareTogether.github.io/index.html",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#outcomes",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#outcomes",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "Outcomes",
    "text": "Outcomes\norganizing academic bioinformaticians into teams:\n\nfosters collaboration while allowing individuals to retain personal ownership & advance their careers\nlowers the barrier to adapt new technologies and techniques\nimproves software quality to enable deeper trust in the resulting scientific discoveries\nbenefits large & small projects alike",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#future-perspectives",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#future-perspectives",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "Future perspectives",
    "text": "Future perspectives\nChallenges in encouraging good software practices\n\nFunding agencies reward novelty; few grants fund maintenance\nAcademia considers journal publications as the token of success\n\n\nSilver linings\n\nFunding for critical software maintenance: Chan Zuckerberg Initiative, Schmidt Futures\nJournal of Open Source Software: great venue for publishing scientific software",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/code-club/2025-02-11_journal-club/slides.html#group-discussion",
    "href": "docs/code-club/2025-02-11_journal-club/slides.html#group-discussion",
    "title": "Improving bioinformatics software quality through teamwork",
    "section": "Group discussion",
    "text": "Group discussion\n\nIn what ways do the authors‚Äô ideas apply to us?\nHow are we already applying some of their ideas?\nAre there ways we could improve our teamwork?\n\n\n\nAcademia vs Core resource\nReusable software vs One-time-use analysis code",
    "crumbs": [
      "Home",
      "Code Club",
      "2025 02 11 Journal Club",
      "Improving bioinformatics software quality through teamwork"
    ]
  },
  {
    "objectID": "docs/zenodo.html",
    "href": "docs/zenodo.html",
    "title": "Zenodo",
    "section": "",
    "text": "Submit a pipeline to Zenodo in order to create a DOI for publication.",
    "crumbs": [
      "Home",
      "Zenodo"
    ]
  },
  {
    "objectID": "docs/zenodo.html#reference-link",
    "href": "docs/zenodo.html#reference-link",
    "title": "Zenodo",
    "section": "Reference link",
    "text": "Reference link\nUse the link for full information, summarized below: https://www.youtube.com/watch?v=A9FGAU9S9Ow",
    "crumbs": [
      "Home",
      "Zenodo"
    ]
  },
  {
    "objectID": "docs/zenodo.html#prepare-github-repository",
    "href": "docs/zenodo.html#prepare-github-repository",
    "title": "Zenodo",
    "section": "Prepare GitHub Repository",
    "text": "Prepare GitHub Repository\nThe GitHub repository should include the following:\n\nREADME page\nDocumentation page, such as mkdocs, with usage and contact information\nA citation CITATION.cff; Example here\nTagged and versioned, stable repository",
    "crumbs": [
      "Home",
      "Zenodo"
    ]
  },
  {
    "objectID": "docs/zenodo.html#link-github-account-to-zenodo",
    "href": "docs/zenodo.html#link-github-account-to-zenodo",
    "title": "Zenodo",
    "section": "Link GitHub account to Zenodo",
    "text": "Link GitHub account to Zenodo\n\nGo to Zenodo\nSelect username in the top right &gt;&gt; Profile. Select `GitHub``\nClick Sync Now (top right) to update repos. NOTE: You may have to refresh the page\nToggle the On button on the repo you wish to publish. This will move the pipeline to the Enable Repositories list.",
    "crumbs": [
      "Home",
      "Zenodo"
    ]
  },
  {
    "objectID": "docs/zenodo.html#prepare-github-repo",
    "href": "docs/zenodo.html#prepare-github-repo",
    "title": "Zenodo",
    "section": "Prepare GitHub Repo",
    "text": "Prepare GitHub Repo\n\nGo to GitHub and find the repository page.\nSelect Releases &gt;&gt; Draft a new release\nCreate a tag, following naming semantics described here\nDescribe the tag with the following: ‚ÄúConnecting pipeline to Zenodo‚Äù",
    "crumbs": [
      "Home",
      "Zenodo"
    ]
  },
  {
    "objectID": "docs/zenodo.html#update-zenodo-information",
    "href": "docs/zenodo.html#update-zenodo-information",
    "title": "Zenodo",
    "section": "Update Zenodo Information",
    "text": "Update Zenodo Information\n\nGo to Zenodo\nSelect My dashboard &gt;&gt; Edit\nUpdate the following information:\n\nResource Type: Software\nTitle: Full Pipeline Name (ShorthandName) (IE Mouse nEoanTigen pRedictOr (METRO)\nCreators: Add creators, including ORCID‚Äôs whenever possible\nDescription: A short description of the main features of the pipeline\nAdditional Description: If you use this software, please cite it as below.\nKeywords and subjects: Add several keywords related to the pipeline\nVersion: add the version used in GitHub\nPublisher: Zenodo\nRelated works: ‚ÄúIs original form of‚Äù ‚Äúgithub website‚Äù URL",
    "crumbs": [
      "Home",
      "Zenodo"
    ]
  },
  {
    "objectID": "docs/zenodo.html#add-doi-citation-to-github",
    "href": "docs/zenodo.html#add-doi-citation-to-github",
    "title": "Zenodo",
    "section": "Add DOI, citation to GitHub",
    "text": "Add DOI, citation to GitHub\n\nGo to Zenodo\nSelect username in the top right &gt;&gt; Profile. Select `GitHub``\nClick Sync Now (top right) to update repos. NOTE: You may have to refresh the page\nCopy the DOI for the repository\nReturn to the GitHub repository and edit the README of the GitHub repo, adding the DOI link.\nUpdate the CITATION.cff as needed.\nCreate a new tagged version.",
    "crumbs": [
      "Home",
      "Zenodo"
    ]
  }
]